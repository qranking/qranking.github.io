<html>
	<head><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110075493-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-110075493-1');
</script>
<!-- Google AdSense -->
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6168511236629369",
    enable_page_level_ads: true
  });
</script>

		<meta charset="UTF-8" />
		<title>Qiita Ranking (sugulu)</title>
		<link rel="stylesheet" type="text/css" href="../qranking.css">
	</head>
	<body>
<div class="headerContainer">
<h1>Qiitaいいね数ランキング (sugulu さんの投稿分)</h1>
</div><!--class="headerContainer"-->
<p><a href="#" onclick="javascript:window.history.back(-1);return false;">[戻る]</a></p>
<p><i><img width="16" height="16" src="../thumb-up-120px.png" /></i>が同じ値の場合は投稿日時の新しいものが上位としています。</p>
<p><i><img width="16" height="16" src="../thumb-up-120px.png" /></i>がついていない記事は表示していません。</p>
<table border="1">
<tr>
	<td rowspan="3"><center>suguluさんの<br />1位</center></td>
	<td colspan="4">
		<kbd><i><img alt="いいね" width="16" height="16" src="../thumb-up-120px.png" /></i>642</kbd>
		<a target="_blank" href="https://qiita.com/sugulu/items/3c7d6cbe600d455e853b">これから強化学習を勉強する人のための「強化学習アルゴリズム・マップ」と、実装例まとめ</a>
	</td>
</tr>
<tr>
	<td style="width:100px;"><center>投稿日時</center></td>
	<td style="width:200px;"><center>投稿者</center></td>
	<td style="width:150px;"><center>タグ</center></td>
	<td style="width:350px;"><center>本文</center></td>
</tr>
<tr>
	<td style="width:100px;">
		<!--投稿日時--><center>2017-10-30 05:26:09</center>
	</td>
	<td style="width:200px;">
		@sugulu<br />(都内IT企業 所属)<br><img width="80" height="80" src="https://qiita-image-store.s3.amazonaws.com/0/191401/profile-images/1509357969">
	</td>
	<td style="width:150px;">
		<!--タグ-->
		<center><b>[Python]</b> <b>[機械学習]</b> <b>[DeepLearning]</b> <b>[強化学習]</b> <b>[TensorFlow]</b> </center>
	</td>
	<td style="width:350px;">
		<!--本文-->
		<div style="width:350px;height:150px;overflow-x:hidden;overflow-y:scroll;"><p>これから強化学習を勉強したい人に向けて、「どんなアルゴリズムがあるのか」、「どの順番で勉強すれば良いのか」を示した強化学習アルゴリズムの「学習マップ」を作成しました。</p>

<p>さらに、各手法を実際にどう実装すれば良いのかを、簡単な例題を対象に実装しました。<br>
本記事では、ひとつずつ解説します。</p>

<p><a href="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" alt="RLmap.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/9bbafddb-7b2b-e26b-994e-a97e9b4b497d.png"></a></p>

<p>オレンジ枠の手法は、実装例を紹介します。</p>

<p>※今回マップを作るにあたっては、以下の文献を参考にしました。<br>
●<a href="https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0-%E3%83%81%E3%83%A7%E3%83%90-%E3%82%B5%E3%83%91%E3%82%B7%E3%83%90%E3%83%AA/dp/4320124227" rel="nofollow noopener" target="_blank">速習 強化学習: 基礎理論とアルゴリズム（書籍）</a><br>
●<a href="https://arxiv.org/pdf/1708.07902.pdf" rel="nofollow noopener" target="_blank">Deep Learning for Video Game Playing</a></p>

<h1>
<span id="強化学習とは" class="fragment"></span><a href="#%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%81%A8%E3%81%AF"><i class="fa fa-link"></i></a>強化学習とは</h1>

<p>強化学習は、画像識別のような教師あり学習や、クラスタリングのような教師なし学習とは少し異なる、機械学習の分野です。</p>

<p>最終的に達成したいゴールはあるけれど、そこにいたる詳細な制御手法は分からないときに、ゴールできたかどうかをベースに、制御手法を構築する学習手法です。</p>

<p>例えばぶつからない車の自動運転技術の開発では、「センサーで車間距離が○ mになったら、ブレーキをさせる」というのは、ルールベースの制御です。</p>

<p>一方で、実際に車を何度もシミュレーションや実車で走らせて、ぶつかった場合は改善し、ぶつからなかった場合はその制御手法を採用し、何度も試行錯誤しながら、ぶつからないというゴールを達成させる学習手法が、強化学習です。</p>

<p>強化学習は主に「何かの制御」や「対戦型ゲームのアルゴリズム」に使用されることが多い手法です。</p>

<p>「何かの制御」であれば、先ほどのぶつからない自動運転技術の開発や、サーバールームの空調代金を安く抑える空調の制御手法の開発、共有サーバーの資源の最適配分、ロボットの運動制御などがあります。<br>
また最近では、文章生成などにも使われたりしています。</p>

<p>基本的に作りたいもののゴールは設定できるけど、そのゴールを達成するための手法・ルール・制御をうまく設計できない場合に、強化学習が使われています。</p>

<p>「対戦型ゲームのアルゴリズム」では、アルファ碁やアルファ碁ゼロに強化学習が使われています。</p>

<p>本記事では「対戦型ゲーム」は取り扱っておらず、どちらかというと、「何かの制御」に使われる強化学習アルゴリズムを紹介します。</p>

<h1>
<span id="強化学習の例題" class="fragment"></span><a href="#%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%81%AE%E4%BE%8B%E9%A1%8C"><i class="fa fa-link"></i></a>強化学習の例題</h1>

<p>強化学習の例題としては、単純な迷路課題や3目並べなども使われますが、OpenAIのCartPoleが最もおすすめです。</p>

<p>CartPoleは以下のような、倒立振子の制御問題です。</p>

<p><a href="https://camo.qiitausercontent.com/2290b5df1644ed62fcfb40e6bbbb26792bc06d26/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39653961613431662d343534642d363738312d643432392d6533303366383564316138332e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/2290b5df1644ed62fcfb40e6bbbb26792bc06d26/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39653961613431662d343534642d363738312d643432392d6533303366383564316138332e676966" alt="openaigym.video.0.2643.video000000.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/9e9aa41f-454d-6781-d429-e303f85d1a83.gif"></a></p>

<p>CartPoleをおすすめする理由は、動画で動きが見れて楽しいのと、適度な複雑さがあるからです。<br>
一方で使用するのは非常に簡単です。</p>

<p>このCartPoleは、小学生が掃除の時間にほうきを手のひらで立てて遊ぶのと同じ事をしています。</p>

<p>各時刻step=tで選択できる行動a(t)は、土台の車を（右に押す, 左に押す）の2択です。<br>
そして、状態s(t)は土台の車の位置x(t)、速度v(t)と、棒の角度θ(t)と角速度ω(t)の4変数です。</p>

<p>やりたいことは、状態s(t)に応じて適切な行動a(t)を実行し、棒を立て続けることです。</p>

<p>本記事ではこのCartPoleを対象に各アルゴリズムの実装を行いました。</p>

<p>それでは次に、各アルゴリズムを紹介します。</p>

<h1>
<span id="deep-learningが生まれる前までの強化学習" class="fragment"></span><a href="#deep-learning%E3%81%8C%E7%94%9F%E3%81%BE%E3%82%8C%E3%82%8B%E5%89%8D%E3%81%BE%E3%81%A7%E3%81%AE%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"><i class="fa fa-link"></i></a>Deep Learningが生まれる前までの強化学習</h1>

<p>強化学習において、Deep Learningが提案される前までのアルゴリズムには、代表的な手法が3つあります。<br>
Q-Learning、SARSA、モンテカルロ法です。</p>

<p><a href="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" alt="RLmap.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/9bbafddb-7b2b-e26b-994e-a97e9b4b497d.png"></a></p>

<h2>
<span id="q-learning" class="fragment"></span><a href="#q-learning"><i class="fa fa-link"></i></a>Q-Learning</h2>

<p>Q-Learning（Q学習）は、最も代表的な手法になります。<br>
まずはこの手法から勉強するのが良いです。</p>

<p>Q-LearningではQ関数と呼ばれる行動価値関数を学習し、制御を実現します。<br>
行動価値関数Q(a|s)とは、状態s(t)のときに行動aを行ったときに、その先どれくらいの報酬がもらえそうかを出力する関数です。</p>

<p>このQ関数に行動右に押すと、左に押すを入力したときの出力を比べ、報酬が多いほうを選べば、自然とCartPoleが立ち続けることになります。</p>

<p>以下の記事で、openAIのCartPoleの使用方法からQ-Learningのアルゴリズム詳細と実装例までを紹介しているので、まずはこちちらをご覧ください。</p>

<p>●<a href="http://neuro-educator.com/rl1/" rel="nofollow noopener" target="_blank">CartPoleでQ学習（Q-learning）を実装・解説【Phythonで強化学習：第1回】</a></p>

<h2>
<span id="sarsaモンテカルロ法" class="fragment"></span><a href="#sarsa%E3%83%A2%E3%83%B3%E3%83%86%E3%82%AB%E3%83%AB%E3%83%AD%E6%B3%95"><i class="fa fa-link"></i></a>SARSA、モンテカルロ法</h2>

<p>SARSAはQ-Learningと同様にQ関数を学習するのですが、少し学習の仕方が異なる手法です。</p>

<p>Q-LearningやSARSAは1stepごとにQ関数を学習していく手法です。<br>
それらとは異なり、CartPoleが倒れるまで行動し、その行動履歴からQ関数を学習する手法がモンテカルロ法です。</p>

<p>今回のマップではモンテカルロ法から進展しているアルゴリズムはありませんが、重要なアルゴリズムです。</p>

<p>SARSAとモンテカルロ法のアルゴリズム詳細と、CartPoleで実装した例を、次の記事にまとめましたので、詳細はこちらをご覧ください。</p>

<p>●<a href="https://qiita.com/sugulu/items/7a14117bbd3d926eb1f2" id="reference-c9a85a9bc8bc794a719c">シンプルな実装例で学ぶSARSA法およびモンテカルロ法【CartPoleで棒立て：1ファイルで完結】</a></p>

<p>Deep Learningが生まれる前までの強化学習手法としては、この3つが実装できれば良いのではと思います。</p>

<h1>
<span id="deep-learningが生まれてからの強化学習" class="fragment"></span><a href="#deep-learning%E3%81%8C%E7%94%9F%E3%81%BE%E3%82%8C%E3%81%A6%E3%81%8B%E3%82%89%E3%81%AE%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"><i class="fa fa-link"></i></a>Deep Learningが生まれてからの強化学習</h1>

<p>Deep Learningの出現にともない、強化学習にもDeep Learningが使われるようになり、ブレイクスルーが生まれました。</p>

<p><a href="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" alt="RLmap.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/9bbafddb-7b2b-e26b-994e-a97e9b4b497d.png"></a></p>

<h2>
<span id="dqnddqn" class="fragment"></span><a href="#dqnddqn"><i class="fa fa-link"></i></a>DQN、DDQN</h2>

<p>これまでQ関数を表すのに、実際には表を使用していました。<br>
表のサイズが、「状態sを離散化した数」×「行動の種類」となります。</p>

<p>ですが、表ではサイズに限りがあるため、ニューラルネットワークを使用したかったのですが、うまくいっていませんでした。</p>

<p>そこでQ関数の表現にDeep Learningを使用したのがDQN(Deep Q-Network もしくは Deep Q-Learning Network)です。</p>

<p>DQNの出現により、より複雑なゲームや制御問題の解決が可能になり、強化学習が注目を集めました。<br>
なおDeep Learning（深層学習）を用いた強化学習は、深層強化学習とも呼ばれます。</p>

<p>またDQNの学習で、2つのQ-networkを使用する手法をDouble DQN（DDQN）と呼びます。</p>

<p>DQN、DDQNのアルゴリズム詳細とCartPoleを実装した例を、以下にまとめましたのでご覧ください。</p>

<p>●<a href="http://neuro-educator.com/rl2/" rel="nofollow noopener" target="_blank">CartPoleでDQN（deep Q-learning）、DDQNを実装・解説【Phythonで強化学習：第2回】</a></p>

<h2>
<span id="prioritized-experience-replaydueling-dqn" class="fragment"></span><a href="#prioritized-experience-replaydueling-dqn"><i class="fa fa-link"></i></a>prioritized experience replay、Dueling DQN</h2>

<p>DDQNをより良くするために、prioritized experience replayやDueling DQNが提案されました。</p>

<p>prioritized experience replayは、Q学習がまだ進んでいない状態s(t)の経験に対して、優先的に学習を実行させる手法です。</p>

<p><a href="https://camo.qiitausercontent.com/24ed68017e14aff067edd297278780efc8a69648/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f36613934373430662d363661322d393761382d383265642d3732653538353638323836622e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/24ed68017e14aff067edd297278780efc8a69648/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f36613934373430662d363661322d393761382d383265642d3732653538353638323836622e706e67" alt="pri.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/6a94740f-66a2-97a8-82ed-72e58568286b.png"></a></p>

<p>こちらでアルゴリズム詳細と実装例を紹介しています。</p>

<p>●<a href="https://qiita.com/sugulu/items/10ac7ce53de40d4c8891" id="reference-908ef97e8e8202feaeac">実装例から学ぶ優先順位付き経験再生 prioritized experience replay DQN </a></p>

<p>Dueling DQNは、Q関数を状態価値関数V(s)とAdvantage関数A(a|s)に分割して学習するネットワークを使用する手法です。</p>

<p>Q(a|s) = V(s) + A(a|s)</p>

<p>となります。</p>

<p>ネットワークで表すと以下の図の通りです。</p>

<p><a href="https://camo.qiitausercontent.com/a3ddc9965aa79398faaa075520a325739438f954/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65383662393931382d303137632d313536312d343430622d6639316534326630313463652e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/a3ddc9965aa79398faaa075520a325739438f954/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65383662393931382d303137632d313536312d343430622d6639316534326630313463652e706e67" alt="duelingnetwork.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/e86b9918-017c-1561-440b-f91e42f014ce.png"></a></p>

<p>こうすることで、どんな行動のときもV(s)を学習するため、Q関数の学習が早く良くなります。</p>

<p>こちらでアルゴリズム詳細と実装例を紹介しています。</p>

<p>●<a href="https://qiita.com/sugulu/items/6c4d34446d4878cde61a" id="reference-1baa4d1b29ab02907d13">実装例から学ぶDueling Network DQN </a></p>

<h1>
<span id="a3cの登場" class="fragment"></span><a href="#a3c%E3%81%AE%E7%99%BB%E5%A0%B4"><i class="fa fa-link"></i></a>A3Cの登場</h1>

<p>DQNの登場後、さらに深層強化学習の性能を上げることになった重要な手法がA3Cです。</p>

<p><a href="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" alt="RLmap.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/9bbafddb-7b2b-e26b-994e-a97e9b4b497d.png"></a></p>

<h2>
<span id="a3c" class="fragment"></span><a href="#a3c"><i class="fa fa-link"></i></a>A3C</h2>

<p>A3Cとは「Asynchronous Advantage Actor-Critic」の略称です。</p>

<p>A3Cの前に、並列化してDQNを行う手法として、GORILA（General Reinforcement Learning Architecture）が提案されていました。</p>

<p>ゴリラ（GORILLA）ではないので、注意してください。</p>

<p>なお"GORILA Python"で画像検索すると、いままで脳で想像したことのない面白い画像があります。<br>
●<a href="https://www.google.co.jp/search?q=GORILLA+Python&amp;rlz=1C1VSNC_enJP591JP713&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwj8yuXgn5XXAhUDXrwKHXAiCgMQ_AUICygC&amp;biw=1526&amp;bih=742" rel="nofollow noopener" target="_blank">"GORILA Python"で画像検索した結果</a></p>

<p>話が脱線しました。</p>

<p>A3Cは並列計算に加え、さらにAdvantageと呼ばれる報酬の計算方法を使用しています。</p>

<p>Advantageは報酬の計算を1step後ではなく、数ステップ後まで行動を実施して行う計算手法です。</p>

<p>さらにA3Cには、Actor-Criticと呼ばれるネットワークが使用されています。</p>

<p><a href="https://camo.qiitausercontent.com/3edcf3031f7ba912bebac04552fdb853e2404438/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f38613038656538372d303064332d386434332d303039612d3635346363393034303236652e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/3edcf3031f7ba912bebac04552fdb853e2404438/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f38613038656538372d303064332d386434332d303039612d3635346363393034303236652e706e67" alt="acnet.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/8a08ee87-00d3-8d43-009a-654cc904026e.png"></a></p>

<p>これは、Q関数ではなく、方策関数π(s)=[p(右), p(左)]で直接、状態sに応じた行動を出力します。<br>
p(右)は状態sで右に押すほうが良い確率を示します。<br>
この部分をActorと呼びます。</p>

<p>さらに同時に、状態価値V(s)も学習させます。<br>
この部分をCriticと呼びます。</p>

<p>DQNから比べると大きく変わっているので、理解が難しいのですが、実際にゆっくり実装してみると理解が深まります。</p>

<p>A3Cのアルゴリズム詳細とCartPoleを実装した例を、以下にまとめましたのでご覧ください。</p>

<p>●<a href="https://qiita.com/sugulu/items/acbc909dd9b74b043e45" id="reference-5f5fcab62be049bc415f">実装しながら学ぶA3C【CartPoleで棒立て：1ファイルで完結】</a></p>

<h2>
<span id="trpogeneralized-advantage-estimator" class="fragment"></span><a href="#trpogeneralized-advantage-estimator"><i class="fa fa-link"></i></a>TRPO、Generalized Advantage Estimator</h2>

<p>その他、強化学習のアルゴリズムとしてはA3Cとほぼ同時期に、TRPOやGAEなども提案されています。</p>

<p>TRPO(Trust Region Policy Optimization)は、方策関数π(s)が一度に大きく更新されすぎないように制限をかけて、方策関数π(s)を更新する手法です。</p>

<p>また、GAE（Generalized Advantage Estimator）という手法は、Advantageを考慮して報酬を計算する際に、どの程度先のステップまで考慮するのかを、1つの式で一般的に表せるようにした枠組みです。</p>

<h1>
<span id="2017年最新の強化学習unrealppo" class="fragment"></span><a href="#2017%E5%B9%B4%E6%9C%80%E6%96%B0%E3%81%AE%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92unrealppo"><i class="fa fa-link"></i></a>2017年最新の強化学習、UNREAL、PPO</h1>

<p><a href="https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0-%E3%83%81%E3%83%A7%E3%83%90-%E3%82%B5%E3%83%91%E3%82%B7%E3%83%90%E3%83%AA/dp/4320124227" rel="nofollow noopener" target="_blank">速習 強化学習: 基礎理論とアルゴリズム（書籍）</a>では、以上のアルゴリズムが紹介されていましたが、2016年末から2017年に発表されたアルゴリズムのうち、UNREALとPPOはとくに重要な手法なので、紹介します。</p>

<p><a href="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" alt="RLmap.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/9bbafddb-7b2b-e26b-994e-a97e9b4b497d.png"></a></p>

<h2>
<span id="unreal" class="fragment"></span><a href="#unreal"><i class="fa fa-link"></i></a>UNREAL</h2>

<p>UNREALは、「UNsupervised REinforcement and Auxiliary Learning」（UNREAL：教師なしの強化および補助学習）の略称です。</p>

<p>Auxiliaryを日本語にすると、「補助の」という意味です。<br>
A3Cのネットワークに、直接の制御目的とは少し異なる補助タスクを組み込んで、補助タスクがうまくできるようになることで、本来のゴールへの制御もうまくなるという作戦です。</p>

<p>例えば、URENALの論文では「3次元空間の迷路タスク」の攻略に、3つの補助タスクを組み込んでいます。</p>

<p>1つ目は、ピクセルコントロールと呼ばれ、画像ピクセルが大きく変化する動きを補助タスクとして学習させることで、迷路を進みやすくし、動き方を学ばせています。</p>

<p>2つ目は、 prioritized experience replayを強化し、報酬がもらえたときの状態s(t)を多く学習させ、現在の状態s(t)から、将来の報酬を予測させる補助タスクを行っています。</p>

<p>3つ目は、A3Cでは状態価値V(s)を学ぶ際に、過去の経験をシャッフルしないのですが、シャッフルしたバージョンで状態価値V(s)を学ぶ補助タスクを行っています。</p>

<p>これらの補助タスクがうまくできるようになることで、本来のタスクと一部共有しているネットワークの重みが学習され、本来のタスクもうまくなる仕組みです。</p>

<h2>
<span id="ppo" class="fragment"></span><a href="#ppo"><i class="fa fa-link"></i></a>PPO</h2>

<p>PPOは、openAIから2017年に発表された手法で、UNREALよりも実装が簡単です。</p>

<p>方策関数π(s)の更新手法として、TRPOが提案されていましたが、TRPOは実装がややこしく、またA3CのActor-Criticなど、出力が複数種類あるネットワークに適用できないという課題がありました。</p>

<p>この点を解決したのがPPOです。</p>

<p>PPOではClippingと呼ばれ、π(s) / π(s_old)が大きく変化した場合には、一定の値にしてしまう操作（Clip）を行います。</p>

<p>例えば、π(s_old)が0.1なのにπ(s)が0.9となった場合には、π(s) / π(s_old) = 9ではなく、1.2などにしてしまい、π(s) / π(s_old)が1.2以上は全部1.2としてしまう操作をします。</p>

<p>こうすることで、TRPOとは異なる手法で、方策関数π(s)の更新が大きく変化しすぎるのを防ぎます。</p>

<p>PPOのアルゴリズム詳細とCartPoleを実装した例を、以下にまとめましたのでご覧ください。</p>

<p>●<a href="https://qiita.com/sugulu/items/8925d170f030878d6582" id="reference-2c478f21573c38dcf8d5">実装しながら学ぶPPO【CartPoleで棒立て：1ファイルで完結】</a></p>

<h1>
<span id="まとめ" class="fragment"></span><a href="#%E3%81%BE%E3%81%A8%E3%82%81"><i class="fa fa-link"></i></a>まとめ</h1>

<p>以上、これから強化学習を勉強したい人に向けて、「どんなアルゴリズムがあるのか」、「どの順番で勉強すれば良いのか」を示した強化学習アルゴリズムの「学習マップ」を作成しました。</p>

<p>これから強化学習を勉強する方の参考になれば幸いです。</p>

<p>ご一読いただきまして、ありがとうございます。</p>

<p><a href="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/9b942a6dfd0333cb71f42483e14750abe6cad154/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39626261666464622d376232622d653236622d393934652d6139376539623462343937642e706e67" alt="RLmap.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/9bbafddb-7b2b-e26b-994e-a97e9b4b497d.png"></a></p>
</div>
	</td>
</tr>
</table>
<br />
<table border="1">
<tr>
	<td rowspan="3"><center>suguluさんの<br />2位</center></td>
	<td colspan="4">
		<kbd><i><img alt="いいね" width="16" height="16" src="../thumb-up-120px.png" /></i>483</kbd>
		<a target="_blank" href="https://qiita.com/sugulu/items/45e3cfaa78e5f13d9389">のび太と学ぶ「機械学習」～FX予測プログラムを作成～【第1話】if文作戦</a>
	</td>
</tr>
<tr>
	<td style="width:100px;"><center>投稿日時</center></td>
	<td style="width:200px;"><center>投稿者</center></td>
	<td style="width:150px;"><center>タグ</center></td>
	<td style="width:350px;"><center>本文</center></td>
</tr>
<tr>
	<td style="width:100px;">
		<!--投稿日時--><center>2017-11-06 05:51:37</center>
	</td>
	<td style="width:200px;">
		@sugulu<br />(都内IT企業 所属)<br><img width="80" height="80" src="https://qiita-image-store.s3.amazonaws.com/0/191401/profile-images/1509357969">
	</td>
	<td style="width:150px;">
		<!--タグ-->
		<center><b>[Python]</b> <b>[初心者]</b> <b>[機械学習]</b> <b>[DeepLearning]</b> <b>[FX]</b> </center>
	</td>
	<td style="width:350px;">
		<!--本文-->
		<div style="width:350px;height:150px;overflow-x:hidden;overflow-y:scroll;"><p>本シリーズでは、「FXの予想プログラム」を作りながら、機械学習・ディープラーニングを解説します。<br>
（注）のび太くんは有名漫画のキャラとは一切関係ありません。</p>

<h1>
<span id="あらすじ" class="fragment"></span><a href="#%E3%81%82%E3%82%89%E3%81%99%E3%81%98"><i class="fa fa-link"></i></a>あらすじ</h1>

<p>「のび太と機械学習」は、20歳の大学生、のび太くんが主人公です。</p>

<p>大学生のび太くんは、お小遣い欲しさにFXに興味を持ちます。</p>

<p>そんなのび太くんに、家庭教師のすぐるさんが、機械学習を解説し、FX予測プログラムを一緒に作ります。</p>

<p>残念なことに、FX予測プログラムは、ちょっとしか儲かりませんでした。</p>

<p>・<br>
・<br>
・</p>

<p>のび太くんはその後、学んだ機械学習を生かして、D-mind社を起業し、汎用人工知能を作り上げます。</p>

<p>高齢になったのび太くんは、D-mind社をTMR社に売却し、TMR社の工場で22世紀、汎用人工知能搭載型ロボットが誕生します。</p>

<p>この物語は、そんな汎用人工知能搭載型ロボット誕生につながる、のび太くんの機械学習・勉強記録です。</p>

<h1>
<span id="第1話のび太if文でfxを予想する" class="fragment"></span><a href="#%E7%AC%AC1%E8%A9%B1%E3%81%AE%E3%81%B3%E5%A4%AAif%E6%96%87%E3%81%A7fx%E3%82%92%E4%BA%88%E6%83%B3%E3%81%99%E3%82%8B"><i class="fa fa-link"></i></a>第1話：のび太、if文でFXを予想する</h1>

<p>ママー！！ママー！！</p>

<p>何よ、もう～。騒がしいわね。</p>

<p>ママ！聞いて！聞いて！！新しいiPhoneが出たんだよ！！<br>
iPhone Xだよ！<br>
ねえ、買って、買って～</p>

<p>無理よ。そんなお金あるわけないでしょ。<br>
それにあなた、携帯持っているじゃない。</p>

<p>ママ！これガラケーだよ。LINEもできなんだよ！！<br>
いまどきガラケーしか持っていないのなんて、僕くらいだよ！！</p>

<p>知りません。<br>
あなたには、大学の入学祝いに買ってあげた、パソコンがあるでしょ。<br>
それにもう20歳なんだから、欲しいものがあれば自分でバイトして買いなさい。</p>

<p>もう、いいよ！</p>

<p>・<br>
・<br>
・</p>

<p>すぐるさん。</p>

<p>どうしたんだい、のび太くん</p>

<p>すぐるさんは、iPhone X・・・　なんて持ってたりしないよね・・・？</p>

<p>僕は持っていないな～</p>

<p>すぐるさん・・・</p>

<p>なんだい、もう、きみが言おうとしていることは分かっている。<br>
何度も言っているけど、お金を増やす裏技はないからね。</p>

<p>だよね・・・。分かっているよ。。。<br>
はっ！たかし！！</p>

<p>たかしくんが、どうかしたんだい？</p>

<p>すぐるさん、この前たかしが、FXで儲けたって言ってたんだよ。<br>
パソコンで株みたいに売り買いするだけで、簡単にお金が増えたって。<br>
僕もFXでお金を増やして、iPhone Xを買ってやるんだ！</p>

<p>のび太くん、やめとき・・・<br>
（待てよ・・・これは良い機会かもしれないぞ！）<br>
分かったよ、のび太くん。<br>
きみがそこまで言うのなら、僕がFXを教えてあげるよ。</p>

<p>本当に！！やったー！！</p>

<p>先に言っとくけど、僕はFXによる資産形成は一切おススメしない。<br>
あくまで、ほんの少しの間、のび太くんの勉強のために付き合うだけだからね。</p>

<p>勉強！？</p>

<p>当たり前だ。適当に売り買いして儲かるわけないだろ。<br>
きちんと過去のデータを分析して、売買するんだ。</p>

<p>分かったよ。<br>
ところで、FXって何？</p>

<p>なんだい、知ってて、たかしくんの話をしてたのかと思ったよ。<br>
いいかいのび太くん、FXというのは・・・</p>

<p>・<br>
・<br>
・</p>

<p>すぐるさん、分かったよ！！<br>
・円とドルなどの外国の通貨同士を売り買いする外貨交換（Foreign eXchange）の略称で、FXなんだね<br>
・1ドル113円のときに、1000ドル分買って、1ドル114円のときに売れば、1円×1000で1,000円儲かる<br>
・逆に1ドル112円になってしまうと、1,000円損する<br>
・でもドルを売るほうからも売買できて、下がる方向を予想しても儲かる<br>
・手数料的なものは、今回は無視できるくらい小さい<br>
・始値は一日の取引の開始値段、終値は一日の取引の終了時の値段、高値は一日で一番高かった値段で、安値は一番安かった値段</p>

<p>ってことだね。</p>

<p>のび太くん、ついに分かってくれたか！！！<br>
もう13回目の説明だよ・・・<br>
今日は疲れた。続きはまた明日にしよう。</p>

<p>・<br>
・<br>
・</p>

<p>のび太くん、今日からFXの予測プログラムを作ろう。</p>

<p>予測プログラム？<br>
ぼくに、そんな難しいもの作れるかな・・・</p>

<p>でも適当に売り買いしても、世界で一番不運なきみでは、一瞬にして貯金がなくなるだけだよ。</p>

<p>なんだよ、そこまで言わなくてもいいじゃないか。</p>

<p>のび太くん、君ももう20歳になったんだ。<br>
自分の力で頑張るんだ。<br>
君がアイデアを考えれば、プログラムの作り方は僕が教えてあげるよ。</p>

<p>分かったよ、考えてみるよ・・・</p>

<p>・<br>
・<br>
・</p>

<p>すぐるさん、すぐるさん！！<br>
良い方法を思いついたよ。</p>

<p>えらいぞ、のび太くん！！で、どんな方法だい？</p>

<p>「今日の終値が昨日の終値よりも高かったら、きっと明日も高くなる。<br>
逆に、安かったら、きっと明日は安くなる。」<br>
どうだい！？</p>

<p>う～ん、のび太くんにしては悪くないアイデアだ！！<br>
じゃあ、実際にプログラムにして、バックテストをしよう！</p>

<p>バックテスト？</p>

<p>過去のデータでその手法を試してみて、本当に儲かるかどうかを検証するんだ。<br>
のび太くん、入学祝いで買ってもらったPCがあるだろ。<br>
それを使ってプログラムを書こう。<br>
よし、「アナコンダ～♪」</p>

<p>あなこんだ？</p>

<p>Anacondaは、データ解析によく使われるPythonというプログラミング言語と、データ解析用ライブラリのセットだよ。</p>

<p>とりあえず、以下の記事などを参考にして、Anacondaをインストールするんだ。<br>
分からないところは、Qiitaで自分で調べるんだ。</p>

<p>AnacondaとTensorFlowのインストール記事の例<br>
<a href="https://qiita.com/uosansatox/items/3e2e8e0a286e1635c548" class="autolink" id="reference-dcf0547973d5397ef1d0">https://qiita.com/uosansatox/items/3e2e8e0a286e1635c548</a></p>

<p>よし、できたよ。</p>

<p>じゃあ次は為替のデータだ。<br>
今回はドル円の一日ごとのデータに絞るね。<br>
僕のGitHubに、ドル円の1997年から2017年10/30までのデータがあるから、それをダウンロードするんだ。<br>
<a href="https://github.com/Nobita-FX/Nobita-FX/tree/master/data" rel="nofollow noopener" target="_blank">データはこちら</a></p>

<p>※為替データは、以下のフリーでログイン不要のデータサイトStooqから取得しました。<br>
<a href="https://stooq.com/q/d/?s=usdjpy&amp;c=0" rel="nofollow noopener" target="_blank">Stooq</a></p>

<p>よし、ダウンロードできたよ。</p>

<p>じゃあ、以下の記事などを参考に、Jupyter Notebookを開いて、プログラムを書こう。<br>
<a href="https://qiita.com/icoxfog417/items/175f69d06f4e590face9" class="autolink" id="reference-5ddd1d234917398cdc72">https://qiita.com/icoxfog417/items/175f69d06f4e590face9</a></p>

<p>今回は2016年と2017年のデータでバックテストをしてみよう。<br>
まずは、ファイルを読み込んでみよう。</p>

<p>今回のコードは全部僕のGitHubにおいて置くね。<br>
<a href="https://github.com/Nobita-FX/Nobita-FX/blob/master/program/NobitaFX_1_if.ipynb" rel="nofollow noopener" target="_blank">本記事の掲載コードはこちら</a></p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># import関連</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span> <span class="c1"># 実行上問題ない注意は非表示にします</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># dataフォルダの場所を各自指定してください</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="s2">"./"</span>

<span class="c1"># FXデータの読み込み</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_dir</span> <span class="o">+</span> <span class="s2">"USDJPY_1997_2017.csv"</span><span class="p">)</span>

<span class="c1"># Close-Openをデータに追加します</span>
<span class="n">data</span><span class="p">[</span><span class="s1">'Change'</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Close</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">Open</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span> <span class="c1"># データの概要を見てみます</span>

<span class="c1"># 2016年のデータを取り出します</span>
<span class="n">data16</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">4935</span><span class="p">:</span><span class="mi">5193</span><span class="p">,:]</span> <span class="c1"># pythonは0番目からindexが始まります</span>

<span class="c1"># 2017年のデータを取り出します</span>
<span class="n">data17</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">5193</span><span class="p">:,:]</span> 
<span class="n">data17</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div></div>

<p><a href="https://camo.qiitausercontent.com/5aa61a8e298c95ddfb3d1b3e535bdfc2e76d6a01/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f33393930333335622d663463612d613464662d386439642d3438396531323462643461332e6a706567" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/5aa61a8e298c95ddfb3d1b3e535bdfc2e76d6a01/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f33393930333335622d663463612d613464662d386439642d3438396531323462643461332e6a706567" alt="nobitafx1_1.JPG" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/3990335b-f4ca-a4df-8d9d-489e124bd4a3.jpeg"></a></p>

<p>このpandasというのは？</p>

<p>pandasはPythonでデータを読み込んで扱うためのライブラリだよ。<br>
<a href="https://qiita.com/hik0107/items/d991cc44c2d1778bb82e" class="autolink" id="reference-cc31369d5c9c63d1d6bd">https://qiita.com/hik0107/items/d991cc44c2d1778bb82e</a></p>

<p>まずは、お手本通りにやって、少しずつ慣れていこう。</p>

<p>よし、じゃあ2016年のデータに、さっきのび太くんが考えた<br>
「今日の終値が昨日の終値よりも高かったら、きっと明日も高くなる。<br>
逆に、安かったら、きっと明日は安くなる。」<br>
作戦を実施してみよう。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 2016年の合計を計算する</span>
<span class="c1"># 前々日終値に比べて前日終値が高い場合は、買い、低い場合は売りで入ります</span>
<span class="n">sum_2016</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data16</span><span class="p">)):</span> <span class="c1"># len()で要素数を取得しています</span>
    <span class="k">if</span> <span class="n">data16</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">data16</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">]:</span>
        <span class="n">sum_2016</span> <span class="o">+=</span> <span class="n">data16</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sum_2016</span> <span class="o">-=</span> <span class="n">data16</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"2016年の利益合計：</span><span class="si">%1.3lf</span><span class="s2">"</span> <span class="o">%</span><span class="n">sum_2016</span><span class="p">)</span> <span class="c1"># 2016年の利益合計</span>
</pre></div></div>

<p>出力：2016年の利益合計：9.675</p>

<p>すぐるさん、9.675だよ！！プラスだよ！！<br>
9.675ってことは、1000通貨だったら、9,675円、1万通貨なら約10万円、10万通貨なら100万円儲かるってことだよね！！<br>
これでiPhone Xが買えるぞ！！<br>
やった、やったー！！</p>

<p>落ち着きなよ、のび太くん。<br>
2017年の結果も見てごらん。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 2017年の合計を計算する</span>
<span class="c1"># 前々日終値に比べて前日終値が高い場合は、買い、低い場合は売りで入ります</span>
<span class="n">sum_2017</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data17</span><span class="p">)):</span> <span class="c1"># len()で要素数を取得しています</span>
    <span class="k">if</span> <span class="n">data17</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">data17</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">]:</span>
        <span class="n">sum_2017</span> <span class="o">+=</span> <span class="n">data17</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sum_2017</span> <span class="o">-=</span> <span class="n">data17</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"2017年の利益合計：</span><span class="si">%1.3lf</span><span class="s2">"</span> <span class="o">%</span><span class="n">sum_2017</span><span class="p">)</span> <span class="c1"># 2017年の利益合計</span>
</pre></div></div>

<p>出力：2017年の利益合計：-3.139</p>

<p>-3.139・・・<br>
マイナスじゃないか～！<br>
どういうことだよ、すぐるさん！</p>

<p>のび太くん、いちいちぼくに聞かないで、自分で考えるんだ。<br>
なんでだと思う？</p>

<p>う～ん、ぼくの作戦が間違っていたのかな？<br>
2016年はたまたまプラスになっただけ？</p>

<p>そうとも言えるし、そうとも言えない。</p>

<p>もう、わからないよ！</p>

<p>仕方ないな～。のび太くんの作戦はうまくいく場合とうまくいかない場合があるんだよ。<br>
「今日の終値が昨日の終値よりも高かったら、きっと明日も高くなる。<br>
逆に、安かったら、きっと明日は安くなる。」作戦は、<br>
大きなトレンドが生まれているときは使えるけど、そうでないときには機能しないんだ。</p>

<p>トレンド？</p>

<p>一度グラフをプロットしてみよう。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 2016年のデータをプロットしてみます</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">'seaborn-darkgrid'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data16</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">95</span><span class="p">,</span><span class="mi">125</span><span class="p">])</span>
</pre></div></div>

<p><a href="https://camo.qiitausercontent.com/d98e1db3a21c35850f18b671c0941253db066fa3/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39313233336162342d616331342d336566322d393932662d6361343133636466343032322e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/d98e1db3a21c35850f18b671c0941253db066fa3/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39313233336162342d616331342d336566322d393932662d6361343133636466343032322e706e67" alt="nobitafx1_2.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/91233ab4-ac14-3ef2-992f-ca413cdf4022.png"></a></p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 2017年からのデータをプロットしてみます</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data17</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">95</span><span class="p">,</span><span class="mi">125</span><span class="p">])</span>
</pre></div></div>

<p><a href="https://camo.qiitausercontent.com/9800043aa357f13f52aade5a9524b368e17b1a69/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f38323235326336612d323739392d303235342d666466662d6238316461396561376136622e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/9800043aa357f13f52aade5a9524b368e17b1a69/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f38323235326336612d323739392d303235342d666466662d6238316461396561376136622e706e67" alt="nobitafx1_3.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/82252c6a-2799-0254-fdff-b81da9ea7a6b.png"></a></p>

<p>トレンドってのは傾向のことで、一方向に上がり続けたり、下がり続けたりすることだよ。<br>
2016年は、前半はブレエグジットなんかがあって、ずっと下がり傾向で、後半はトランプ大統領への期待から、上がり傾向の、大きなトレンドが生まれていた年だったんだよ。</p>

<p>でも2017年は、そうした大きな傾向があまりないんだよ。<br>
同じ範囲内を上がったり、下がったりしている、レンジ相場っていうんだ。<br>
「今日の終値が昨日の終値よりも高かったら、きっと明日も高くなる。」って考える人もいれば、<br>
「今日の終値が昨日の終値よりも高くて、もう十分。きっと明日は安くなる。」って考える人も多いってことさ。</p>

<p>そうか～、せっかく考えたぼくの作戦はダメなのか・・・</p>

<p>そんなことないよ、のび太くん。<br>
2016年がプラスだったように、相場にトレンドがあるときにはのび太くんの作戦も使えないことはない。</p>

<p>う～ん、難しいんだね・・・</p>

<p>いや、のび太くんにしては、ここまで考えてやってみただけでも、十分さ！<br>
今日はここまでして、次回はもっと良い方法を使ってみよう。</p>

<p>良い方法？</p>

<p>機械学習と呼ばれる手法さ。<br>
はじめは単純な機械学習から実装してみて、うまくいく方法を探してみよう。</p>

<p>よし、次こそ、iPhone Xが買える作戦を立てるぞ！</p>

<p>その意気だ！のび太くん。<br>
次回は、「線形モデルによる回帰分析」で、FX予測をしてみよう。</p>

<p>１０００系モデルによる怪奇分析？<br>
よく分からないけど、大丈夫かな・・・</p>

<p>大丈夫だよ、細かい理論や実装方法は、僕がきちんと解説するから。</p>

<p>・<br>
・<br>
・</p>

<p><a href="https://qiita.com/sugulu/items/b6f891e060beab94b99b" id="reference-dcee39de5cac32320d73">「第2話：のび太、線形回帰を学ぶ」</a>に続く</p>

<hr>

<p>以上、ご一読いただきありがとうございます。<br>
記事やコードの間違え、改善点、不明点など、気軽にコメントいただければ幸いです。<br>
感謝</p>
</div>
	</td>
</tr>
</table>
<br />
<table border="1">
<tr>
	<td rowspan="3"><center>suguluさんの<br />3位</center></td>
	<td colspan="4">
		<kbd><i><img alt="いいね" width="16" height="16" src="../thumb-up-120px.png" /></i>107</kbd>
		<a target="_blank" href="https://qiita.com/sugulu/items/e3fc39f2e552f2355209">【機械学習初心者向け】scikit-learn「アルゴリズム・チートシート」の全手法を実装・解説してみた</a>
	</td>
</tr>
<tr>
	<td style="width:100px;"><center>投稿日時</center></td>
	<td style="width:200px;"><center>投稿者</center></td>
	<td style="width:150px;"><center>タグ</center></td>
	<td style="width:350px;"><center>本文</center></td>
</tr>
<tr>
	<td style="width:100px;">
		<!--投稿日時--><center>2017-09-23 07:44:02</center>
	</td>
	<td style="width:200px;">
		@sugulu<br />(都内IT企業 所属)<br><img width="80" height="80" src="https://qiita-image-store.s3.amazonaws.com/0/191401/profile-images/1509357969">
	</td>
	<td style="width:150px;">
		<!--タグ-->
		<center><b>[Python]</b> <b>[機械学習]</b> <b>[scikit-learn]</b> </center>
	</td>
	<td style="width:350px;">
		<!--本文-->
		<div style="width:350px;height:150px;overflow-x:hidden;overflow-y:scroll;"><p>scikit-learnのアルゴリズム・チートシートで紹介されている手法を全て実装し、解説してみました。</p>

<h1>
<span id="概要" class="fragment"></span><a href="#%E6%A6%82%E8%A6%81"><i class="fa fa-link"></i></a>概要</h1>

<p><a href="https://camo.qiitausercontent.com/475abe0d01d9ed0026e5d3f21989910a2aeeb3ce/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f33643435303838642d643965612d373539392d623139322d6539343762653734663830382e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/475abe0d01d9ed0026e5d3f21989910a2aeeb3ce/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f33643435303838642d643965612d373539392d623139322d6539343762653734663830382e706e67" alt="slcsheet.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/3d45088d-d9ea-7599-b192-e947be74f808.png"></a></p>

<p><a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/" rel="nofollow noopener" target="_blank">scikit-learn アルゴリズム・チートシート</a></p>

<p>【対象者】機械学習を使用したい方、初心者向けの機械学習本を読んで少し実装してみた方<br>
scikit-learnの説明は英語で分かりにくいし、実装例もシンプルでなくて、よく分からんという方</p>

<p>【得られるもの】模擬データを用いて、各手法を使用したミニマム・シンプルなプログラムが実装できるようになります。<br>
アルゴリズムの詳細な数式は理解できませんが、だいたい何をやりたいのか、意図と心、エッセンスが分かります。</p>

<p>アルゴリズムマップの手法をひとつずつ実装・解説します。<br>
コード全部載せるのは大変なので、各内容の詳細はブログに掲載しております。</p>

<p>詳細な数式を掲載しているのではなく、初学者がエッセンスをつかめるような説明を目指して記事を執筆しました。</p>

<h1>
<span id="クラス分類問題教師あり学習" class="fragment"></span><a href="#%E3%82%AF%E3%83%A9%E3%82%B9%E5%88%86%E9%A1%9E%E5%95%8F%E9%A1%8C%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92"><i class="fa fa-link"></i></a>クラス分類問題・教師あり学習</h1>

<p><a href="https://camo.qiitausercontent.com/e2b3b5ba5bed11dd69427f8ece7b17e0b6394ff7/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f32656339303737302d643966632d313333332d636365302d3664616261386663653838362e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/e2b3b5ba5bed11dd69427f8ece7b17e0b6394ff7/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f32656339303737302d643966632d313333332d636365302d3664616261386663653838362e706e67" alt="ml1.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/2ec90770-d9fc-1333-cce0-6daba8fce886.png"></a></p>

<p>まずはじめにWindows PCにJupyter Notebookとscikit-learnを導入し、Pythonで機械学習する環境を整えます。<br>
そして、「大規模なデータ」の線形なクラス分類手法であるSGD（stochastic gradient descent）というアルゴリズムを実装・解説します。<br>
模擬データとしては、有名なワインの分類データを使用します。<br>
ワインの色や成分から、ブドウの品種を分類します。<br>
<a href="http://neuro-educator.com/mlearn1/" rel="nofollow noopener" target="_blank">SGD｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>大規模なデータの非線形なクラス分類手法であるカーネル近似を実装・解説します。<br>
模擬データとしては、XORの関係を持つデータを使用します。<br>
通常のSGDでは、うまく分類できないですが、カーネル近似を使用することでうまくいくことを紹介します。<br>
<a href="http://neuro-educator.com/ml2/" rel="nofollow noopener" target="_blank">カーネル近似｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>データの線形なクラス分類手法であるLinear SVC（SVM classification）を実装・解説します。<br>
SVMの心や、通常の誤差を用いるSGDなどとの違いを説明します。<br>
模擬データとしては、ワインの分類を使用します。 <br>
<a href="http://neuro-educator.com/ml3/" rel="nofollow noopener" target="_blank">Linear SVC｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>データの非線形なクラス分類手法であるk近傍法を実装・解説します。<br>
模擬データとしては、XORの分類を使用します。 <br>
<a href="http://neuro-educator.com/ml4/" rel="nofollow noopener" target="_blank">k近傍法｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>データの非線形なクラス分類手法であるkernel SVCを実装・解説します。<br>
模擬データとしては、XORの分類を使用します。 <br>
<a href="http://neuro-educator.com/ml5/" rel="nofollow noopener" target="_blank">kernel SVC｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>データの非線形なクラス分類手法であるEnsemble Classificationを実装・解説します。<br>
ここではランダムフォレストをベースとします。<br>
模擬データとしては、XORの分類を使用します。 <br>
<a href="http://neuro-educator.com/ml6/" rel="nofollow noopener" target="_blank">Ensemble Classification｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>ここまでは単純なデータを扱いましたが、次にテキストデータのクラス分類を行います。<br>
ivedoorニュースコーパスの記事を使用し、ニュース記事のカテゴリーをナイーブベイズにより分類する手法を実装します。<br>
テキストデータを扱ううえで重要な、形態素解析による分かち書き、BoW（Bag-of-Words）、ナイーブベイズを順番に実装・解説します。<br>
また、ナイーブって日本語では”うぶ”という意味ですが、「うぶなベイズ」ってどういう意味なのかなども説明します。<br>
<a href="http://neuro-educator.com/ml7/" rel="nofollow noopener" target="_blank">ナイーブベイズ｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>以上でクラス分類の実装は終了ですが、各アルゴリズムを実装するさいに重要なハイパーパラメータをグリッドサーチと呼ばれる手法で決定する方法を実装・解説します。<br>
<a href="http://neuro-educator.com/ml8/" rel="nofollow noopener" target="_blank">ハイパーパラメータをグリッドサーチ｜Python、scikit-learnで機械学習を実装</a></p>

<h1>
<span id="クラスタ分析教師なし学習" class="fragment"></span><a href="#%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E5%88%86%E6%9E%90%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%AD%A6%E7%BF%92"><i class="fa fa-link"></i></a>クラスタ分析・教師なし学習</h1>

<p><a href="https://camo.qiitausercontent.com/4c16de6d5bebbbaa1b75ab6ec5c5da13cc19e93a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f66663535643334642d336330362d346534352d313237332d3861633031313066336636632e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/4c16de6d5bebbbaa1b75ab6ec5c5da13cc19e93a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f66663535643334642d336330362d346534352d313237332d3861633031313066336636632e706e67" alt="ml2.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/ff55d34d-3c06-4e45-1273-8ac0110f3f6c.png"></a></p>

<p>つぎに教師なし学習であるクラスタ分析を行います。</p>

<p>まずはじめに、いくつのクラスタに分かれるのかが事前にわかっている状況でのクラスタ分析手法を紹介します。</p>

<p>基本的な手法であるKMeansを実装・解説します。<br>
模擬データとしてはワインのデータを使用します。<br>
ワインデータには教師ラベル（どの品種か）が本当はわかりますが、これがなかったとしたらどのようにクラスター分けされるのかを確認します。<br>
<a href="http://neuro-educator.com/ml9/" rel="nofollow noopener" target="_blank">KMeans｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>KMeansではうまくクラスタ分けできないような非線形な構造を持ったデータに対するクラスタ分けを行います。<br>
ここではスペクトラルクラスタリングを実装・解説します。<br>
模擬データとしてはムーンと呼ばれる三日月が2つあるデータを使用します。<br>
KMeansではうまくクラスタ分けできず、スペクトラルクラスタリングで分けられることを確認します。<br>
<a href="http://neuro-educator.com/ml10/" rel="nofollow noopener" target="_blank">スペクトラルクラスタリング｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>非線形な構造を持ったデータに対するクラスタ分けである、GMM（Gaussian mixture models）を紹介します。<br>
模擬データとしてはムーンを使用します。<br>
<a href="http://neuro-educator.com/ml11/" rel="nofollow noopener" target="_blank">GMM｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>つぎに、いくつのクラスタに分かれるのか分からない状況でのクラスタ分類を行います。</p>

<p>MeanShiftと呼ばれる手法を実装・解説します。<br>
模擬データとしてはムーンを使用します。<br>
<a href="http://neuro-educator.com/ml12/" rel="nofollow noopener" target="_blank">MeanShift｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>VBGMM（Variational Bayesian Gaussian Mixture）と呼ばれる手法を実装・解説します。<br>
模擬データとしてはワインのデータを使用します。<br>
<a href="http://neuro-educator.com/ml13/" rel="nofollow noopener" target="_blank">VBGMM｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<h1>
<span id="回帰分析" class="fragment"></span><a href="#%E5%9B%9E%E5%B8%B0%E5%88%86%E6%9E%90"><i class="fa fa-link"></i></a>回帰分析</h1>

<p><a href="https://camo.qiitausercontent.com/d78f8d8a5c817b29cb6b5fd58ca2918a4ae61c69/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f66636332353638322d313539372d376430302d663033302d6535666332613030636366312e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/d78f8d8a5c817b29cb6b5fd58ca2918a4ae61c69/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f66636332353638322d313539372d376430302d663033302d6535666332613030636366312e706e67" alt="ml3.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/fcc25682-1597-7d00-f030-e5fc2a00ccf1.png"></a></p>

<p>目的変数y（非説明変数）が連続値であるときに、説明変数xを用いてyを近似的に求める手法（回帰）を実装・解説します。</p>

<p>もっとも基本的なSGD回帰分析（stochastic gradient descent Regressor）を実装・解説します。<br>
模擬データとしては、ボストン住宅価格と呼ばれるデータを使用します。<br>
部屋数や大通りへの距離、地域の犯罪発生率など13次元の説明変数xから、住宅価格yを求める式を立てます。<br>
<a href="http://neuro-educator.com/ml14/" rel="nofollow noopener" target="_blank">SGD回帰｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>つぎに、説明変数xの一部が重要な場合の回帰分析を行います。</p>

<p>LASSO回帰を実装・解説します。<br>
LASSO回帰はL1ノルムの正則化になります。<br>
L1ノルムで正則化することで、一部の次元だけが重要視される点などを解説します。<br>
模擬データとしては、ボストン住宅価格を使用します。<br>
<a href="http://neuro-educator.com/ml15/" rel="nofollow noopener" target="_blank">LASSO回帰｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>ElasticNet回帰を実装・解説します。<br>
ElasticNet回帰はL1ノルムとL2ノルムの正則化を両方使用します。<br>
模擬データとしては、ボストン住宅価格を使用します。<br>
<a href="http://neuro-educator.com/ml17/" rel="nofollow noopener" target="_blank">ElasticNet回帰｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>つぎに、説明変数xのすべてが重要な場合の回帰分析を行います。</p>

<p>説明変数xのすべてが重要な場合の回帰分析であるRidge回帰を実装・解説します。<br>
Ridge回帰はL2ノルムの正則化を使用します。<br>
L2ノルムで正則化することで、一部の次元だけが重要視されにくい点などを解説します。<br>
模擬データとしては、ボストン住宅価格を使用します。<br>
<a href="http://neuro-educator.com/ml16/" rel="nofollow noopener" target="_blank">Ridge回帰｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>Linear SVR回帰を実装・解説します。<br>
回帰分析にSVMを使用するイメージがわきませんが、どのように使用されているのかを解説します。<br>
模擬データとしては、ボストン住宅価格を使用します。<br>
<a href="http://neuro-educator.com/ml18/" rel="nofollow noopener" target="_blank">Linear SVR回帰｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>ここまでは直線で近似してきました。つぎに、非線形な回帰分析を行います。</p>

<p>rbfカーネル（ガウスカーネル）を使用したSVR回帰を実装・解説します。<br>
模擬データとしてはsin波を使用します。<br>
<a href="http://neuro-educator.com/ml19/" rel="nofollow noopener" target="_blank">SVR回帰｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>Ensemble Rigressorを使用した回帰分析を実装・解説します。<br>
ここでは、決定木をベースとしたBaggingRegressorを実装します。<br>
模擬データとしてはsin波を使用します。<br>
<a href="http://neuro-educator.com/ml20/" rel="nofollow noopener" target="_blank">Ensemble Rigressor｜Python、scikit-learnで機械学習を実装</a>  </p>

<h1>
<span id="データの次元圧縮" class="fragment"></span><a href="#%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E6%AC%A1%E5%85%83%E5%9C%A7%E7%B8%AE"><i class="fa fa-link"></i></a>データの次元圧縮</h1>

<p><a href="https://camo.qiitausercontent.com/3a75c053970e19760a9f13bd6238f8c0039af9a6/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65306635373936612d636236392d393561652d303439612d6334343033613534623064302e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/3a75c053970e19760a9f13bd6238f8c0039af9a6/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65306635373936612d636236392d393561652d303439612d6334343033613534623064302e706e67" alt="ml4.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/e0f5796a-cb69-95ae-049a-c4403a54b0d0.png"></a></p>

<p>多次元データの次元を圧縮する手法を解説します。</p>

<p>もっとも基本的なPCAについて実装・解説します。<br>
模擬データとしては、ボストン住宅価格を使用します。<br>
<a href="http://neuro-educator.com/ml21/" rel="nofollow noopener" target="_blank">PCA｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>つづいて非線形な構造をもつデータに対する次元圧縮手法を解説します。</p>

<p>カーネルトリックを利用した、Kernel-PCAを実装・解説します。<br>
模擬データとしてはムーンデータを使用します。<br>
ムーンは2次元データなので、次元は2次元のまま下がっていませんが、よりデータ構造が見やすいように新たな<br>
軸が作られることを確認します。<br>
<a href="http://neuro-educator.com/ml22/" rel="nofollow noopener" target="_blank">カーネルPCA｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>SpectralEmbeddingによる次元圧縮を実装・解説します。<br>
模擬データにはムーンデータを使用します。<br>
<a href="http://neuro-educator.com/ml23/" rel="nofollow noopener" target="_blank">SpectralEmbedding｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>多様体理論を利用したIsomapによる次元圧縮を実装・解説します。<br>
多様体というとなんだか難しそうですが、要は何をしているのかを説明します。<br>
模擬データにはムーンデータを使用します。<br>
<a href="http://neuro-educator.com/ml24/" rel="nofollow noopener" target="_blank">Isomap｜Python、scikit-learnで機械学習を実装</a></p>

<hr>

<p>多様体理論を利用したLLE（LocallyLinearEmbedding）による次元圧縮を実装・解説します。<br>
模擬データにはムーンデータを使用します。<br>
<a href="http://neuro-educator.com/ml25/" rel="nofollow noopener" target="_blank">LLE（LocallyLinearEmbedding）｜Python、scikit-learnで機械学習を実装</a> </p>

<p>以上でscikit-learnのアルゴリズムチートマップの全実装・解説となります。</p>

<h1>
<span id="おまけ" class="fragment"></span><a href="#%E3%81%8A%E3%81%BE%E3%81%91"><i class="fa fa-link"></i></a>おまけ</h1>

<p>最後におまけとして、初めてディープラーニングを使用して、ドラゴンボールの画像識別を行ってみました。</p>

<p><a href="http://neuro-educator.com/dbscouter/" rel="nofollow noopener" target="_blank">ディープラーニングを使ってドラゴンボールZのスカウターっぽいのを作ってみた (Python, Chainer, dlib, PyQt)</a></p>

<p><a href="https://camo.qiitausercontent.com/42c2b083cf051208f7e602bb915272678bda1007/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f35386532333361622d343366632d623636352d303365312d3735643435613362376565372e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/42c2b083cf051208f7e602bb915272678bda1007/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f35386532333361622d343366632d623636352d303365312d3735643435613362376565372e706e67" alt="scouter.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/58e233ab-43fc-b665-03e1-75d45a3b7ee7.png"></a></p>

<p>以上となります。</p>
</div>
	</td>
</tr>
</table>
<br />
<table border="1">
<tr>
	<td rowspan="3"><center>suguluさんの<br />4位</center></td>
	<td colspan="4">
		<kbd><i><img alt="いいね" width="16" height="16" src="../thumb-up-120px.png" /></i>95</kbd>
		<a target="_blank" href="https://qiita.com/sugulu/items/5c1b03cd445f27fd3e28">のび太と学ぶ「機械学習」【第3話】完成：FX予測プログラム（線形回帰版）</a>
	</td>
</tr>
<tr>
	<td style="width:100px;"><center>投稿日時</center></td>
	<td style="width:200px;"><center>投稿者</center></td>
	<td style="width:150px;"><center>タグ</center></td>
	<td style="width:350px;"><center>本文</center></td>
</tr>
<tr>
	<td style="width:100px;">
		<!--投稿日時--><center>2017-11-13 05:32:04</center>
	</td>
	<td style="width:200px;">
		@sugulu<br />(都内IT企業 所属)<br><img width="80" height="80" src="https://qiita-image-store.s3.amazonaws.com/0/191401/profile-images/1509357969">
	</td>
	<td style="width:150px;">
		<!--タグ-->
		<center><b>[Python]</b> <b>[初心者]</b> <b>[機械学習]</b> <b>[DeepLearning]</b> <b>[FX]</b> </center>
	</td>
	<td style="width:350px;">
		<!--本文-->
		<div style="width:350px;height:150px;overflow-x:hidden;overflow-y:scroll;"><p>本シリーズでは、「FXの予測プログラム」を作りながら、機械学習・ディープラーニングを解説します。<br>
（注）のび太くんは有名漫画のキャラとは一切関係ありません。</p>

<p>この物語は、大学生のび太くんが、家庭教師のすぐるさんから機械学習を学び、将来D-mind社を起業して、汎用人工知能搭載型ロボットを誕生させるまでの記録です。</p>

<h1>
<span id="これまでのお話" class="fragment"></span><a href="#%E3%81%93%E3%82%8C%E3%81%BE%E3%81%A7%E3%81%AE%E3%81%8A%E8%A9%B1"><i class="fa fa-link"></i></a>これまでのお話</h1>

<p>のび太くんはiPhone Xを買うために、FX予測プログラムを作成しようと、すぐるさんから機械学習を学び始めました。</p>

<p><a href="https://qiita.com/sugulu/items/45e3cfaa78e5f13d9389" id="reference-923f60d3e677bf0e11f8">第1話：のび太、if文でFXを予想する</a><br>
<a href="https://qiita.com/sugulu/items/b6f891e060beab94b99b" id="reference-6404199d342bcae048f8">第2話：のび太、線形回帰を学ぶ</a></p>

<p>今回は、前話で学習した線形回帰を利用して、ついにのび太くんのFX予想プログラムが完成します。</p>

<p>（注）本ページを参考にして生じた損益については責任を持てませんので、あしからず。</p>

<h1>
<span id="第3話のび太完成fx予測プログラム線形回帰版" class="fragment"></span><a href="#%E7%AC%AC3%E8%A9%B1%E3%81%AE%E3%81%B3%E5%A4%AA%E5%AE%8C%E6%88%90fx%E4%BA%88%E6%B8%AC%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0%E7%89%88"><i class="fa fa-link"></i></a>第3話：のび太、完成：FX予測プログラム（線形回帰版）</h1>

<p>さて、のび太くん、本日は「線形回帰」を利用したFX予測プログラムを作成しようか。</p>

<p>今日は突然本題から入るんだね。<br>
よしやるぞ！</p>

<p>ところで、線形回帰は覚えているかい？</p>

<p>もちろんさ。本を買って、復習もしたよ。<br>
学習データの$Y_{train}$と$X_{train}$の間にある線形モデル</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>y_p(t)=\mathbf{w}^T\mathbf{x}(t)+b
</pre></div></div>

<p>を求めて、その後テストデータで予測具合を確かめる。<br>
この線形モデルの係数、$\mathbf{w}=w_0～w_{N}$と$b$は、学習データの「予測の2乗誤差の和」</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>E(\mathbf{w}, b)=\sum_{t=0}^{M}\left [ {y_{train}(t)-y_p(t)} \right ]^2=\sum_{t=0}^{M}\left [ {y_{train}(t)-\left \{\mathbf{w}^T\mathbf{x}(t)+b  \right \}} \right ]^2
</pre></div></div>

<p>に対して、$E$が最小値となる値を、$\mathbf{w}$と$b$で$E(\mathbf{w}, b)$を偏微分して、少しずつ更新して求める。<br>
これで合っている？</p>

<p>すごいな、のび太くん。正解だよ。<br>
補足すると、実際の線形回帰は勾配降下法ではなく、一度の計算で$\mathbf{w}$と$b$を求めているけど、やりたいことは一緒だよ。<br>
あと前回言い忘れたけど、$Y$を被説明変数や目的変数と呼び、$X$を説明変数と呼ぶよ。</p>

<p>やった！じゃあ、今日は早速FXのデータでやろうよ！<br>
ツネ夫がiPhone Xをもう持っていて、自慢してくるんだよ・・・</p>

<p>よし、少しずつコードを作りながら、説明していこう。</p>

<h2>
<span id="実装その1" class="fragment"></span><a href="#%E5%AE%9F%E8%A3%85%E3%81%9D%E3%81%AE1"><i class="fa fa-link"></i></a>実装その1</h2>

<p>まず作りたいプログラムを整理しよう。</p>

<p>2007年～2016年のドル円の日足データを学習データとして使用し、<br>
翌日のドル円の終値が、当日の終値に対して、何円高いか（もしくは安いか）を予測するプログラムを作成するよ。</p>

<p>入力データには過去25日分のデータを使用する。</p>

<p>うん、25日分でだいたい1ヶ月のデータになるからだよね。</p>

<p>そうだね。<br>
このような、入力データから数値を予測する機械学習を「回帰」と呼ぶよ。</p>

<p>さあ、早速作っていこう。<br>
今回のコードは全部僕のGitHubにおいて置くね。<br>
<a href="https://github.com/Nobita-FX/Nobita-FX/blob/master/program/NobitaFX_3_linear_regression_1.ipynb" rel="nofollow noopener" target="_blank">本記事の掲載コードはこちら</a></p>

<p>最初はいつものおまじないゾーンだ。<br>
って言っても、これから使うライブラリを宣言しているだけだよ。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># import関連</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span> <span class="c1"># 実行上問題ない注意は非表示にする</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div></div>

<p>次に、ドル円の為替データを読み込むよ。<br>
フォルダの場所はのび太くんが為替データを置いた場所を指定してね。<br>
為替データは、GitHubに置いているよ。<br>
<a href="https://github.com/Nobita-FX/Nobita-FX/tree/master/data" rel="nofollow noopener" target="_blank">データはこちら</a></p>

<p>※為替データは、フリーでログイン不要のデータサイトStooqから取得しました。<br>
<a href="https://stooq.com/q/d/?s=usdjpy&amp;c=0" rel="nofollow noopener" target="_blank">Stooq</a></p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># dataフォルダの場所を各自指定してください</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="s2">"./"</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_dir</span> <span class="o">+</span> <span class="s2">"USDJPY_1997_2017.csv"</span><span class="p">)</span> <span class="c1"># FXデータの読み込み（データは同じリポジトリのdataフォルダに入っています）</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span> <span class="c1"># データの概要を見てみます</span>
</pre></div></div>

<p><a href="https://camo.qiitausercontent.com/e8462b398c917662b5ec97d538e4950ae4ace3a3/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f38393732336339312d646463342d656230632d636433312d6131613038613364663931352e6a706567" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/e8462b398c917662b5ec97d538e4950ae4ace3a3/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f38393732336339312d646463342d656230632d636433312d6131613038613364663931352e6a706567" alt="nobitaFX3_8.JPG" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/89723c91-ddc4-eb0c-cd31-a1a08a3df915.jpeg"></a></p>

<p>つぎに、読み込んだデータをNumPyの形式に変換するよ。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># pandasのDataFrameのままでは、扱いにくい+実行速度が遅いので、numpyに変換して処理します</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div></div>

<p>どうして、numpyに変換するの？</p>

<p>pandasはデータ形式としては扱いやすいけど、数値計算が遅いし、有効桁数も少ないからだよ。<br>
numpyは数値計算に適したデータ形式なんだ。</p>

<p>次に説明変数$X$を作成するよ。<br>
Xは、行ごとに、当日から24日前までの終値を列方向に並べた25列の行列になるよ。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 説明変数となる行列Xを作成します</span>
<span class="n">day_ago</span> <span class="o">=</span> <span class="mi">25</span> <span class="c1"># 何日前までのデータを使用するのかを設定</span>
<span class="n">num_sihyou</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># 終値</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span> <span class="n">day_ago</span><span class="o">*</span><span class="n">num_sihyou</span><span class="p">))</span> 

<span class="c1"># 終値をfor文でday_ago日前まで一気に追加する</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">day_ago</span><span class="p">):</span>
    <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">data2</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)</span><span class="o">-</span><span class="n">i</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
</pre></div></div>

<p>なんだか、難しいfor文でよく分からないよ。<br>
何をしているの？</p>

<p>data2の列4は当日終値だよ。<br>
これは過去25日分のデータを横向きに並べているんだよ。<br>
一度Xをpandasに戻して、中身を見てみようか。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># Xの確認です</span>
<span class="n">data_show</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">data_show</span>
</pre></div></div>

<p><a href="https://camo.qiitausercontent.com/a77c9ed3908ec75469ef673467564c325489b980/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f37333363313265652d616631302d303463392d616337612d3039623065616234663064612e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/a77c9ed3908ec75469ef673467564c325489b980/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f37333363313265652d616631302d303463392d616337612d3039623065616234663064612e706e67" alt="nobitaFX_3_1.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/733c12ee-af10-04c9-ac7a-09b0eab4f0da.png"></a></p>

<p>当日の終値が、1行下の翌日には、1日前の終値に入っているんだね。<br>
なるほど。<br>
分かった。じゃあ、同じように目的変数（被説明変数）$Y$も作成してみるよ。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 被説明変数となる Y = pre_day後の終値-当日終値 を作成します</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">))</span>

<span class="c1"># 何日後を値段の差を予測するのか決めます</span>
<span class="n">pre_day</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span><span class="o">-</span><span class="n">pre_day</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">pre_day</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">-</span><span class="n">pre_day</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</pre></div></div>

<p>うん、のび太くん、OKだ。良い感じだね。<br>
ここから重要ポイントに入るね。<br>
データを「正規化」するよ。</p>

<p>正規化？</p>

<p>正規化にもいろいろあるけど、一般には、各データに対して、平均を引き算し、標準偏差で割って<br>
平均0、標準偏差1になるように変換することだよ。</p>

<p>う～ん、難しい話だね。</p>

<p>要は、1ドル110円台のときのデータも、100円台のときのデータも同じように扱えるようにしたいんだよ。<br>
ただ、FXの為替データの場合は、個人的には標準偏差で割らないほうが良いと感じている。</p>

<p>どうして？</p>

<p>標準偏差のような変動の上下幅の大きさは、為替では重要な情報になるからだよ。</p>

<p>よく分からないや・・・<br>
とりあえず、平均を引き算するんだね。<br>
よしコードを書いてみるよ。</p>

<p>いやここは難しいから、僕が書こう。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 【重要】X, Yを正規化します</span>
<span class="n">original_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># コピーするときは、そのままイコールではダメ</span>
<span class="n">tmp_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">day_ago</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
    <span class="n">tmp_mean</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">original_X</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">day_ago</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># 25日分の平均値</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span> 
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">tmp_mean</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="c1"># Xを正規化</span>
    <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span>  <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># X同士の引き算しているので、Yはそのまま</span>
</pre></div></div>

<p>ただ、引き算するだけなのに、なんかややこしいね。<br>
np.copy(X)って何？</p>

<p>numpyは単純に<br>
original_X = X<br>
ってしてしまうと、Xが変化したときに、original_Xも変化してしまうんだよ。<br>
それを避けるために、np.copyを使っているよ。</p>

<p>どうしてoriginal_Xまで変わるの？</p>

<p>numpyはポインタ参照で値を受け渡すからだよ。</p>

<p>うん、難しいや。<br>
とりあえず、単純に = で変数をコピーできないことは分かったよ。<br>
正規化で$Y$はそのままなんだね。</p>

<p>$Y$は$X$の引き算になっているから、平均値を引く操作はそこでキャンセルされるからだよ。</p>

<p>この正規化しだいで、予測の性能が大きく変わるから、とても重要なんだよ。</p>

<p>さて、次に$X$と$Y$を学習データとテストデータに分けてみようか。<br>
ここはのび太くん、書いてくれるかな。<br>
ただし、最初のほうは使わないで、行200から使ってくれるかな。</p>

<p>うん、えーと・・・<br>
カタカタ・・・</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># XとYを学習データとテストデータ(2017年～)に分ける</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">200</span><span class="p">:</span><span class="mi">5193</span><span class="p">,:]</span> <span class="c1"># 次のプログラムで200日平均を使うので、それ以降を学習データに使用します</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="mi">200</span><span class="p">:</span><span class="mi">5193</span><span class="p">]</span> 

<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">5193</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">-</span><span class="n">pre_day</span><span class="p">,:]</span> 
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="mi">5193</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span><span class="o">-</span><span class="n">pre_day</span><span class="p">]</span>
</pre></div></div>

<p>OKだよ。のび太くん。<br>
行5193からが2017年だからね。</p>

<p>でも、どうして行200からを使用したの？</p>

<p>これはちょっと先を見越しての話だから、とりあえず保留でいいかな。</p>

<p>怪しいな～。分かったよ。<br>
学習データとテストデータができたら、前回の線形回帰の練習問題と同じだね。<br>
前と同じようにやってみるよ。<br>
<a href="https://qiita.com/sugulu/items/b6f891e060beab94b99b">第2話：のび太、線形回帰を学ぶ</a></p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 学習データを使用して、線形回帰モデルを作成します</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span> <span class="c1"># scikit-learnライブラリの関数を使用します</span>
<span class="n">linear_reg_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>

<span class="n">linear_reg_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span> <span class="c1"># モデルに対して、学習データをフィットさせ係数を学習させます</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"回帰式モデルの係数"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">linear_reg_model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span> 
<span class="k">print</span><span class="p">(</span><span class="n">linear_reg_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
</pre></div></div>

<p>のび太くん、OKだよ。<br>
じゃあテストデータで予測してみよう。<br>
前回のコードに加えて、予測の正答率も計算してくれるかな。</p>

<p>えっと、<br>
カタカタ・・・</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 2017年のデータで予想し、グラフで予測具合を見る</span>

<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">linear_reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1"># 予測する</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">)</span> <span class="c1"># 予測</span>
<span class="n">result</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Y_pred'</span><span class="p">]</span>
<span class="n">result</span><span class="p">[</span><span class="s1">'Y_test'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y_test</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">'darkgrid'</span><span class="p">)</span> 
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">'Y_pred'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">'Y_test'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">result</span><span class="p">)</span> <span class="c1">#plotする</span>


<span class="c1"># 正答率を計算</span>
<span class="n">success_num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">success_num</span><span class="o">+=</span><span class="mi">1</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"予測日数："</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">))</span><span class="o">+</span><span class="s2">"、正答日数："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">success_num</span><span class="p">)</span><span class="o">+</span><span class="s2">"正答率："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">success_num</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</pre></div></div>

<p>予測日数：214、正答日数：103、正答率：48.13084112149533</p>

<p><a href="https://camo.qiitausercontent.com/7c63de60a2264603fcd26e2dec4c84f17d8d36d0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65353865393838312d626337622d656535382d623337652d6365633135633636353865612e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/7c63de60a2264603fcd26e2dec4c84f17d8d36d0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65353865393838312d626337622d656535382d623337652d6365633135633636353865612e706e67" alt="nobitaFX_3_2.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/e58e9881-bc7b-ee58-b37e-cec15c6658ea.png"></a></p>

<p>あれ、前回と違って、近似直線の周りに薄い青の塗りが出てきたよ。</p>

<p>これは、regplotでデータ数が多いと、自動で95%範囲を表示してくれるんだよ。</p>

<p>95%範囲？</p>

<p>近似直線は95%の確率でこの薄い青色範囲内にありますよってことを示している。<br>
正答率を見ると、48%だから、適当に上がるか下がるかを予測した場合（50%）とほぼ変わらないね。<br>
テスト期間（2017年1月初日から10月末まで）の予測結果の合計と変化を見てみようか。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 2017年の予測結果の合計を計算ーーーーーーーーー</span>
<span class="c1"># 前々日終値に比べて前日終値が高い場合は、買いとする</span>
<span class="n">sum_2017</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">)):</span> <span class="c1"># len()で要素数を取得しています</span>
    <span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">sum_2017</span> <span class="o">+=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sum_2017</span> <span class="o">-=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"2017年の利益合計：</span><span class="si">%1.3lf</span><span class="s2">"</span> <span class="o">%</span><span class="n">sum_2017</span><span class="p">)</span> 


<span class="c1"># 予測結果の総和グラフを描くーーーーーーーーー</span>
<span class="n">total_return</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">))</span>

<span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span><span class="mi">0</span><span class="p">:</span> <span class="c1"># 2017年の初日を格納</span>
    <span class="n">total_return</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">total_return</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)):</span> <span class="c1"># 2017年の2日以降を格納</span>
    <span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">total_return</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_return</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">total_return</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_return</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">total_return</span><span class="p">)</span>
</pre></div></div>

<p>2017年の利益合計：0.245</p>

<p><a href="https://camo.qiitausercontent.com/69eccfe1005b535647b837cea5f5646964e2f6fa/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f64616461653134612d653336652d393836392d313861372d3434383438643365333462302e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/69eccfe1005b535647b837cea5f5646964e2f6fa/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f64616461653134612d653336652d393836392d313861372d3434383438643365333462302e706e67" alt="nobitaFX_3_3.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/dadae14a-e36e-9869-18a7-44848d3e34b0.png"></a></p>

<p>すぐるさん、0.245って少しプラスになったけど、だめだ～。<br>
グラフを見ると、50日目くらいで14まで増えたのに、後半すごい減っているよ・・・</p>

<p>そうだね。<br>
正答率も48%だし、やっぱりうまくいっていないね。</p>

<p>って！！すぐるさん、教えてくれるって言ったじゃないか！</p>

<p>あとちょっとだよ、のび太くん。<br>
どうしてうまくいかないのか、考えるんだ。</p>

<p>・<br>
・<br>
・</p>

<h2>
<span id="実装その2" class="fragment"></span><a href="#%E5%AE%9F%E8%A3%85%E3%81%9D%E3%81%AE2"><i class="fa fa-link"></i></a>実装その2</h2>

<p>う～ん・・・<br>
分からないよ。</p>

<p>そうだね。一緒に考えてみようか。<br>
機械学習の性能は、</p>

<p>(機械学習の性能) = (入力データ) × (モデルの表現力)　</p>

<p>で概念的に表されるよ。</p>

<p>今回は線形モデルというとても単純なモデルを使用しているから、（モデルの表現力）がとても低いんだ。<br>
例えば、最近はやりのディープラーニングの場合は（モデルの表現力）が高いから、入力データが単純でも良い予測ができる。</p>

<p>ってことは、線形モデルは（モデルの表現力）が低いから、もっと良い入力データを入れれば良くなるってこと？</p>

<p>単純にそうとは言い切れないけど、多くの場合はその通りだよ、のび太くん。</p>

<p>でも、これ以上何を入力データに加えればよいんだろう・・・？<br>
分からないよ。</p>

<p>そうだね。いろいろな選択肢があるけど、<br>
FXで頻繁に使用されるテクニカル指標を使うのが良いよ。<br>
なぜならそのテクニカル指標を使って多くの人が判断しているから、将来の為替の結果に影響するんだ。</p>

<p>どんなテクニカル指標があるの？</p>

<p>いろいろあるけど、今回は移動平均線と一目均衡表、ボリンジャーバンドの3種類を使用しよう。</p>

<p>移動平均線は一定期間のただの平均だよ。<br>
一目均衡表は期間内の最高値と最安値を使った指標だよ。<br>
ボリンジャーバンドは一定期間の標準偏差を使った指標だよ。</p>

<p>今回追加するテクニカル指標のパラメータには代表的な数値を使用しよう。<br>
FXのお話ではなく、機械学習のお話だから、詳しくは自分で調べてみてね。<br>
<a href="https://www.gaitameonline.com/academy_chart.jsp" rel="nofollow noopener" target="_blank">例えば: 外為オンラインのFX実戦チャート術ページ</a></p>

<p>よし、これらを加えたプログラムに修正してみよう。</p>

<p>修正コードも僕のGitHubにおいて置くね。<br>
<a href="https://github.com/Nobita-FX/Nobita-FX/blob/master/program/NobitaFX_3_linear_regression_2.ipynb" rel="nofollow noopener" target="_blank">本記事の掲載コードはこちら</a></p>

<p>まず、5日の移動平均線を追加しよう。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 5日移動平均を追加します</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">data2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span><span class="mi">1</span><span class="p">))]</span> <span class="c1"># 列の追加</span>
<span class="n">ave_day</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ave_day</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)):</span>
    <span class="n">tmp</span> <span class="o">=</span><span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">ave_day</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="c1"># pythonは0番目からindexが始まります</span>
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
</pre></div></div>

<p>pythonはスライスといって、行列の要素を 0 : i みたいにindexで指定したときと、ダイレクトにindexの i って指定したときで、挙動が異なるから、気をつけてね。</p>

<p>例えば以下のプログラムの出力は</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>
<span class="n">i</span><span class="o">=</span><span class="mi">4</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div></div>

<p>[10, 20, 30, 40]<br>
50</p>

<p>ってなるよ。</p>

<p>指標を作るときに未来の情報がXに入っちゃうと、予測できて当然になるから気をつけてね。<br>
同じように25, 75, 200日の移動平均線を追加してみて。</p>

<p>うん、カタカタ・・・</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 25日移動平均を追加します</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">data2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span><span class="mi">1</span><span class="p">))]</span>
<span class="n">ave_day</span> <span class="o">=</span> <span class="mi">25</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ave_day</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)):</span>
    <span class="n">tmp</span> <span class="o">=</span><span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">ave_day</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>

<span class="c1"># 75日移動平均を追加します</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">data2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span><span class="mi">1</span><span class="p">))]</span> <span class="c1"># 列の追加</span>
<span class="n">ave_day</span> <span class="o">=</span> <span class="mi">75</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ave_day</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)):</span>
    <span class="n">tmp</span> <span class="o">=</span><span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">ave_day</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">7</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>

<span class="c1"># 200日移動平均を追加します</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">data2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span><span class="mi">1</span><span class="p">))]</span> <span class="c1"># 列の追加</span>
<span class="n">ave_day</span> <span class="o">=</span> <span class="mi">200</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ave_day</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)):</span>
    <span class="n">tmp</span> <span class="o">=</span><span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">ave_day</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">8</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
</pre></div></div>

<p>次は一目均衡表とボリンジャーバンドを足すね。<br>
ここは僕が書こう</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 一目均衡表を追加します (9,26, 52) </span>
<span class="n">para1</span> <span class="o">=</span><span class="mi">9</span>
<span class="n">para2</span> <span class="o">=</span> <span class="mi">26</span>
<span class="n">para3</span> <span class="o">=</span> <span class="mi">52</span>

<span class="c1"># 転換線 = （過去(para1)日間の高値 + 安値） ÷ 2</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">data2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span><span class="mi">1</span><span class="p">))]</span> <span class="c1"># 列の追加</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">para1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)):</span>
    <span class="n">tmp_high</span> <span class="o">=</span><span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">para1</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">tmp_low</span> <span class="o">=</span><span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">para1</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">9</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tmp_high</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">tmp_low</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span> 

<span class="c1"># 基準線 = （過去(para2)日間の高値 + 安値） ÷ 2</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">data2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span><span class="mi">1</span><span class="p">))]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">para2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)):</span>
    <span class="n">tmp_high</span> <span class="o">=</span><span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">para2</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">tmp_low</span> <span class="o">=</span><span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">para2</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tmp_high</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">tmp_low</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span> 

<span class="c1"># 先行スパン1 = ｛ （転換値+基準値） ÷ 2 ｝を(para2)日先にずらしたもの</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">data2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span><span class="mi">1</span><span class="p">))]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)</span><span class="o">-</span><span class="n">para2</span><span class="p">):</span>
    <span class="n">tmp</span> <span class="o">=</span><span class="p">(</span><span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">9</span><span class="p">]</span> <span class="o">+</span> <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span> 
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">para2</span><span class="p">,</span><span class="mi">11</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span>


<span class="c1"># 先行スパン2 = ｛ （過去(para3)日間の高値+安値） ÷ 2 ｝を(para2)日先にずらしたもの</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">data2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span><span class="mi">1</span><span class="p">))]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">para3</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)</span><span class="o">-</span><span class="n">para2</span><span class="p">):</span>
    <span class="n">tmp_high</span> <span class="o">=</span><span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">para3</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">tmp_low</span> <span class="o">=</span><span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">para3</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">para2</span><span class="p">,</span><span class="mi">12</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tmp_high</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">tmp_low</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span> 
</pre></div></div>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 25日ボリンジャーバンド（±1, 2シグマ）を追加します</span>
<span class="n">parab</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">data2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">data2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span><span class="mi">4</span><span class="p">))]</span> <span class="c1"># 列の追加</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">parab</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)):</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">parab</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">13</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.0</span><span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span> 
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">14</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span><span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span> 
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">15</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span> <span class="o">+</span> <span class="mf">2.0</span><span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span> 
    <span class="n">data2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">16</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span> <span class="o">-</span> <span class="mf">2.0</span><span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span> 
</pre></div></div>

<p>これで最初は終値だけから予測していたのが、<br>
終値1本、移動平均線4本、一目均衡表4本、ボリンジャー4本の合計13本になったよ。</p>

<p>すぐるさん、ということは13×25で、え～っと、<br>
325個の入力データで線形モデルを作るってことだね。<br>
ここで200日移動平均線を使用するから、最初のプログラムも200日以降を学習データにしたんだね。</p>

<p>その通りだよ。<br>
じゃあ最初と同じように、各日ごとに325個が列方向に並ぶように変換してもらえるかな。</p>

<p>よし、カタカタ・・・</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 説明変数となる行列Xを作成します</span>
<span class="n">day_ago</span> <span class="o">=</span> <span class="mi">25</span> <span class="c1"># 何日前までのデータを使用するのかを設定</span>
<span class="n">num_sihyou</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">+</span><span class="mi">4</span> <span class="c1"># 終値1本、MVave4本、itimoku4本、ボリンジャー4本</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span> <span class="n">day_ago</span><span class="o">*</span><span class="n">num_sihyou</span><span class="p">))</span> 

<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_sihyou</span><span class="p">):</span> <span class="c1"># 日にちごとに横向きに並べる</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">day_ago</span><span class="p">):</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">),</span><span class="n">day_ago</span><span class="o">*</span><span class="n">s</span><span class="o">+</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">data2</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">data2</span><span class="p">)</span><span class="o">-</span><span class="n">i</span><span class="p">,</span><span class="n">s</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span>
</pre></div></div>

<p>あとは1回目と同じように線形回帰を実行してもらえるかな。</p>

<p>うん、カタカタ・・・</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 2017年のデータで予想し、グラフで予測具合を見る</span>

<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">linear_reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1"># 予測する</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">)</span> <span class="c1"># 予測</span>
<span class="n">result</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Y_pred'</span><span class="p">]</span>
<span class="n">result</span><span class="p">[</span><span class="s1">'Y_test'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y_test</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">'darkgrid'</span><span class="p">)</span> 
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">'Y_pred'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">'Y_test'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">result</span><span class="p">)</span> <span class="c1">#plotする</span>


<span class="c1"># 正答率を計算</span>
<span class="n">success_num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">success_num</span><span class="o">+=</span><span class="mi">1</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"予測日数："</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">))</span><span class="o">+</span><span class="s2">"、正解日数："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">success_num</span><span class="p">)</span><span class="o">+</span><span class="s2">"正解率："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">success_num</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</pre></div></div>

<p>予測日数：214、正解日数：124、正解率：57.943925233644855</p>

<p><a href="https://camo.qiitausercontent.com/fa5b86e22832c62e339ba9f29bd78e88a134cdd2/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f31363364616263382d346264342d316438362d323033662d6363336661376665313133372e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/fa5b86e22832c62e339ba9f29bd78e88a134cdd2/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f31363364616263382d346264342d316438362d323033662d6363336661376665313133372e706e67" alt="nobitaFX_3_4.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/163dabc8-4bd4-1d86-203f-cc3fa7fe1137.png"></a></p>

<p>正答率が58%になったよ！！</p>

<p>うん、いい感じだね。</p>

<p>よし、合計も計算してみるぞ！</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 2017年の予測結果の合計を計算ーーーーーーーーー</span>
<span class="c1"># 前々日終値に比べて前日終値が高い場合は、買いとする</span>
<span class="n">sum_2017</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">)):</span> <span class="c1"># len()で要素数を取得しています</span>
    <span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">sum_2017</span> <span class="o">+=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sum_2017</span> <span class="o">-=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"2017年の利益合計：</span><span class="si">%1.3lf</span><span class="s2">"</span> <span class="o">%</span><span class="n">sum_2017</span><span class="p">)</span> 


<span class="c1"># 予測結果の総和グラフを描くーーーーーーーーー</span>
<span class="n">total_return</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">))</span>

<span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span><span class="mi">0</span><span class="p">:</span> <span class="c1"># 2017年の初日を格納</span>
    <span class="n">total_return</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">total_return</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">Y_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)):</span> <span class="c1"># 2017年の2日以降を格納</span>
    <span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">total_return</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_return</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">total_return</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_return</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">total_return</span><span class="p">)</span>
</pre></div></div>

<p>2017年の利益合計：22.089</p>

<p><a href="https://camo.qiitausercontent.com/c95fb1fc7dd0cfdb623eaaa8dc6db56566a6b54d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f66383438613636372d343734332d323063382d666565392d6234303263623433366235612e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/c95fb1fc7dd0cfdb623eaaa8dc6db56566a6b54d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f66383438613636372d343734332d323063382d666565392d6234303263623433366235612e706e67" alt="nobitaFX_3_5.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/f848a667-4743-20c8-fee9-b402cb436b5a.png"></a></p>

<p>すぐるさん、22でプラスになったよ！！<br>
やったー！！</p>

<p>ときおり減るときもあるけど、右肩上がりに増えているね。<br>
完成したね！これが線形回帰でのFXの予測プログラムだよ。<br>
<a href="https://github.com/Nobita-FX/Nobita-FX/blob/master/program/NobitaFX_3_linear_regression_2.ipynb" rel="nofollow noopener" target="_blank">本記事の掲載コードはこちら</a></p>

<p>+22ってことは、1000通貨だったら、22,000円、1万通貨なら22万円、10万通貨なら220万円儲かるってことだよね！<br>
でも、手数料は考えなくていいの？</p>

<p>1日1回の取引なら、手数料はほぼ無視できると考えていいよ。<br>
正確には手数料（本当はスプレッド）が0.3銭（0.003円）だとすると、約200日の取引で0.6くらいになるから、<br>
22-0.6 が、正しい値だよ。</p>

<p>よし！じゃあこれでiPhone Xが買えるぞ！！</p>

<p>・<br>
・<br>
・</p>

<p>あれ？すぐるさんもこの方法でFXをやっているの？</p>

<p>いや、僕は使っていないよ。<br>
これは、今回のび太くんの勉強のために作成したプログラムだからね。</p>

<p>のび太くん、線形回帰でFX（為替）を予測するプログラムなんて、20年以上も前からあるものなんだよ。<br>
今回の場合、正解率も60%弱だし。<br>
使えないことはないけど、最近の進歩した機械学習・AI技術を使えばもっと良いものが作れるよ。</p>

<p>たしかに最近、囲碁でAIがプロの方に勝ったニュースとか聞いたよ。<br>
それもFXに使えるの？</p>

<p>もちろんさ。<br>
囲碁のAIはディープラーニングや強化学習をベースにしているよ。<br>
あまりFXに固執して欲しくないけど、ディープラーニングや強化学習は、FXを含め様々な場面で使用できるよ。</p>

<p>でも、AIとか良く分からないや・・・<br>
難しいんでしょ？</p>

<p>大丈夫だよ。<br>
今回みたいに、ひとつずつゆっくり勉強して、実際に実装してみれば、<br>
のび太くんにもマスターできるよ。</p>

<p>僕が書いたAIのまとめ記事があるよ。<br>
昔に一気に書いたから微妙なところもあるけど、まずはこちらをおすすめするよ。</p>

<p><a href="https://camo.qiitausercontent.com/1867598280cf09b4aa362747dc44a7d37987cb30/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39623230383564352d386433342d333532622d633936652d3165383732396261373161312e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/1867598280cf09b4aa362747dc44a7d37987cb30/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39623230383564352d386433342d333532622d633936652d3165383732396261373161312e706e67" alt="nobitaFX_3_6.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/9b2085d5-8d34-352b-c96e-1e8729ba71a1.png"></a></p>

<p><a href="http://neuro-educator.com/?s=AI%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA" rel="nofollow noopener" target="_blank">AI関連の記事一覧：【図解：3分で解説】 人工知能・AIの歴史｜機械学習、ディープラーニングとは、等</a></p>

<p>うん、読んでみるよ。<br>
FXはともかく、もっとうまく未来を予測できるプログラムを作ってみたい！！</p>

<p>よし！！その意気だ、のび太くん。<br>
じゃあ次回は、別の機械学習手法を学びながら、FXプログラムをさらに改善していこう！</p>

<p>・<br>
・<br>
・</p>

<p>【完】のび太と学ぶ「機械学習」シーズン1 ～線形回帰～</p>

<p>次回、シーズン2へ続く（のか・・・？）</p>

<hr>

<p>以上、ご一読いただきありがとうございます。</p>

<p>「ここが分かりにくいよ～不明」など、気軽にコメントください。<br>
可能な限り、加筆修正して、分かりやすくしたいです。</p>

<p>また、記事やコードの間違え、改善点、不明点、感想なども<br>
気軽にコメントいただければ幸いです。</p>
</div>
	</td>
</tr>
</table>
<br />
<table border="1">
<tr>
	<td rowspan="3"><center>suguluさんの<br />5位</center></td>
	<td colspan="4">
		<kbd><i><img alt="いいね" width="16" height="16" src="../thumb-up-120px.png" /></i>69</kbd>
		<a target="_blank" href="https://qiita.com/sugulu/items/b6f891e060beab94b99b">のび太と学ぶ「機械学習」～FX予測プログラムを作成～【第2話】線形回帰モデル</a>
	</td>
</tr>
<tr>
	<td style="width:100px;"><center>投稿日時</center></td>
	<td style="width:200px;"><center>投稿者</center></td>
	<td style="width:150px;"><center>タグ</center></td>
	<td style="width:350px;"><center>本文</center></td>
</tr>
<tr>
	<td style="width:100px;">
		<!--投稿日時--><center>2017-11-08 21:24:22</center>
	</td>
	<td style="width:200px;">
		@sugulu<br />(都内IT企業 所属)<br><img width="80" height="80" src="https://qiita-image-store.s3.amazonaws.com/0/191401/profile-images/1509357969">
	</td>
	<td style="width:150px;">
		<!--タグ-->
		<center><b>[Python]</b> <b>[初心者]</b> <b>[機械学習]</b> <b>[DeepLearning]</b> <b>[FX]</b> </center>
	</td>
	<td style="width:350px;">
		<!--本文-->
		<div style="width:350px;height:150px;overflow-x:hidden;overflow-y:scroll;"><p>本シリーズでは、「FXの予測プログラム」を作りながら、機械学習・ディープラーニングを解説します。<br>
（注）のび太くんは有名漫画のキャラとは一切関係ありません。</p>

<p>この物語は、大学生のび太くんが、家庭教師のすぐるさんから機械学習を学び、将来D-mind社を起業して、汎用人工知能搭載型ロボットを誕生させる記録です。</p>

<h1>
<span id="前回のお話" class="fragment"></span><a href="#%E5%89%8D%E5%9B%9E%E3%81%AE%E3%81%8A%E8%A9%B1"><i class="fa fa-link"></i></a>前回のお話</h1>

<p>前回は「if文で翌日のFX（為替）を予測する作戦」を実行してみたものの、失敗に終わったのび太くんでした。<br>
<a href="https://qiita.com/sugulu/items/45e3cfaa78e5f13d9389" id="reference-7f94c40a4f3b6b60368a">第1話：のび太、if文でFXを予想する</a></p>

<p>本日は、家庭教師のすぐるさんから、「線形モデルによる回帰」を学びます。</p>

<h1>
<span id="第2話のび太線形回帰を学ぶ" class="fragment"></span><a href="#%E7%AC%AC2%E8%A9%B1%E3%81%AE%E3%81%B3%E5%A4%AA%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0%E3%82%92%E5%AD%A6%E3%81%B6"><i class="fa fa-link"></i></a>第2話：のび太、線形回帰を学ぶ</h1>

<p>すぐるさん、最近「犬型ロボット」が予約発売されたんだって。<br>
ぼくも欲しいな～</p>

<p>22世紀には「しゃべる猫型ロボット」も生まれるかもしれないね。</p>

<p>しゃべる猫・・・<br>
ぼくは、かわいい犬の方がいいな。</p>

<p>さて、のび太くん、今日は線形モデルによる回帰をやってみようか。<br>
<b>最初に線形回帰モデルを説明をし、後半に練習問題で実装をしてみよう。</b></p>

<p>線形モデル？</p>

<p>「線形モデル」というのは、単純な掛け算と足し算だけで作られる「モデル」のことだよ。</p>

<p>う～ん、まず、その「モデル」っていうのが分かんないよ。</p>

<p>そうだね。すまない。<br>
少し整理しようか。<br>
今回予測したいものを、「翌日の終値 - 当日の終値」とするね。</p>

<p>この予測したい「翌日の終値 - 当日の終値」を<br>
$y_p(t)$<br>
と書くことにしよう。</p>

<p>$p$って何？</p>

<p>$p$はprediction（予測）のpだよ。<br>
続いて、予測に使う材料を$x$で表すことにしよう。</p>

<p>予測の材料には何を使うの？</p>

<p>それはとても難しい質問だね。<br>
はっきり言って、答えはない。</p>

<p>！！！教えてくれるって言ったじゃないか！！</p>

<p>完璧な答えはないけど、だいたいは分かるよ。<br>
要は、翌日の終値に影響しそうなものを使えばいいんだよ。</p>

<p>う～ん、影響しそうなもの・・・<br>
あんまり、FXや為替が分からないから、何が影響しそうか分からないよ。</p>

<p>そうだね。そのあたりは、社会人になるにつれ、経済を学べば分かるようになるよ。<br>
とりあえず今回は、「ドル円の、当日から過去25日分の終値」、25個の値を予測に使う材料としよう。</p>

<p>どうして25個なの？</p>

<p>FXは平日のみ行われているから、25日分でだいたい1か月分のデータになるんだよ。<br>
つまり、1ヶ月前までのデータ25個から、<br>
「翌日の終値が、当日の終値に対していくら変化するのか」<br>
を予測することになる。<br>
今回の場合、その予測のための「式」や「手法」を、「モデル」と呼ぶんだよ。</p>

<p>う～ん、なんとなく分かったよ。<br>
でも線形ってどういう意味？</p>

<p>よしまずは、今回作るモデルを見てみようか。<br>
今回作り上げるモデルは次の式で表される。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>y_p(t)=w_{1}x_{1}(t)+w_{2}x_{2}(t)+\cdots +w_{N}x_{N}(t)+b
</pre></div></div>

<p>この$y_p(t)$が、実際の結果<br>
$y(t)=(t+1日の終値)-(t日の終値)$<br>
と、同じになると、うまく予測できたことになる。</p>

<p>$x_1(t)～x_N(t)$は、予測に使う入力データだよ。<br>
今回は$x_1(t)$は$t$日の終値、$x_2(t)$は$t-1$日の終値・・・<br>
という感じで、$t$日を基準に、$N-1$日前までの終値を表しているよ。</p>

<p>そして、$w_{1}～w_{N}$が、今回求めたい線形モデルの係数だよ。<br>
のび太くん、ここまでは大丈夫？</p>

<p>・<br>
・<br>
・</p>

<p>むにゃむにゃ・・・</p>

<p>のび太くん、起きて！！<br>
まだ$y$と$w$と$x$が出てきただけだよ。</p>

<p>難しいよ。数式とか分からない・・・</p>

<p>大丈夫だよ。ゆっくり説明するから。<br>
この式を見ても、$x$の○乗とか、expとか、難しそうなものがない。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>y_p(t)=w_{0}x_{0}(t)+w_{1}x_{1}(t)+\cdots +w_{N}x_{N}(t)+b
</pre></div></div>

<p>ただの「かけ算と足し算」だろう。<br>
こういうのを線形モデルっていうんだよ。</p>

<p>う～ん・・・</p>

<p>ベクトルで書くとすっきりして、分かりやすくなるよ。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>\mathbf{x}(t)=(x_1(t), x_2(t)\cdots x_{N}(t))^T
</pre></div></div>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>\mathbf{w}=(w_1, w_2,\cdots w_{N})^T
</pre></div></div>

<p>と書くことにするね。</p>

<p>ベクトルか～。なんか勉強したような・・・<br>
終わりにある$T$は？</p>

<p>終わりにある$T$は転置(Transpose)という意味だよ。<br>
本当は縦方向のベクトルを書きたいんだけど、書きにくいから横方向に書いて、転置してねって意味だよ。<br>
すると、元の線形モデルは、次のように書かれるよ。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>y_p(t)=\mathbf{w}^T\mathbf{x}(t)+b
</pre></div></div>

<p>ベクトルだと、すっきりしただろ。<br>
ベクトルで書いたときに、$\mathbf{y_p}$と$\mathbf{x}$が、この式の形で書き表されるモデルを、「線形モデル」と呼ぶんだ。</p>

<p>うん、分かった。そういうことにするよ。</p>

<p>・<br>
・<br>
・</p>

<p>でも、$\mathbf{w}=w_0～w_{N}$と、$b$はどうやって決めるの？</p>

<p>それが今回の問題だよ。<br>
そこが決まれば、予測モデルの完成だからね。<br>
そのために、「学習データ」を使用するんだ。<br>
今回は2007年から2016年のデータを「学習データ」としよう。</p>

<p>この学習データを使用して、最もうまく予想できる$w_0～w_{N}$と$b$を決定するんだよ。<br>
そして、求めた$w_0～w_{N}$と$b$で、2017年のデータを「テストデータ」として推定し、どれくらいうまく予測できるかを確かめるんだ。</p>

<p>とりあえず学習データを使って、$\mathbf{w}$と$b$を求めるってことは分かったよ。</p>

<p>・<br>
・<br>
・</p>

<p>う～ん、でも、どうやって2007年から2016年のデータから、$w_0～w_{N}$と$b$を決めるの？</p>

<p>それをこれから説明しよう。<br>
$\mathbf{w}$と$b$を求めるために、次の式のように、「予測と実際の値の誤差の2乗和」を書き表すんだ。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>E(\mathbf{w}, b)=\sum_{t=0}^{M}\left [ {y(t)-y_p(t)} \right ]^2=\sum_{t=0}^{M}\left [ {y(t)-\left \{\mathbf{w}^T\mathbf{x}(t)+b  \right \}} \right ]^2
</pre></div></div>

<p>$t=0$が2007年の初日、$t=M$が2016年の最後の日を表しているよ。</p>

<p>$E$は誤差(Error)という意味だ。<br>
損失(Loss)を使用して、$L$で書かれることもあるよ。<br>
今回は掛けていないけど、$\frac{1}{2}$を掛けていることも多いよ。</p>

<p>この誤差関数$E$は$\mathbf{w}$と$b$の関数となっていて、$\mathbf{w}, b$を入力として、モデルの「予測誤差の2乗和」を出力することになる。<br>
ここまでは分かったかい？</p>

<p>・<br>
・<br>
・</p>

<p>もう一回言って。</p>

<p>よし。$\mathbf{w}$と$b$を求めるために、予測の誤差の2乗和を求めるんだ。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>E(\mathbf{w}, b)=\sum_{t=0}^{M}\left [ {y(t)-y_p(t)} \right ]^2=\sum_{t=0}^{M}\left [ {y(t)-\left \{\mathbf{w}^T\mathbf{x}(t)+b  \right \}} \right ]^2
</pre></div></div>

<p>$t=0$が2007年の初日、$t=M$が2016年の最後の日を表している。</p>

<p>$E$は誤差(Error)という意味だ。</p>

<p>この誤差関数$E$は$\mathbf{w}, b$の関数となっていて、$\mathbf{w}, b$を入力として、モデルの予測誤差を出力することになる。<br>
どうだい、分かったかい？</p>

<p>どうして、引き算の結果を2乗しないといけないの？<br>
面倒だし、したくないな～</p>

<p>2乗しないと、誤差が0, 0の場合も、1, -1の場合も、足すと0になってしまうだろ。<br>
だから2乗するんだ。</p>

<p>じゃあ、4乗でも6乗でも良いってこと？</p>

<p>それは良い質問だよ。そういう好奇心がとても大切だ。<br>
実はそれを理解するのは、いまの、のび太くんでは難しい。<br>
でも、あえて言っておくね。</p>

<p>この線形モデルの誤差は、様々な小さな誤差の集まりでできていると考えると、中心極限定理により、モデルの誤差が正規分布を描くと想定できる。<br>
すると、データ$y$が生成される$\mathbf{w}, b$を最尤推定で求めると、正規分布の対数尤度を求めることになり、2乗和が出てくるから、今回は2乗和を使うんだよ。</p>

<p>うん、分かんないや。</p>

<p>まあ、最尤推定の話をしたときに、またこの話はしよう。</p>

<p>考えてみれば、4乗より2乗の方が簡単そうだし、ボクは2乗でいいや。<br>
とりあえず、誤差の2乗和$E$が最小となる$(\mathbf{w}, b)$を求めるってことは分かったよ。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>E(\mathbf{w}, b)=\sum_{t=0}^{M}\left [ {y(t)-y_p(t)} \right ]^2=\sum_{t=0}^{M}\left [ {y(t)-\left \{\mathbf{w}^T\mathbf{x}(t)+b  \right \}} \right ]^2
</pre></div></div>

<p>・<br>
・<br>
・</p>

<p>でも、どうやって、$E$から$(\mathbf{w}, b)$を求めるの？</p>

<p>ここで、微分、正しくは偏微分が出てくる。<br>
誤差が最小ということは、求めたい$(\mathbf{w}, b)$の値のときに、$E$が最小値、つまり谷の形の底の部分になる。</p>

<p>そして、適当な$(\mathbf{w}, b)$の値で、$E$の偏微分を求めると、そのベクトルは、その谷底の逆方向を示している。<br>
なぜなら微分（偏微分）というのは、傾きが最も大きくなる方向と、その傾きの大きさを示すからね。</p>

<p><a href="https://camo.qiitausercontent.com/260ab20c390204d4c6c958a3880b8659d8dd5d0f/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f30383138656435652d666639342d636161612d376633352d3532393463383136373636312e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/260ab20c390204d4c6c958a3880b8659d8dd5d0f/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f30383138656435652d666639342d636161612d376633352d3532393463383136373636312e706e67" alt="nobitaFX2_2.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/0818ed5e-ff94-caaa-7f35-5294c8167661.png"></a></p>

<p>偏微分か・・・<br>
う～ん、まあそういうものだと思うことにするよ。</p>

<p>最初はそれで良いよ。<br>
まず適当に$(\mathbf{w}, b)$を決めてあげる。<br>
そして、$(\mathbf{w}, b)$のそれぞれで$E$の偏微分を求めたベクトルの逆方向、すなわちマイナス方向に、$(\mathbf{w}, b)$を更新するんだ。</p>

<p>式で書くと次の通りだよ。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>\mathbf{w}_{new} = \mathbf{w}-\eta \frac{\partial }{\partial \mathbf{w}}E(\mathbf{w}, b)
</pre></div></div>

<div class="code-frame" data-lang="math"><div class="highlight"><pre><span></span>b_{new} = b-\eta \frac{\partial }{\partial b}E(\mathbf{w}, b)
</pre></div></div>

<p>ここで$\eta $は、学習率と呼ばれる小さな値だよ。<br>
一気に$(\mathbf{w}, b)$を更新すると、谷底を行き過ぎてしまうかもしれないからね。<br>
少しずつ更新してあげるんだ。</p>

<p>この更新を繰り返せば、いつか$(\mathbf{w}, b)$は、谷底の値になり、誤差関数の出力（誤差の2乗和）も最小になるんだ。</p>

<p>このように、偏微分を使って、少しずつ更新する方法を「勾配降下法」と呼ぶんだよ。<br>
今回の式の場合は「最急降下法」とも呼ばれるよ。</p>

<p>なんとなくイメージはついたよ。<br>
微分したら、谷底の方向が分かるから、少しずつ、そこに近づけるんだね。</p>

<p>・<br>
・<br>
・</p>

<p>でも、この偏微分を計算する部分が良く分からないや。</p>

<p>この偏微分の計算は、落ち着いてやれば手計算でできるよ。<br>
ちなみに線形回帰の場合は、勾配降下法を使わなくても、一気に谷底の$(\mathbf{w}, b)$を計算することができる。<br>
でも一気に谷底の値が分かるのは、今回くらいだ。<br>
それに、実際にプログラムを書くときには、ライブラリの関数が行ってくれるから、偏微分のコードを自分で書くことはないよ。</p>

<p>なんだ、良かった♪</p>

<p>でも、きちんとプログラムが背後で何をやっているのかを理解しておくことは大事だからね。<br>
じゃあ、今日は最後に練習問題として、自分で線形モデルを構築して、解いてみるコードを実装してみよう。</p>

<p>もう、疲れたよ。</p>

<p>最後の一息だよ。頑張ろう！</p>

<p>分かったよ。<br>
これもiPhone Xへの一歩だ。</p>

<h1>
<span id="線形回帰モデルを機械学習で実装" class="fragment"></span><a href="#%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%A7%E5%AE%9F%E8%A3%85"><i class="fa fa-link"></i></a>線形回帰モデルを機械学習で実装</h1>

<p>今回のコードは全部僕のGitHubにおいて置くね。<br>
<a href="https://github.com/Nobita-FX/Nobita-FX/blob/master/program/NobitaFX_2_linear_regression.ipynb" rel="nofollow noopener" target="_blank">本記事の掲載コードはこちら</a></p>

<p>最初はいつものおまじないゾーンだ。<br>
って言っても、これから使うライブラリを宣言しているだけだよ。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># import関連</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span> <span class="c1"># 実行上問題ない注意は非表示にする</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div></div>

<p>よし、それじゃあ、自分で問題を作ろう。<br>
今回は$w_1=1.0$、$w_2=2.0$、$b=3.0$というモデルが先にあったとする。<br>
このモデルから学習データと、テストデータを作成しよう。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 練習問題: 問題を作成</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="o">*</span> <span class="c1">#乱数のライブラリをimport</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 0〜1の乱数で 100行2列の行列を生成</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 0〜1の乱数で 100行2列の行列を生成</span>

<span class="c1"># 係数を設定</span>
<span class="n">w1</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">w2</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">3.0</span>

<span class="c1"># モデルからの誤差となるノイズを作成 </span>
<span class="n">noise_train</span> <span class="o">=</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">noise_test</span> <span class="o">=</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="n">Y_train</span> <span class="o">=</span> <span class="n">w1</span><span class="o">*</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w2</span><span class="o">*</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>  <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise_train</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">w1</span><span class="o">*</span><span class="n">X_test</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w2</span><span class="o">*</span><span class="n">X_test</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise_test</span> 
</pre></div></div>

<p>うん、できたよ。</p>

<p>よし、じゃあ、これからしたいことを整理しよう。<br>
分かっているのは、$Y_{train}$と$X_{train}$、そして、$Y_{test}$と$X_{test}$だよ。</p>

<p>つまり、$Y_{train}$と$X_{train}$の間にある線形モデルを求めて、その後、$X_{test}$から、$Y_{p}$を求めるってことだよね。</p>

<p>その通り！<br>
そして$Y_{p}$が$Y_{test}$と、ほぼ一緒の値になると予測成功だ。</p>

<p>線形回帰モデルの係数を求めるライブラリはいろいろあるけど、<br>
Pythonnの代表的な機械学習のライブラリであるscikit-learnを使用しよう。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># 練習問題: 問題を線形回帰モデルで解く</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span> <span class="c1"># scikit-learnライブラリの関数を使用します</span>

<span class="n">linear_reg_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span> <span class="c1"># モデルの定義</span>

<span class="n">linear_reg_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span> <span class="c1"># モデルに対して、学習データをフィットさせ係数を学習させます</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"回帰式モデルの係数"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">linear_reg_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
<span class="k">print</span><span class="p">(</span><span class="n">linear_reg_model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span> 
</pre></div></div>

<p>出力：<br>
回帰式モデルの係数<br>
[ 1.01672273  1.93190578]<br>
3.02237850998</p>

<p>だいたい、出力が1、2、3になったよ！！</p>

<p>うん、これは学習データから、うまくモデルが推定できたことを表しているよ。<br>
といっても、今回は、1、2、3から作ったという情報があるから分かるけど、<br>
実際の問題では、この係数からだけではうまくいっているかは分からない。<br>
テストデータから予測してみて、結果を比べてみよう。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># グラフで予測具合を見る</span>
<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">linear_reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1"># テストデータから予測してみる</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">)</span> <span class="c1"># 予測</span>
<span class="n">result</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Y_pred'</span><span class="p">]</span>
<span class="n">result</span><span class="p">[</span><span class="s1">'Y_test'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y_test</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">'darkgrid'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">'Y_pred'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">'Y_test'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">result</span><span class="p">)</span> 
</pre></div></div>

<p><a href="https://camo.qiitausercontent.com/a9162b87b68f02501eb655563010eababa89900a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f38353735633833302d646631302d356364382d343664322d3166663131333864336332662e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/a9162b87b68f02501eb655563010eababa89900a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f38353735633833302d646631302d356364382d343664322d3166663131333864336332662e706e67" alt="nobitaFX_2_1.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/8575c830-df10-5cd8-46d2-1ff1138d3c2f.png"></a></p>

<p>横軸がモデルから予測した値、縦軸が実際の値だよ。<br>
この直線は点の近似直線だ。</p>

<p>横軸と縦軸の値がだいたい一緒になってるよ！</p>

<p>うん、近似直線も$y=x$の形になっていて、傾きがほぼ1になっているね。</p>

<p>この結果は、$Y_{p}$が$Y_{test}$と、ほぼ一緒の値になったことを示している。<br>
つまり、学習データ$Y_{train}$と$X_{train}$を使用して、きちんと予測モデルを作れたことを示しているよ。</p>

<p>ちなみに、今回使用したlinear_model.LinearRegression() は、<br>
勾配降下法でなく、一気に谷底の値を求めているよ。</p>

<p>ふ～、やっとできたのか。</p>

<p>これで練習問題は終了だ。<br>
のび太くんも「線形回帰モデル」が使えるようになったね。</p>

<p>う～ん、分かったような・・・<br>
もう疲れたよ。</p>

<p>僕も今日はもう疲れたから、次回、実際のFXデータで線形回帰をしてみよう。<br>
ただ覚えておいて欲しいのは、線形回帰はFXだけでなく、ビジネスのいろいろな場面で使われるんだよ。<br>
例えば、来期の生産量を決定するために、長期天気予報や過去のデータから、製品の需要を予測することもできる。</p>

<p>いろいろ便利なんだね。</p>

<p>次回は、いよいよFXデータで線形回帰をしてみよう。</p>

<p>うん！</p>

<p>・<br>
・<br>
・</p>

<p>次回：<a href="https://qiita.com/sugulu/items/5c1b03cd445f27fd3e28" id="reference-a937d1a8ca33a017f3c9">「線形モデル回帰分析で、FXを予測」</a>に続く</p>

<hr>

<p>以上、ご一読いただきありがとうございます。</p>

<p>「ここが分かりにくいよ～不明」など、気軽にコメントください。<br>
可能な限り、加筆修正して、分かりやすくしたいです。</p>

<p>また、記事やコードの間違え、改善点、不明点、感想なども<br>
気軽にコメントいただければ幸いです。<br>
感謝</p>
</div>
	</td>
</tr>
</table>
<br />
<table border="1">
<tr>
	<td rowspan="3"><center>suguluさんの<br />6位</center></td>
	<td colspan="4">
		<kbd><i><img alt="いいね" width="16" height="16" src="../thumb-up-120px.png" /></i>30</kbd>
		<a target="_blank" href="https://qiita.com/sugulu/items/bc7c70e6658f204f85f9">【強化学習初心者向け】シンプルな実装例で学ぶQ学習、DQN、DDQN【CartPoleで棒立て：1ファイルで完結、Kearas使用】</a>
	</td>
</tr>
<tr>
	<td style="width:100px;"><center>投稿日時</center></td>
	<td style="width:200px;"><center>投稿者</center></td>
	<td style="width:150px;"><center>タグ</center></td>
	<td style="width:350px;"><center>本文</center></td>
</tr>
<tr>
	<td style="width:100px;">
		<!--投稿日時--><center>2017-09-27 08:58:36</center>
	</td>
	<td style="width:200px;">
		@sugulu<br />(都内IT企業 所属)<br><img width="80" height="80" src="https://qiita-image-store.s3.amazonaws.com/0/191401/profile-images/1509357969">
	</td>
	<td style="width:150px;">
		<!--タグ-->
		<center><b>[Python]</b> <b>[機械学習]</b> <b>[DeepLearning]</b> <b>[強化学習]</b> <b>[Keras]</b> </center>
	</td>
	<td style="width:350px;">
		<!--本文-->
		<div style="width:350px;height:150px;overflow-x:hidden;overflow-y:scroll;"><p>「倒立振子（棒立て問題）」を、強化学習のQ学習、DQNおよびDDQN（Double DQN）で実装・解説したので、紹介します。</p>

<p>ディープラーニングのライブラリにはKerasを使用しました。</p>

<p>（※追記：17/09/27にHuber関数部分を修正しました）<br>
（※追記：17/10/01にQ学習更新のr抜けを修正しました）<br>
（※追記：17/10/03にQ学習報酬のrewardを修正しました）</p>

<p><a href="https://camo.qiitausercontent.com/676cdc0a86a5e11d411402d9d08807de4280b372/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f66613330333839352d336638342d366339312d623039332d3035303736643461646234372e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/676cdc0a86a5e11d411402d9d08807de4280b372/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f66613330333839352d336638342d366339312d623039332d3035303736643461646234372e676966" alt="DDQN.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/fa303895-3f84-6c91-b093-05076d4adb47.gif"></a></p>

<h1>
<span id="概要" class="fragment"></span><a href="#%E6%A6%82%E8%A6%81"><i class="fa fa-link"></i></a>概要</h1>

<p>Open AI GymのCartPole(棒立て)をQ学習（Q-learning）、DQN（deep Q-learning）およびDDQN（Dobule DQN）で実装しました。</p>

<p>【対象者】<br>
・強化学習に興味がある方<br>
・強化学習を実装例から学びたい方<br>
・<a href="https://qiita.com/icoxfog417/items/242439ecd1a477ece312" id="reference-03d3e35e348691d7957e">ゼロからDeepまで学ぶ強化学習</a>を読み、次は簡単な実装例が見たい方<br>
・<a href="https://www.amazon.co.jp/%E3%81%93%E3%82%8C%E3%81%8B%E3%82%89%E3%81%AE%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E7%89%A7%E9%87%8E-%E8%B2%B4%E6%A8%B9/dp/4627880316" rel="nofollow noopener" target="_blank">これからの強化学習</a>（白い本）や<a href="https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-Richard-S-Sutton/dp/4627826613/ref=pd_lpo_sbs_14_img_0?_encoding=UTF8&amp;psc=1&amp;refRID=1DKC7EBKCNA5N67JAR5Z" rel="nofollow noopener" target="_blank">強化学習</a>（青い本）を読んだが、理解が難しく、簡単な実装例を知りたい方</p>

<p>【得られるもの】<br>
各手法を使用したミニマム・シンプルなプログラムが実装できるようになります。</p>

<p><strong>※保守性や汎用性を重視した「分かる人には扱いやすいプログラム」は書いていません。</strong></p>

<p><strong>強化学習の初学者が分かりやすいように、上から下に読んで、すんなり理解できるプログラムを書くように心がけました。</strong></p>

<p><strong>シンプルなクラス設計、関数設計にし、1ファイルでプログラムが完結しています。</strong></p>

<p><strong>コメントを多めに入れるようにしました。</strong></p>

<p>※各プログラムは120行から170行程度です。<br>
※各コードの詳細説明を掲載すると長くすぎるので、別ページに掲載しております。</p>

<h1>
<span id="棒立てcartpoleについて" class="fragment"></span><a href="#%E6%A3%92%E7%AB%8B%E3%81%A6cartpole%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6"><i class="fa fa-link"></i></a>棒立て、CartPoleについて</h1>

<p>CartPoleはOpen AI Gymと呼ばれるライブラリの「棒を立て続けるタスク」です（冒頭の動画）。</p>

<p>子供の頃、掃除の時間に、ほうきを手のひらで立てて遊んだと思いますが、あれです。<br>
手のひらの代わりに、Cart（車）になっています。</p>

<p>時刻tでの「状態s(t)」はcartの位置x、cartの速度v、棒の角度θ、棒の角速度ωの4次元で表現されます。</p>

<p>私たちにできる行動（Action）は、cartを右に押すか、左に押すかの2択です。</p>

<p>状態s(t)に応じて、適切な行動a(t)を選択し、棒を200stepの間、立て続けられたら成功です。</p>

<p>Open AI gymやディープラーニングのライブラリはWindowsでは使用しにくいので、Ubuntu環境をおすすめします。</p>

<p>私はVirtual Boxを使用したり、PCをWindowsとUbuntuのデュアルブートにしています。</p>

<h1>
<span id="強化学習について" class="fragment"></span><a href="#%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6"><i class="fa fa-link"></i></a>強化学習について</h1>

<p>今回やりたいことは、<br>
状態s(t)=[x(t)、v(t)、θ(t)、ω(t)]において、適切な行動a(t)を求めることです。</p>

<p>Q学習、DQNについては、以下の解説が分かりやすいです。<br>
まずはこちらを読んで、イメージをつかんでください。</p>

<p>●<a href="https://qiita.com/icoxfog417/items/242439ecd1a477ece312">ゼロからDeepまで学ぶ強化学習</a></p>

<p>上記サイトで分かりづらかった点のみ、補足的に説明します。</p>

<h1>
<span id="q学習q-learning" class="fragment"></span><a href="#q%E5%AD%A6%E7%BF%92q-learning"><i class="fa fa-link"></i></a>Q学習（Q-learning）</h1>

<h2>
<span id="q関数の表現方法" class="fragment"></span><a href="#q%E9%96%A2%E6%95%B0%E3%81%AE%E8%A1%A8%E7%8F%BE%E6%96%B9%E6%B3%95"><i class="fa fa-link"></i></a>Q関数の表現方法</h2>

<p>私は、Q関数の実装方法が最初よく分からなかったので、解説します。</p>

<p>Q学習のゴールは、正しい行動価値関数Q(s, a)を求めることです。</p>

<p>行動価値関数Q(s,a)は、状態sのときに行動aをした場合、その先最大で得られるであろう報酬合計R(t)を返す関数です。<br>
※未来の報酬は、時間割引率をかけて、若干小さくなっています</p>

<p>Q学習では、このQ関数はテーブル（表）形式で実装されることになります。</p>

<p>今回、カートの位置x(t)などの状態変数は全て連続値ですが、「xは-2.4から2.4までを6分割した離散値にする」などとして、「離散化」を行います。</p>

<p>仮に4つの状態変数を全て6分割したとすると、6^4 = 1296 通りの状態で表されます。</p>

<p>Q関数は、行方向が1296、列方向は右に押すか、左に押すかの2通りで、[1296×2]の行列で表現されます。<br>
各行列のマスの中身はQ(s,a)の値となります。</p>

<h2>
<span id="実装cartpoleをq学習で解く" class="fragment"></span><a href="#%E5%AE%9F%E8%A3%85cartpole%E3%82%92q%E5%AD%A6%E7%BF%92%E3%81%A7%E8%A7%A3%E3%81%8F"><i class="fa fa-link"></i></a>実装：CartPoleをQ学習で解く</h2>

<p>実際にQ学習でCartPoleを実装したコードがこちらです。<br>
100行ちょっとになります。</p>

<div class="code-frame" data-lang="python">
<div class="code-lang"><span class="bold">qlearning.py</span></div>
<div class="highlight"><pre><span></span><span class="c1"># coding:utf-8</span>
<span class="c1"># [0]ライブラリのインポート</span>
<span class="kn">import</span> <span class="nn">gym</span>  <span class="c1">#倒立振子(cartpole)の実行環境</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>  <span class="c1">#gymの画像保存</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>


<span class="c1"># [1]Q関数を離散化して定義する関数　------------</span>
<span class="c1"># 観測した状態を離散値にデジタル変換する</span>
<span class="k">def</span> <span class="nf">bins</span><span class="p">(</span><span class="n">clip_min</span><span class="p">,</span> <span class="n">clip_max</span><span class="p">,</span> <span class="n">num</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">clip_min</span><span class="p">,</span> <span class="n">clip_max</span><span class="p">,</span> <span class="n">num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># 各値を離散値に変換</span>
<span class="k">def</span> <span class="nf">digitize_state</span><span class="p">(</span><span class="n">observation</span><span class="p">):</span>
    <span class="n">cart_pos</span><span class="p">,</span> <span class="n">cart_v</span><span class="p">,</span> <span class="n">pole_angle</span><span class="p">,</span> <span class="n">pole_v</span> <span class="o">=</span> <span class="n">observation</span>
    <span class="n">digitized</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">cart_pos</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">)),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">cart_v</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">)),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">pole_angle</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">)),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">pole_v</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">))</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_dizitized</span><span class="o">**</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">digitized</span><span class="p">)])</span>


<span class="c1"># [2]行動a(t)を求める関数 -------------------------------------</span>
<span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>
           <span class="c1">#徐々に最適行動のみをとる、ε-greedy法</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">episode</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&lt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">next_action</span>


<span class="c1"># [3]Qテーブルを更新する関数 -------------------------------------</span>
<span class="k">def</span> <span class="nf">update_Qtable</span><span class="p">(</span><span class="n">q_table</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">next_Max_Q</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span>
    <span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span>\
            <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">next_Max_Q</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">q_table</span>

<span class="c1"># [4]. メイン関数開始 パラメータ設定--------------------------------------------------------</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">'CartPole-v0'</span><span class="p">)</span>
<span class="n">max_number_of_steps</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1">#1試行のstep数</span>
<span class="n">num_consecutive_iterations</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1">#学習完了評価に使用する平均試行回数</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">2000</span>  <span class="c1">#総試行回数</span>
<span class="n">goal_average_reward</span> <span class="o">=</span> <span class="mi">195</span>  <span class="c1">#この報酬を超えると学習終了（中心への制御なし）</span>
<span class="c1"># 状態を6分割^（4変数）にデジタル変換してQ関数（表）を作成</span>
<span class="n">num_dizitized</span> <span class="o">=</span> <span class="mi">6</span>  <span class="c1">#分割数</span>
<span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
    <span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_dizitized</span><span class="o">**</span><span class="mi">4</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>

<span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_consecutive_iterations</span><span class="p">)</span>  <span class="c1">#各試行の報酬を格納</span>
<span class="n">final_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_episodes</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1">#学習後、各試行のt=200でのｘの位置を格納</span>
<span class="n">islearned</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1">#学習が終わったフラグ</span>
<span class="n">isrender</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1">#描画フラグ</span>


<span class="c1"># [5] メインルーチン--------------------------------------------------</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>  <span class="c1">#試行数分繰り返す</span>
    <span class="c1"># 環境の初期化</span>
    <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">digitize_state</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
    <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_number_of_steps</span><span class="p">):</span>  <span class="c1">#1試行のループ</span>
        <span class="k">if</span> <span class="n">islearned</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1">#学習終了したらcartPoleを描画する</span>
            <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
            <span class="k">print</span> <span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1">#カートのx位置を出力</span>

        <span class="c1"># 行動a_tの実行により、s_{t+1}, r_{t}などを計算する</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c1"># 報酬を設定し与える</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="mi">195</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">200</span>  <span class="c1">#こけたら罰則</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1">#立ったまま終了時は罰則はなし</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1">#各ステップで立ってたら報酬追加</span>

        <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>  <span class="c1">#報酬を追加</span>

        <span class="c1"># 離散状態s_{t+1}を求め、Q関数を更新する</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">digitize_state</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>  <span class="c1">#t+1での観測状態を、離散値に変換</span>
        <span class="n">q_table</span> <span class="o">=</span> <span class="n">update_Qtable</span><span class="p">(</span><span class="n">q_table</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>

        <span class="c1">#  次の行動a_{t+1}を求める </span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">get_action</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">episode</span><span class="p">)</span>    <span class="c1"># a_{t+1} </span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="c1">#終了時の処理</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="si">%d</span><span class="s1"> Episode finished after </span><span class="si">%f</span><span class="s1"> time steps / mean </span><span class="si">%f</span><span class="s1">'</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">episode</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
            <span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">total_reward_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
                                          <span class="n">episode_reward</span><span class="p">))</span>  <span class="c1">#報酬を記録</span>
            <span class="k">if</span> <span class="n">islearned</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1">#学習終わってたら最終のx座標を格納</span>
                <span class="n">final_x</span><span class="p">[</span><span class="n">episode</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">break</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;=</span>
            <span class="n">goal_average_reward</span><span class="p">):</span>  <span class="c1"># 直近の100エピソードが規定報酬以上であれば成功</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Episode </span><span class="si">%d</span><span class="s1"> train agent successfuly!'</span> <span class="o">%</span> <span class="n">episode</span><span class="p">)</span>
        <span class="n">islearned</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="c1">#np.savetxt('learned_Q_table.csv',q_table, delimiter=",") #Qtableの保存する場合</span>
        <span class="k">if</span> <span class="n">isrender</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1">#env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合</span>
            <span class="n">isrender</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1">#10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す</span>
    <span class="c1">#if episode&gt;10:</span>
    <span class="c1">#    if isrender == 0:</span>
    <span class="c1">#        env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合</span>
    <span class="c1">#        isrender = 1</span>
    <span class="c1">#    islearned=1;</span>

<span class="k">if</span> <span class="n">islearned</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s1">'final_x.csv'</span><span class="p">,</span> <span class="n">final_x</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">","</span><span class="p">)</span>
</pre></div>
</div>

<p>実行結果の一例は以下の通りです。</p>

<p>100試行ほどで、立ちますが、どっか行ってしまいます。<br>
（子供がホウキ立てをやってるときに似ていて興味深い）</p>

<p><a href="https://camo.qiitausercontent.com/f2a266ba0ab397002eb1ac34a497ecbc5b7a8cee/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f63656130666433392d656131342d386330352d396534652d6533616136643934396434362e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/f2a266ba0ab397002eb1ac34a497ecbc5b7a8cee/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f63656130666433392d656131342d386330352d396534652d6533616136643934396434362e676966" alt="learn100.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/cea0fd39-ea14-8c05-9e4e-e3aa6d949d46.gif"></a></p>

<p>1000試行ほどで、200step立てるようになります。</p>

<p><a href="https://camo.qiitausercontent.com/457f29d2af966b7e284534b2e2d75040ae5b9d16/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61323336333633642d306665372d326435662d393530372d3565363832366234626637652e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/457f29d2af966b7e284534b2e2d75040ae5b9d16/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61323336333633642d306665372d326435662d393530372d3565363832366234626637652e676966" alt="learned_notcenter.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/a236363d-0fe7-2d5f-9507-5e6826b4bf7e.gif"></a></p>

<p><strong>「Q学習の詳細な説明」および、「掲載したコードの詳細な解説」はこちらを御覧ください。</strong></p>

<p>●<a href="http://neuro-educator.com/rl1/" rel="nofollow noopener" target="_blank">CartPoleでQ学習（Q-learning）を実装・解説【Phythonで強化学習：第1回】</a></p>

<h1>
<span id="dqn" class="fragment"></span><a href="#dqn"><i class="fa fa-link"></i></a>DQN</h1>

<p>DQNはQ学習のQ関数をDL（ディープラーニング）で表した方法です。</p>

<p>やはりまずはこちらを読んで、イメージをつかんでください。</p>

<p>●<a href="https://qiita.com/icoxfog417/items/242439ecd1a477ece312">ゼロからDeepまで学ぶ強化学習</a></p>

<p>また、こちらの新しい本にも、最新の強化学習を紹介する章が用意されており、DNQやDDQNからA3Cまで説明があります。</p>

<p>●<a href="https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0-%E3%83%81%E3%83%A7%E3%83%90-%E3%82%B5%E3%83%91%E3%82%B7%E3%83%90%E3%83%AA/dp/4320124227" rel="nofollow noopener" target="_blank">速習 強化学習: 基礎理論とアルゴリズム（書籍）</a></p>

<h2>
<span id="qネットワーク" class="fragment"></span><a href="#q%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF"><i class="fa fa-link"></i></a>Qネットワーク</h2>

<p>Q関数をディープニューラルネットワークで表したQネットワークについて説明します。</p>

<p>入力層のニューロン数は、状態の次元数となります。<br>
今回であれば、cartの位置x、cartの速度v、棒の角度θ、棒の角速度ωの4つです。</p>

<p>状態の値をそのまま入力層の素子に与えます。<br>
Q学習とは異なり、離散化はしません。<br>
連続値をそのまま入力します。</p>

<p>出力層のニューロン数は、選択できる行動の数となります。<br>
今回であれば、右か左の2つです。</p>

<p>そして出力ニューロンが出力する値は、「右に押すニューロン」であれば、Q(s(t), 右）の値です。<br>
状態s(t)のときにa(t) = 右に押す<br>
を行った場合に、その先に得られる報酬の総計が出力されます。</p>

<p>今回の例ですと以下のようなネットワークを使用します。</p>

<p><a href="https://camo.qiitausercontent.com/16ed6207d6fe23e5b21b3c0e911024d089dd290e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f64666439646139382d303232332d346632312d393463642d3830613537306264376430372e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/16ed6207d6fe23e5b21b3c0e911024d089dd290e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f64666439646139382d303232332d346632312d393463642d3830613537306264376430372e706e67" alt="Qnetwork.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/dfd9da98-0223-4f21-94cd-80a570bd7d07.png"></a></p>

<h2>
<span id="qネットワークの学習" class="fragment"></span><a href="#q%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AE%E5%AD%A6%E7%BF%92"><i class="fa fa-link"></i></a>Qネットワークの学習</h2>

<p>Qネットワークの重みを変化させて、より良いQネットワークを実現します。</p>

<p>状態s(t)でa(t)=右に押す　の場合、Qネットワークの出力層、右ニューロンはQ(s(t), 右）という値を出力します。</p>

<p>時刻tの時点で出力してほしいのは、r(t)+γ・MAX[Q(s_{t+1}, a_{t+1})]です。<br>
※この教師信号も本当は学習途中です<br>
※r(t)は時刻tでもらう報酬、γは時間割引率</p>

<p>Q(s(t), 右）= r(t)+γ・MAX[Q(s_{t+1}, a_{t+1})]<br>
となれば、学習は終了していることになります。</p>

<p>この、Q(s(t), 右に押す）と r(t)+γ・MAX[Q(s_{t+1}, a_{t+1})]の差が小さくなる方向にQネットワークの重みを更新します。</p>

<h2>
<span id="dqn特有の4つの工夫" class="fragment"></span><a href="#dqn%E7%89%B9%E6%9C%89%E3%81%AE4%E3%81%A4%E3%81%AE%E5%B7%A5%E5%A4%AB"><i class="fa fa-link"></i></a>DQN特有の4つの工夫</h2>

<p>DQNは単純にQ関数をDL（ディープラーニング）にする以外に、4つの工夫があります。</p>

<p>こちらのページが分かりやすいので、まずイメージをつかんでください。<br>
●<a href="https://elix-tech.github.io/ja/2016/06/29/dqn-ja.html" rel="nofollow noopener" target="_blank">DQNをKerasとTensorFlowとOpenAI Gymで実装する</a></p>

<p>1つ目の工夫Experience Replayは、学習内容をメモリに保存して、ランダムにとりだして学習します。</p>

<p>2つ目の工夫Fixed Target Q-Networkは、1step分ずつ学習するのでなく、複数ステップ分をまとめて学習（バッチ学習）します。</p>

<p>3つ目の工夫報酬のclippingは、各ステップでの報酬を-1から1の間にします。<br>
今回は各ステップで立っていたら報酬0、こけたら報酬-1、195 step以上立って終了したら報酬+1とクリップしました。</p>

<p>4つ目の誤差関数の工夫は、誤差が1以上では二乗誤差でなく絶対値誤差を使用するHuber関数を実装します。</p>

<h2>
<span id="ddqn" class="fragment"></span><a href="#ddqn"><i class="fa fa-link"></i></a>DDQN</h2>

<p>DDQN(Double DQN)は行動価値関数Qを、価値と行動を計算するメインのQmainと、MAX[Q(s_{t+1}, a_{t+1})]を評価するQtargetに分ける方法です。</p>

<p>分けることで、Q関数の誤差が増大するのを防ぎます。</p>

<p>今回は、試行ごとにQtargetを更新することでDDQNを実現しました。<br>
各試行ではQtargetはひとつ前の試行のQmainの最終値を使用します。</p>

<h2>
<span id="dqnddqnの実装" class="fragment"></span><a href="#dqnddqn%E3%81%AE%E5%AE%9F%E8%A3%85"><i class="fa fa-link"></i></a>DQN、DDQNの実装</h2>

<p>実装したコードがこちらです。</p>

<p>DQN、DDQNは同じファイルです。<br>
途中のDQN_MODEという変数で切り替えています。</p>

<div class="code-frame" data-lang="python">
<div class="code-lang"><span class="bold">dqnddqn.py</span></div>
<div class="highlight"><pre><span></span><span class="c1"># coding:utf-8</span>
<span class="c1"># [0]必要なライブラリのインポート</span>
<span class="kn">import</span> <span class="nn">gym</span>  <span class="c1"># 倒立振子(cartpole)の実行環境</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>  <span class="c1"># gymの画像保存</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>


<span class="c1"># [1]損失関数の定義</span>
<span class="c1"># 損失関数にhuber関数を使用します 参考https://github.com/jaara/AI-blog/blob/master/CartPole-DQN.py</span>
<span class="k">def</span> <span class="nf">huberloss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span>
    <span class="n">cond</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
    <span class="n">L2</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
    <span class="n">L1</span> <span class="o">=</span> <span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">L2</span><span class="p">,</span> <span class="n">L1</span><span class="p">)</span>  <span class="c1"># Keras does not cover where function in tensorflow :-(</span>
    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>


<span class="c1"># [2]Q関数をディープラーニングのネットワークをクラスとして定義</span>
<span class="k">class</span> <span class="nc">QNetwork</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">state_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">action_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">state_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">action_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>  <span class="c1"># 誤差を減らす学習方法はAdam</span>
        <span class="c1"># self.model.compile(loss='mse', optimizer=self.optimizer)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">huberloss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>

    <span class="c1"># 重みの学習</span>
    <span class="k">def</span> <span class="nf">replay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">mini_batch</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">state_b</span><span class="p">,</span> <span class="n">action_b</span><span class="p">,</span> <span class="n">reward_b</span><span class="p">,</span> <span class="n">next_state_b</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_b</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">reward_b</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">next_state_b</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">state_b</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
                <span class="c1"># 価値計算（DDQNにも対応できるように、行動決定のQネットワークと価値観数のQネットワークは分離）</span>
                <span class="n">retmainQs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state_b</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">retmainQs</span><span class="p">)</span>  <span class="c1"># 最大の報酬を返す行動を選択する</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">reward_b</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">targetQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state_b</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="n">next_action</span><span class="p">]</span>

            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state_b</span><span class="p">)</span>    <span class="c1"># Qネットワークの出力</span>
            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">action_b</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span>               <span class="c1"># 教師信号</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># epochsは訓練データの反復回数、verbose=0は表示なしの設定</span>


<span class="c1"># [3]Experience ReplayとFixed Target Q-Networkを実現するメモリクラス</span>
<span class="k">class</span> <span class="nc">Memory</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experience</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)),</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span> <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">len</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>


<span class="c1"># [4]カートの状態に応じて、行動を決定するクラス</span>
<span class="k">class</span> <span class="nc">Actor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">):</span>   <span class="c1"># [C]ｔ＋１での行動を返す</span>
        <span class="c1"># 徐々に最適行動のみをとる、ε-greedy法</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">+</span> <span class="mf">0.9</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">+</span><span class="n">episode</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&lt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">retTargetQs</span> <span class="o">=</span> <span class="n">targetQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">retTargetQs</span><span class="p">)</span>  <span class="c1"># 最大の報酬を返す行動を選択する</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># ランダムに行動する</span>

        <span class="k">return</span> <span class="n">action</span>


<span class="c1"># [5] メイン関数開始----------------------------------------------------</span>
<span class="c1"># [5.1] 初期設定--------------------------------------------------------</span>
<span class="n">DQN_MODE</span> <span class="o">=</span> <span class="mi">1</span>    <span class="c1"># 1がDQN、0がDDQNです</span>
<span class="n">LENDER_MODE</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># 0は学習後も描画なし、1は学習終了後に描画する</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">'CartPole-v0'</span><span class="p">)</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">299</span>  <span class="c1"># 総試行回数</span>
<span class="n">max_number_of_steps</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># 1試行のstep数</span>
<span class="n">goal_average_reward</span> <span class="o">=</span> <span class="mi">195</span>  <span class="c1"># この報酬を超えると学習終了</span>
<span class="n">num_consecutive_iterations</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 学習完了評価の平均計算を行う試行回数</span>
<span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_consecutive_iterations</span><span class="p">)</span>  <span class="c1"># 各試行の報酬を格納</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>    <span class="c1"># 割引係数</span>
<span class="n">islearned</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 学習が終わったフラグ</span>
<span class="n">isrender</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 描画フラグ</span>
<span class="c1"># ---</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">16</span>               <span class="c1"># Q-networkの隠れ層のニューロンの数</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.00001</span>         <span class="c1"># Q-networkの学習係数</span>
<span class="n">memory_size</span> <span class="o">=</span> <span class="mi">10000</span>            <span class="c1"># バッファーメモリの大きさ</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>                <span class="c1"># Q-networkを更新するバッチの大記載</span>

<span class="c1"># [5.2]Qネットワークとメモリ、Actorの生成--------------------------------------------------------</span>
<span class="n">mainQN</span> <span class="o">=</span> <span class="n">QNetwork</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>     <span class="c1"># メインのQネットワーク</span>
<span class="n">targetQN</span> <span class="o">=</span> <span class="n">QNetwork</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>   <span class="c1"># 価値を計算するQネットワーク</span>
<span class="c1"># plot_model(mainQN.model, to_file='Qnetwork.png', show_shapes=True)        # Qネットワークの可視化</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">Memory</span><span class="p">(</span><span class="n">max_size</span><span class="o">=</span><span class="n">memory_size</span><span class="p">)</span>
<span class="n">actor</span> <span class="o">=</span> <span class="n">Actor</span><span class="p">()</span>

<span class="c1"># [5.3]メインルーチン--------------------------------------------------------</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>  <span class="c1"># 試行数分繰り返す</span>
    <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># cartPoleの環境初期化</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span>  <span class="c1"># 1step目は適当な行動をとる</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>   <span class="c1"># list型のstateを、1行4列の行列に変換</span>
    <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">targetQN</span> <span class="o">=</span> <span class="n">mainQN</span>   <span class="c1"># 行動決定と価値計算のQネットワークをおなじにする</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_number_of_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># 1試行のループ</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">islearned</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">LENDER_MODE</span><span class="p">:</span>  <span class="c1"># 学習終了したらcartPoleを描画する</span>
            <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># カートのx位置を出力するならコメントはずす</span>

        <span class="n">action</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">mainQN</span><span class="p">)</span>   <span class="c1"># 時刻tでの行動を決定する</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>   <span class="c1"># 行動a_tの実行による、s_{t+1}, _R{t}を計算する</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>     <span class="c1"># list型のstateを、1行4列の行列に変換</span>

        <span class="c1"># 報酬を設定し、与える</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 次の状態s_{t+1}はない</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="mi">195</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># 報酬クリッピング、報酬は1, 0, -1に固定</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 立ったまま195step超えて終了時は報酬</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 各ステップで立ってたら報酬追加（はじめからrewardに1が入っているが、明示的に表す）</span>

        <span class="n">episode_reward</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># reward  # 合計報酬を更新</span>

        <span class="n">memory</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">))</span>     <span class="c1"># メモリの更新する</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>  <span class="c1"># 状態更新</span>


        <span class="c1"># Qネットワークの重みを学習・更新する replay</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">memory</span><span class="o">.</span><span class="n">len</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">batch_size</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">islearned</span><span class="p">:</span>
            <span class="n">mainQN</span><span class="o">.</span><span class="n">replay</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">DQN_MODE</span><span class="p">:</span>
            <span class="n">targetQN</span> <span class="o">=</span> <span class="n">mainQN</span>  <span class="c1"># 行動決定と価値計算のQネットワークをおなじにする</span>

        <span class="c1"># 1施行終了時の処理</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">total_reward_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">episode_reward</span><span class="p">))</span>  <span class="c1"># 報酬を記録</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="si">%d</span><span class="s1"> Episode finished after </span><span class="si">%f</span><span class="s1"> time steps / mean </span><span class="si">%f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">episode</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
            <span class="k">break</span>

    <span class="c1"># 複数施行の平均報酬で終了を判断</span>
    <span class="k">if</span> <span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">goal_average_reward</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Episode </span><span class="si">%d</span><span class="s1"> train agent successfuly!'</span> <span class="o">%</span> <span class="n">episode</span><span class="p">)</span>
        <span class="n">islearned</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">isrender</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>   <span class="c1"># 学習済みフラグを更新</span>
            <span class="n">isrender</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># env = wrappers.Monitor(env, './movie/cartpoleDDQN')  # 動画保存する場合</span>
            <span class="c1"># 10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す</span>
            <span class="c1"># if episode&gt;10:</span>
            <span class="c1">#    if isrender == 0:</span>
            <span class="c1">#        env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合</span>
            <span class="c1">#        isrender = 1</span>
            <span class="c1">#    islearned=1;</span>
</pre></div>
</div>

<p>実行結果の一例は以下の通りです。<br>
100試行ほどで、200step立てるようになります。</p>

<p>Q学習のときは学習に1000試行かかりましたが、DQN、DDQNは1/10の100試行ほどで立てるようになりました。</p>

<p>DQNよりDDQNの方が収束が早い気がします。</p>

<p>DQN</p>

<p><a href="https://camo.qiitausercontent.com/7348bcbc250b0590f7c1b61805c07b16595889a2/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f37376564656565362d306562332d303734392d346236642d3036323738303961316536312e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/7348bcbc250b0590f7c1b61805c07b16595889a2/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f37376564656565362d306562332d303734392d346236642d3036323738303961316536312e676966" alt="DQN.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/77edeee6-0eb3-0749-4b6d-0627809a1e61.gif"></a></p>

<p>DDQN</p>

<p><a href="https://camo.qiitausercontent.com/05a73f8f58d0962d40ceff256639df73e96b4bc5/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f33343938386665332d353663382d316630332d636463642d6362353965663336643464332e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/05a73f8f58d0962d40ceff256639df73e96b4bc5/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f33343938386665332d353663382d316630332d636463642d6362353965663336643464332e676966" alt="DDQN.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/34988fe3-56c8-1f03-cdcd-cb59ef36d4d3.gif"></a></p>

<p><strong>「DQN、DDQNの詳細な説明」および、「掲載したコードの詳細な解説」はこちらを御覧ください。</strong></p>

<p>●<a href="http://neuro-educator.com/rl2/" rel="nofollow noopener" target="_blank">CartPoleでDQN（deep Q-learning）、DDQNを実装・解説【Phythonで強化学習：第2回】</a></p>

<p>以上、CartPoleでQ学習、DQN、DDQNをシンプルに実装する方法を紹介しました。</p>

<p>強化学習の実装イメージを持ってもらえれば幸いです。</p>

<p>次回はディープラーニングを用いたより発展的な強化学習である<br>
dueling network、prioritized experience replay、A3C<br>
あたりを実装する予定です。</p>

<p>しばしお待ち下さい。</p>

<p>以上、ご一読いただき、ありがとうございました。</p>
</div>
	</td>
</tr>
</table>
<br />
<table border="1">
<tr>
	<td rowspan="3"><center>suguluさんの<br />7位</center></td>
	<td colspan="4">
		<kbd><i><img alt="いいね" width="16" height="16" src="../thumb-up-120px.png" /></i>29</kbd>
		<a target="_blank" href="https://qiita.com/sugulu/items/acbc909dd9b74b043e45">﻿【強化学習】実装しながら学ぶA3C【CartPoleで棒立て：1ファイルで完結】</a>
	</td>
</tr>
<tr>
	<td style="width:100px;"><center>投稿日時</center></td>
	<td style="width:200px;"><center>投稿者</center></td>
	<td style="width:150px;"><center>タグ</center></td>
	<td style="width:350px;"><center>本文</center></td>
</tr>
<tr>
	<td style="width:100px;">
		<!--投稿日時--><center>2017-10-23 06:26:55</center>
	</td>
	<td style="width:200px;">
		@sugulu<br />(都内IT企業 所属)<br><img width="80" height="80" src="https://qiita-image-store.s3.amazonaws.com/0/191401/profile-images/1509357969">
	</td>
	<td style="width:150px;">
		<!--タグ-->
		<center><b>[Python]</b> <b>[機械学習]</b> <b>[DeepLearning]</b> <b>[強化学習]</b> <b>[TensorFlow]</b> </center>
	</td>
	<td style="width:350px;">
		<!--本文-->
		<div style="width:350px;height:150px;overflow-x:hidden;overflow-y:scroll;"><p>2016年に発表された強化学習のアルゴリズム「A3C」を実装しながら、解説します。<br>
（エイ・スリー・シー）と呼ぶそうです。</p>

<p>A3Cは、アルファ碁ゼロをはじめ、最新の強化学習を学ぶうえで、避けては通れない重要なアルゴリズムです。</p>

<p>世界一分かりやすいA3C、猫でもわかるA3Cの紹介を目指して、記事を書きます。</p>

<p>※ 171115<br>
tarutoさまにお気づきいただき、AgentクラスのAct関数を修正しました。</p>

<p><a href="https://camo.qiitausercontent.com/29e5bb2b5446c8359406a14daf0690e4e7b93bc2/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f66303462613931612d343932652d623961322d643832632d3939643232323463626566392e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/29e5bb2b5446c8359406a14daf0690e4e7b93bc2/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f66303462613931612d343932652d623961322d643832632d3939643232323463626566392e676966" alt="openaigym.video.0.3522.video000000.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/f04ba91a-492e-b9a2-d82c-99d2224cbef9.gif"></a></p>

<h1>
<span id="概要" class="fragment"></span><a href="#%E6%A6%82%E8%A6%81"><i class="fa fa-link"></i></a>概要</h1>

<p>OpenAI GymのCartPoleを題材に、「A3C」の実装・解説をします。<br>
プログラムが1ファイルで完結し、学習・理解しやすいようにしています。</p>

<p>本記事では、</p>

<ol>
<li>A3Cとは（概要）</li>
<li>A3Cのアルゴリズム解説</li>
<li>A3Cを少しずつ実装しながら、実装方法の解説</li>
<li>最終的なコード</li>
</ol>

<p>の順番で紹介します。</p>

<p>【対象者】<br>
・強化学習DQNの発展版に興味がある方<br>
・<a href="https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0-%E3%83%81%E3%83%A7%E3%83%90-%E3%82%B5%E3%83%91%E3%82%B7%E3%83%90%E3%83%AA/dp/4320124227" rel="nofollow noopener" target="_blank">速習 強化学習: 基礎理論とアルゴリズム（書籍）</a>を読んで、A3Cを知ったが、実装方法がよく分からない方</p>

<p>【得られるもの】<br>
ミニマム・シンプルなプログラムの実装例から、A3Cを理解・実装できるようになります。</p>

<p>【注意】<br>
本記事に入る前に、OpenAI gymのCartPoleの使い方、Q学習、DQN、Dueliing DQNをなんとなく知っておくと良いと思います。</p>

<p>●<a href="http://neuro-educator.com/rl1/" rel="nofollow noopener" target="_blank">CartPoleでQ学習（Q-learning）を実装・解説【Phythonで強化学習：第1回】</a><br>
●<a href="http://neuro-educator.com/rl2/" rel="nofollow noopener" target="_blank">CartPoleでDQN（deep Q-learning）、DDQNを実装・解説【Phythonで強化学習：第2回】</a><br>
●<a href="https://qiita.com/sugulu/items/6c4d34446d4878cde61a" id="reference-0f906e6426599b3461ae">実装例から学ぶDueling Network DQN</a></p>

<h1>
<span id="a3cとは概要" class="fragment"></span><a href="#a3c%E3%81%A8%E3%81%AF%E6%A6%82%E8%A6%81"><i class="fa fa-link"></i></a>A3Cとは（概要）</h1>

<p>A3Cとは「Asynchronous Advantage Actor-Critic」の略称です。</p>

<p>強化学習におけるA3Cの立ち位置を紹介します。<br>
　強化学習の分野は、ディープラーニングを取り入れた強化学習である「DQN」が2013年に発表され、大きく進展しました。</p>

<p>その後、DQNを少し発展させたDDQNやDueling Network、prioritized experience replayなどの手法が発表されました。</p>

<p>また、これらの流れとは別に、Gorila(General Reinforcment Learning Architecutre)のような、並列計算で性能を上げる手法なども発表されました。</p>

<p>A3CはこれらのDQNの発展と、並列化の流れが合体したような手法です。強化学習の世界では、DQNの次の世代の手法として注目を浴びた革新的なアルゴリズムになります。</p>

<p>　しかしながらA3Cは、DQNの次の世代的存在であるため、DQNからの変化幅が大きく、理解するのがなかなか難しいです。</p>

<p>アルゴリズムそのものが難しいです。<br>
その実装方法も難しいです。</p>

<p>ですがアルファ碁ゼロをはじめ、最新の強化学習の世界を理解するには、A3Cは避けて通れない重要なアルゴリズムです。</p>

<h1>
<span id="a3cのアルゴリズム解説" class="fragment"></span><a href="#a3c%E3%81%AE%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E8%A7%A3%E8%AA%AC"><i class="fa fa-link"></i></a>A3Cのアルゴリズム解説</h1>

<p>A3Cのアルゴリズムを解説します。3つのAをひとつずつ紹介します。</p>

<p>「Asynchronous Advantage Actor-Critic」ですが、<br>
1. Advantage<br>
2. Actor-Critic<br>
3. Asynchronous<br>
の順番で説明します。</p>

<h2>
<span id="advantage" class="fragment"></span><a href="#advantage"><i class="fa fa-link"></i></a>Advantage</h2>

<p>通常のQ学習、DQNではQ関数の更新を、Q(s,a) が、 r(t) + γ・max[Q(s_,a)] に近づくように、Q関数を学習していきました。<br>
ここで、r(t)は時刻tで得た報酬、s_は状態sでaの行動をした結果の状態です。<br>
つまり、s_=s(t+1)。<br>
γは時間割引率です。</p>

<p>Q(s,a) →　r(t) + γ・max[Q(s_,a)]<br>
で再帰的にQ関数を学んでいきます。</p>

<p>はじめはイメージがつきにくいですが、CartPoleの場合は、t=200もしくは倒れたときが終端（終了）となるので、終端のsの場合、次の状態s_がないため、<br>
Q(s,a) →　r(t)<br>
とQ関数が再帰的でなく確定します。<br>
このように、終端の状態sからQ関数がどんどん確からしくなっていきます。</p>

<p>Advantageは、このQ関数の更新を「1ステップ先でなく、2ステップ以上先まで動かして、更新しよう」という考え方です。</p>

<p>例えば、2ステップ先を考慮したAdvantageは以下のようになります。</p>

<p>Q(s,a) →　r(t) + γ・r(t+1) + (γ^2)・max[Q(s_,a)]<br>
となります。</p>

<p>ここで、時刻tで状態はsです。時刻tで行動a(t)を行い、状態sが変わります。<br>
また報酬r(t)を得ます。<br>
次になんらか行動a(t+1)を行い、状態がs_となり、報酬r(t+1)を得ます。<br>
つまり、s_=s(t+2)となっています。</p>

<p>これがAdvantageの考え方です。</p>

<p>これだけ聞くと、「めっちゃ先までAdvantageした方がいいやん♪」って思いますが、単純にそうでもありません。<br>
というのも、途中の行動a(t+1)を決めるときに、完成途中のQ関数を使用するので、そこが間違っていたら、その先もどんどん間違うことになります。<br>
そのため、どんどん先のAdvantageを使えば良いというわけでもなく、少し先くらいまでのAdvantageを使うのがバランスが良いです。</p>

<p>以上がAdvantageの考え方です。</p>

<h2>
<span id="actor-critic" class="fragment"></span><a href="#actor-critic"><i class="fa fa-link"></i></a>Actor-Critic</h2>

<p>これまでQ学習の枠組みで話をしました。</p>

<p>Q学習、DQNなどは、状態sにおいて、行動aを行った場合に、「その先得られるであろう報酬の合計を時間割引した総報酬R」（割引総報酬）を出力する<br>
R = Q(s,a)<br>
のQ関数を使用して強化学習を実施していました。</p>

<p>このようなQ関数を用いた強化学習は、Value-Basedと呼ばれます。</p>

<p>一方で、「Actor-Critic」はPolicy-Basedと呼ばれる別の枠組みと、value-Basedの組み合わせとなります。<br>
Policy-BasedはQ関数を求めず、状態sから直接行動aを決める手法です。</p>

<p>アルファ碁ゼロも、一つのネットワークにPolicy-Basedと、value-Basedを組み合わせたActor-Criticになっています。</p>

<p>Actor-Criticの場合には、ネットワークが行動を出力するActor部分と、状態sの割引総報酬Rを出力するCritic部分に分かれています。</p>

<p>よく以下の絵で紹介されます。</p>

<p><a href="https://camo.qiitausercontent.com/87ea4ac6f6fe135cfde3708d2492c6721d457504/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f37613231383632322d656133362d663333612d333033332d3731393135343830363433622e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/87ea4ac6f6fe135cfde3708d2492c6721d457504/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f37613231383632322d656133362d663333612d333033332d3731393135343830363433622e706e67" alt="ac.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/7a218622-ea36-f33a-3033-71915480643b.png"></a></p>

<p>※Sutton, Berto. 1998<br>
※<a href="https://www.slideshare.net/ssuser07aa33/introduction-to-a3c-model" rel="nofollow noopener" target="_blank">ディープラーニングの最新動向強化学習とのコラボ編6 A3C</a></p>

<p>ですが、この絵を見ても私にはさっぱり分からないです。</p>

<p>そこでCartPoleのネットワークでActor-Crticを書くと次の通りです。</p>

<p><a href="https://camo.qiitausercontent.com/3edcf3031f7ba912bebac04552fdb853e2404438/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f38613038656538372d303064332d386434332d303039612d3635346363393034303236652e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/3edcf3031f7ba912bebac04552fdb853e2404438/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f38613038656538372d303064332d386434332d303039612d3635346363393034303236652e706e67" alt="acnet.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/8a08ee87-00d3-8d43-009a-654cc904026e.png"></a></p>

<p>Fig. 1 CartPoleにおけるActor-Criticなニューラルネットワーク</p>

<p>入力素子は4つです。<br>
状態s(t)の各要素を入力します。<br>
なお、状態s(t)=[x(t), v(t), θ(t), ω(t)]であり、それぞれカートの位置、速度、棒の角度、角速度を表します。</p>

<p>出力素子は、行動a=[右にカートを押す、左にカードを押す]の2つ（Actor）と、状態sでの割引総報酬R=V(s)を出力（Critic）の3つです。</p>

<p>Actorの2つの素子は、状態sの場合に、右に押すのが良い確率p(右)と、左に押すのが良い確率p(左)を示します。<br>
p(右)+p(左)=1となります。</p>

<p>そして、時刻tで状態sの場合に、行動aはこの確率pに従って、右か左に決まります。</p>

<p>また最後の出力層までの途中はActorとCriticが共有しているのが一般的です。</p>

<p>これでActor-Critic・ネットワークで、入力sから行動aを決める流れが説明できました。<br>
(フォワード側の流れ)</p>

<p>ではバックワード側の流れ、つまりどうやってネットワークの素子と素子をつなぐ重み係数を学習するのでしょうか？</p>

<p>Criticに対しては、Q関数が価値関数V(s)に変わっているので、その出力V(s)の更新は<br>
V(s) →　r(t) + γ・V(s_)<br>
となるように、ネットワークを更新すれば良いことが分かります。</p>

<p>Advantageを考慮した場合には、例えば<br>
V(s) →　r(t) + γ・r(t) + (γ^2)・V(s_)</p>

<p>となります。</p>

<p>そして、Actor側も更新してあげる必要があります。</p>

<p>Actor側の更新は、<br>
log[pi_θ(a|s)]*A(t)<br>
を大きくするように更新すれば良いことが知られており、Policy Gradient Theorem（方策勾配定理）と呼ばれています。<br>
ここでA(t)=(R(t)-V(s))であり、パラメータ更新の際にA(t)は定数として扱う必要があります。</p>

<p>例えば、時刻tで状態sの場合に、行動a(t)=右に押す、を行った場合、<br>
log(p(右))＊{R(t)−V(s)}<br>
となります。</p>

<p>ここでR(t)はV(t)であり、2step-Advantageの場合は、<br>
R(t) = r(t) + γ・r(t) + (γ^2)・V(s_)<br>
です。</p>

<p>なぜこれで良いのかは、きちんと方策勾配定理を理解するしかないので、ここでは割愛します。</p>

<p>気になる方はこちらのスライドをおすすめします。<br>
●<a href="https://www.slideshare.net/nishio/3-71708970" rel="nofollow noopener" target="_blank">強化学習その3 </a></p>

<p>この最大化したい対象の式には、p()の項が含まれているので、Actor側のネットワークの重みが更新できることになります。</p>

<p>※なお、TensorFlowでは最大化はできないので、マイナスをかけたものを最小化させる方向に、ネットワークの重みを更新します。<br>
また実装時にはp()が一気に更新されてlocalminimunに落ちないように、エントロピー項をつけて、収束しづらくしています。</p>

<h2>
<span id="asynchronous" class="fragment"></span><a href="#asynchronous"><i class="fa fa-link"></i></a>Asynchronous</h2>

<p>最後にAsynchronous、日本語で非同期という概念について説明します。</p>

<p>Asynchronousは、非同期的でマルチエージェントな分散学習になります。</p>

<p>これはマルチスレッドで複数の学習環境を用意し、各環境のAgentがそれぞれ勝手に経験を積み重ねます。</p>

<p>各スレッドは各自自分の、Advantage-Actor-Criticのネットワークを持っています。<br>
さらに、全スレッドで共有したAdvantage-Actor-Criticのネットワーク（Parameter Server）が存在します。</p>

<p>各Agentは勝手に動いて溜めた経験から、よりたくさん報酬が得られるように、Advantage-Actor-Criticで、ネットワークの重みを更新させる方向（gradient）を求めます。<br>
gradientを求めるタイミングは、一定ステップTmaxが経過するか、終端に達したときです。</p>

<p>ここでそのAgentは自分のネットワークの重みを更新するのではなく、全スレッドで共有したParameterServerにネットワークの重みを更新させる方向（gradient）を渡します。<br>
そして、共有ネットワーク（Prameter Server）でネットワークの重みをgradientの方向に更新します。</p>

<p>gradientを渡したスレッドは、更新されたPrameterServerの重みをコピーしてきて、シミュレーションを継続します。</p>

<p>これらを各スレッドが非同期的に勝手なタイミングで実施します。</p>

<p>つまり、以下の流れとなります。</p>

<ol>
<li>スレッドはParameter Serverからネットワークの重みをコピーする</li>
<li>スレッドのAgentは自分のネットワークにsを入力して、aを得る</li>
<li>aを実行し、r(t)とs_を得る</li>
<li>(s，a, r, s_)をスレッドのメモリに格納する</li>
<li>2〜4を繰り返す(各スレッドでTmaxステップ経過もしくは、終端に達するまで)</li>
<li>経験が十分に溜まったら、自分スレッドのメモリの内容を利用して、ネットワークの重みを更新させる方向gradを求める</li>
<li>gradをParameter Serverに渡す</li>
<li>Parameter Serverはgradの方向にParameter Serverのネットワークを更新する</li>
<li>1.へ戻る</li>
</ol>

<p>これが非同期的な分散学習です。</p>

<p><a href="https://camo.qiitausercontent.com/397ba750fb744f08fc4e4732388e4ea60460b649/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f30303438616533632d653035312d373639352d303337622d6530643737643131313063622e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/397ba750fb744f08fc4e4732388e4ea60460b649/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f30303438616533632d653035312d373639352d303337622d6530643737643131313063622e706e67" alt="asynchrnous.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/0048ae3c-e051-7695-037b-e0d77d1110cb.png"></a></p>

<p>Fig. 2 Asyncrhousな学習</p>

<p>イメージとしては、漫画NARUTOで主人公が分身して修行する感じです。</p>

<p>NARUTOは影分身してたくさんの人数で修行し、一人に戻ると、分身して体験した修行内容を一気に習得できます。</p>

<p>NARUTOもA3Cなどの分散学習アルゴリズムを実装していたのだと思われます。</p>

<p><a href="https://camo.qiitausercontent.com/5bfa6a2bfaedc40da729904c64c10d637a810a58/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f33646130343332392d623036322d383733332d643564352d6266343164633137643530632e6a706567" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/5bfa6a2bfaedc40da729904c64c10d637a810a58/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f33646130343332392d623036322d383733332d643564352d6266343164633137643530632e6a706567" alt="naruto.jpg" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/3da04329-b062-8733-d5d5-bf41dc17d50c.jpeg"></a><br>
<a href="http://www.mangajunky.net/img/1423053905407.jpg/" class="autolink" rel="nofollow noopener" target="_blank">http://www.mangajunky.net/img/1423053905407.jpg/</a></p>

<p>影分身はどうでも良いのですが、非同期的な学習には、分散して早い以上に大きな利点があります。<br>
それはDQNと異なり、experimental replayを使用しないことです。</p>

<p>経験を溜め、それをシャッフルして学習するexperimental replayは、時系列がぐちゃぐちゃになるため、ニューラルネットワークにLSTM(Long short-term memory)を使用できません。</p>

<p>一方でA3CではLSTMも使用できるため、過去の時系列を考慮したニューラルネットワークが実現できます。<br>
※今回の実装例ではLSTMは使用していません</p>

<p>以上が、A3Cのアルゴリズムの解説となります。</p>

<p>これら3つの手法</p>

<ol>
<li>Advantage</li>
<li>Actor-Critic</li>
<li>Asynchronous</li>
</ol>

<p>を実装したものを、A3C「Asynchronous Advantage Actor-Critic」と呼びます。</p>

<p>本記事以外に、以下のSlideShareでもA3Cの解説があるので、合わせて参考にしてください。</p>

<p>●<a href="https://www.slideshare.net/ssuser07aa33/introduction-to-a3c-model" rel="nofollow noopener" target="_blank">ディープラーニングの最新動向強化学習とのコラボ編6 A3C</a></p>

<h1>
<span id="a3cを少しずつ実装しながら実装方法の解説" class="fragment"></span><a href="#a3c%E3%82%92%E5%B0%91%E3%81%97%E3%81%9A%E3%81%A4%E5%AE%9F%E8%A3%85%E3%81%97%E3%81%AA%E3%81%8C%E3%82%89%E5%AE%9F%E8%A3%85%E6%96%B9%E6%B3%95%E3%81%AE%E8%A7%A3%E8%AA%AC"><i class="fa fa-link"></i></a>A3Cを少しずつ実装しながら、実装方法の解説</h1>

<p>それではA3Cを実装してみます。<br>
ですが、これはなかなかの大変さです。</p>

<p>というのもA3Cを実装するには、<br>
1. TensorFlowをマルチスレッドで走らせる<br>
2. 複数のニューラルネットワークを用意して、ネットワーク間で重みをコピーする<br>
3. gradをActor-Criticのloss関数を利用して求める<br>
4. localスレッドで求めたgradでParameter Serverのネットワークを更新する</p>

<p>を実装できる必要があります。</p>

<p>とても大変ですが、ひとつずつ解説します。</p>

<p>なお実装には以下の2つのサイトを参考にしました</p>

<p>●<a href="https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation/" rel="nofollow noopener" target="_blank">Let’s make an A3C: Implementation</a><br>
●<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/experiments/Solve_LunarLander/A3C.py" rel="nofollow noopener" target="_blank">GitHub A3C.py by MorvanZhou</a></p>

<h2>
<span id="実装コードのクラス構成" class="fragment"></span><a href="#%E5%AE%9F%E8%A3%85%E3%82%B3%E3%83%BC%E3%83%89%E3%81%AE%E3%82%AF%E3%83%A9%E3%82%B9%E6%A7%8B%E6%88%90"><i class="fa fa-link"></i></a>実装コードのクラス構成</h2>

<p>実装コードのクラス構成を紹介します。</p>

<p>クラス名とメソッド名、各内容の概要です。<br>
メインメソッドと5つのクラスからなります。</p>

<p><a href="https://camo.qiitausercontent.com/507bb9e2a7a3903a44e09c571217c9e03c6f036d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61663332306531342d303961312d663465642d303434622d6230623666636264613436362e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/507bb9e2a7a3903a44e09c571217c9e03c6f036d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61663332306531342d303961312d663465642d303434622d6230623666636264613436362e706e67" alt="a3cclass.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/af320e14-09a1-f4ed-044b-b0b6fcbda466.png"></a></p>

<p>Fig.3 クラス構成</p>

<p>・メインメソッド（マルチスレッドを実行します）</p>

<p>・ParameterServer(全スレッドで共有するネットワークのクラス)<br>
　build_model（ネットワークの形を定義するメソッド、ここでは4入力、、中間層1層、(2出力と1出力)の、以下のネットワークを定義します）</p>

<p><a href="https://camo.qiitausercontent.com/616f1a5cafe582d969e958e7e7c848a2be9c4c97/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f32613939386630662d653134622d316266332d343965322d6462326330643963356162372e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/616f1a5cafe582d969e958e7e7c848a2be9c4c97/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f32613939386630662d653134622d316266332d343965322d6462326330643963356162372e706e67" alt="A3C.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/2a998f0f-e14b-1bf3-49e2-db2c0d9c5ab7.png"></a></p>

<p>・LocalBrain(各スレッドが持つネットワークのクラスです。メンバ変数にParameterServerと、記憶キューを持ちます)<br>
　build_model（ParamerServerクラスと同じく、ネットワークを定義するメソッドです）<br>
　build_graph（ネットワークのloss関数などを定義するメソッドです）<br>
　pull_parameter_server（localスレッドがParameterServerの重みをコピーします）<br>
　push_parameter_server(localスレッドの重みをParameterServerにコピーします)<br>
　update_parameter_server(localスレッドでgradを求め、ParameterServerの重みをgradで学習・更新します）<br>
　predict_p（状態sから各actionの確率pベクトルを返します）<br>
　train_push（s, a, r, s_を、自分のキューに格納します）</p>

<p>・Agent(open AI gymの環境で活動します。メンバ変数にLocalBrainと記憶メモリを持ちます)<br>
　act（ε-greedy法のもと、自分のLocalBrainのネットワークから行動aを取得します）<br>
　advantage_push_local_brain（s, a, Advantageを考慮したnステップの割引総報酬R, nステップ後の状態s_、をLocalBrainのキューに追加します）</p>

<p>・Environment(openAI gymを走らせる環境です、メンバ変数にAgentを持ちます)<br>
　run（シミュレーションを1試行実行します）</p>

<p>・Worker_Thread（分散して非同期的に実行されるスレッドです。メンバ変数にEnvironmentを持ちます。学習するlearningスレッドと学習後にテストを行うtestスレッドがあります）<br>
　run(Environmentのrunを実行します、学習中は学習を行い、学習後はテスト行程で描画しながら実行します)</p>

<p>以上のクラス構成とメソッドです。<br>
もう少しクラスを減らすこともできますが、これくらいの分割の方が分かりやすいので、この分け方にしました。</p>

<h2>
<span id="メイン関数" class="fragment"></span><a href="#%E3%83%A1%E3%82%A4%E3%83%B3%E9%96%A2%E6%95%B0"><i class="fa fa-link"></i></a>メイン関数</h2>

<p>メイン関数を紹介します。<br>
ここでは、TensorFlowをマルチスレッドで実行します。</p>

<p>メイン関数はほとんどTensorFlowでマルチスレッドを走らせるときのお手本コード通りです。<br>
Worker_threadクラスを生成し、同時に走らせています。</p>

<p>工夫点は次の2つです。</p>

<p>・各スレッドには名前をつけています。この名前はlocalBrainのTensorFlowのネットワークの名前にまで、引き継がれます<br>
・スレッドは、training用のスレッド複数個と、学習後に実行されるtestスレッド1つがあります</p>

<p>学習とテストはファイルを分割し、学習後のパラメータを保存し、別ファイルで読み込んで走らせる方が実用的ですが、今回はスレッドを2種類用意し、学習とテストをひとつのファイルで実行します。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># -- main ここからメイン関数です------------------------------</span>
<span class="c1"># M0.global変数の定義と、セッションの開始です</span>
<span class="n">frames</span> <span class="o">=</span> <span class="mi">0</span>              <span class="c1"># 全スレッドで共有して使用する総ステップ数</span>
<span class="n">isLearned</span> <span class="o">=</span> <span class="bp">False</span>       <span class="c1"># 学習が終了したことを示すフラグ</span>
<span class="n">SESS</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>     <span class="c1"># TensorFlowのセッション開始</span>

<span class="c1"># M1.スレッドを作成します</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"/cpu:0"</span><span class="p">):</span>
    <span class="n">parameter_server</span> <span class="o">=</span> <span class="n">ParameterServer</span><span class="p">()</span>    <span class="c1"># 全スレッドで共有するパラメータを持つエンティティです</span>
    <span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>     <span class="c1"># 並列して走るスレッド</span>
    <span class="c1"># 学習するスレッドを用意</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_WORKERS</span><span class="p">):</span>
        <span class="n">thread_name</span> <span class="o">=</span> <span class="s2">"local_thread"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker_thread</span><span class="p">(</span><span class="n">thread_name</span><span class="o">=</span><span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="o">=</span><span class="s2">"learning"</span><span class="p">,</span> <span class="n">parameter_server</span><span class="o">=</span><span class="n">parameter_server</span><span class="p">))</span>

    <span class="c1"># 学習後にテストで走るスレッドを用意</span>
    <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker_thread</span><span class="p">(</span><span class="n">thread_name</span><span class="o">=</span><span class="s2">"test_thread"</span><span class="p">,</span> <span class="n">thread_type</span><span class="o">=</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">parameter_server</span><span class="o">=</span><span class="n">parameter_server</span><span class="p">))</span>

<span class="c1"># M2.TensorFlowでマルチスレッドを実行します</span>
<span class="n">COORD</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Coordinator</span><span class="p">()</span>                  <span class="c1"># TensorFlowでマルチスレッドにするための準備です</span>
<span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>     <span class="c1"># TensorFlowを使う場合、最初に変数初期化をして、実行します</span>

<span class="n">running_threads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">worker</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
    <span class="n">job</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">worker</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>      <span class="c1"># この辺は、マルチスレッドを走らせる作法だと思って良い</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">job</span><span class="p">)</span>
    <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="c1">#running_threads.append(t)</span>

<span class="c1"># M3.スレッドの終了を合わせます</span>
<span class="c1">#COORD.join(running_threads)</span>
</pre></div></div>

<h2>
<span id="worker_thread" class="fragment"></span><a href="#worker_thread"><i class="fa fa-link"></i></a>Worker_Thread</h2>

<p>ローカルスレッドです。<br>
メンバ変数として、Environmentを持ちます。<br>
またthread_typeはlearnigかtestで、学習用スレッドか学習後に使用するテストスレッドかを指定します。</p>

<p>run関数の内容が分かりにくいですが、学習が終わるまではlearningスレッドを走らせ、テストスレッドはスリープさせておきます。</p>

<p>学習後は、learningスレッドはスリープさせ、testスレッドを走らせています。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># --スレッドになるクラスです　-------</span>
<span class="k">class</span> <span class="nc">Worker_thread</span><span class="p">:</span>
    <span class="c1"># スレッドは学習環境environmentを持ちます</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">environment</span> <span class="o">=</span> <span class="n">Environment</span><span class="p">(</span><span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="o">=</span> <span class="n">thread_type</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>     <span class="c1"># learning threadが走る</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

            <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>    <span class="c1"># test threadを止めておく</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">isLearned</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>     <span class="c1"># learning threadを止めておく</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">isLearned</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>     <span class="c1"># test threadが走る</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div></div>

<h2>
<span id="environment" class="fragment"></span><a href="#environment"><i class="fa fa-link"></i></a>Environment</h2>

<p>次に、Envrionmentクラスを紹介します。<br>
Environmentは、メンバ変数にAgentクラスを持ちます。</p>

<p>メソッドはrun()だけです。<br>
CartPoleを1試行実行します。</p>

<p>行っていることはAsynchronousで説明した、以下の実行です。</p>

<ol>
<li>スレッドはParameter Serverからネットワークの重みをコピーする</li>
<li>スレッドのAgentはネットワークにsを入力して、aを得る</li>
<li>aを実行し、r(t)とs_を得る</li>
<li>(s，a,r,s_)をスレッドのメモリに格納する</li>
<li>2〜4を繰り返す(各スレッドでTmaxステップ経過もしくは、終端に達するまで)</li>
<li>経験が十分に溜まったら、メモリの内容を利用して、ネットワークの重みを更新させる方向gradを求める</li>
<li>gradをParameter Serverに渡す</li>
<li>Parameter Serverはgradの方向にParameter Serverのネットワークを更新する</li>
<li>1.へ戻る</li>
</ol>

<p>注意点は次のとおりです。<br>
・各スレッドで共有して使うグローバル変数を変更する場合は、globalで変数宣言する必要があります<br>
・最後の部分がややこしいですが、自分のスレッドで10試行平均の性能が199ステップを越えたら、そのときのパラメータをPrameterServerにコピーしています</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># --CartPoleを実行する環境です、TensorFlowのスレッドになります　-------</span>
<span class="k">class</span> <span class="nc">Environment</span><span class="p">:</span>
    <span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># 総報酬を10試行分格納して、平均総報酬をもとめる</span>
    <span class="n">count_trial_each_thread</span> <span class="o">=</span> <span class="mi">0</span>     <span class="c1"># 各環境の試行数</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="o">=</span> <span class="n">thread_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">ENV</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">)</span>    <span class="c1"># 環境内で行動するagentを生成</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">pull_parameter_server</span><span class="p">()</span>  <span class="c1"># ParameterSeverの重みを自身のLocalBrainにコピー</span>
        <span class="k">global</span> <span class="n">frames</span>  <span class="c1"># セッション全体での試行数、global変数を書き換える場合は、関数内でglobal宣言が必要です</span>
        <span class="k">global</span> <span class="n">isLearned</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">wrappers</span><span class="o">.</span><span class="n">Monitor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="s1">'./movie/A3C'</span><span class="p">)</span>  <span class="c1"># 動画保存する場合</span>

        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>   <span class="c1"># 学習後のテストでは描画する</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

            <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>   <span class="c1"># 行動を決定</span>
            <span class="n">s_</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>   <span class="c1"># 行動を実施</span>
            <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">frames</span> <span class="o">+=</span> <span class="mi">1</span>     <span class="c1"># セッショントータルの行動回数をひとつ増やします</span>

            <span class="n">r</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>  <span class="c1"># terminal state</span>
                <span class="n">s_</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="k">if</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="mi">199</span><span class="p">:</span>
                    <span class="n">r</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">r</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># Advantageを考慮した報酬と経験を、localBrainにプッシュ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">advantage_push_local_brain</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>

            <span class="n">s</span> <span class="o">=</span> <span class="n">s_</span>
            <span class="n">R</span> <span class="o">+=</span> <span class="n">r</span>
            <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="p">(</span><span class="n">step</span> <span class="o">%</span> <span class="n">Tmax</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>  <span class="c1"># 終了時がTmaxごとに、parameterServerの重みを更新し、それをコピーする</span>
                <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">update_parameter_server</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">pull_parameter_server</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">step</span><span class="p">))</span>  <span class="c1"># トータル報酬の古いのを破棄して最新10個を保持</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># このスレッドの総試行回数を増やす</span>
                <span class="k">break</span>
        <span class="c1"># 総試行数、スレッド名、今回の報酬を出力</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">"スレッド："</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">"、試行数："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"、今回のステップ:"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">step</span><span class="p">)</span><span class="o">+</span><span class="s2">"、平均ステップ："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

        <span class="c1"># スレッドで平均報酬が一定を越えたら終了</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">199</span><span class="p">:</span>
            <span class="n">isLearned</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>     <span class="c1"># この間に他のlearningスレッドが止まります</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">push_parameter_server</span><span class="p">()</span>    <span class="c1"># この成功したスレッドのパラメータをparameter-serverに渡します</span>

</pre></div></div>

<h2>
<span id="agent" class="fragment"></span><a href="#agent"><i class="fa fa-link"></i></a>Agent</h2>

<p>Agentクラスはメンバ変数にLocalBrainと、メモリを持ちます。<br>
メモリはAdvantageを考慮した、(s, a, r, s_)を格納します。</p>

<p>act()メソッドはε-greedy法でランダム行動と、最適行動を選択します。<br>
最適行動は自身のLocalBrainのネットワークから求めます。</p>

<p>advantage_push_local_brain（）メソッドは、メモリをLocalBrainのキューに格納します。<br>
このときにAdvantageを考慮した計算を行います。</p>

<p>工夫点は、<br>
・行動aはone-hotcoding(もし選択肢が3つあって、2つ目なら、[0,1,0]の形)にしています<br>
・nステップ分の割引総報酬self.Rを計算する際に、前ステップの結果を利用して計算しています（ヤロミルさんのサイト参照）</p>

<p>●<a href="https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation/" rel="nofollow noopener" target="_blank">Let's make an A3C</a></p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># --行動を決定するクラスです、CartPoleであれば、棒付き台車そのものになります　-------</span>
<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">brain</span> <span class="o">=</span> <span class="n">LocalBrain</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">)</span>   <span class="c1"># 行動を決定するための脳（ニューラルネットワーク）</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="p">[]</span>        <span class="c1"># s,a,r,s_の保存メモリ、　used for n_step return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="mf">0.</span>             <span class="c1"># 時間割引した、「いまからNステップ分あとまで」の総報酬R</span>

    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">frames</span> <span class="o">&gt;=</span> <span class="n">EPS_STEPS</span><span class="p">:</span>   <span class="c1"># ε-greedy法で行動を決定します 171115修正</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">EPS_END</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">EPS_START</span> <span class="o">+</span> <span class="n">frames</span> <span class="o">*</span> <span class="p">(</span><span class="n">EPS_END</span> <span class="o">-</span> <span class="n">EPS_START</span><span class="p">)</span> <span class="o">/</span> <span class="n">EPS_STEPS</span>  <span class="c1"># linearly interpolate</span>

        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">NUM_ACTIONS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># ランダムに行動</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="p">])</span>
            <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">predict_p</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

            <span class="c1"># a = np.argmax(p)  # これだと確率最大の行動を、毎回選択</span>

            <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="c1"># probability = p のこのコードだと、確率p[0]にしたがって、行動を選択</span>
            <span class="c1"># pにはいろいろな情報が入っていますが確率のベクトルは要素0番目</span>
            <span class="k">return</span> <span class="n">a</span>

    <span class="k">def</span> <span class="nf">advantage_push_local_brain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">):</span>   <span class="c1"># advantageを考慮したs,a,r,s_をbrainに与える</span>
        <span class="k">def</span> <span class="nf">get_sample</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>  <span class="c1"># advantageを考慮し、メモリからnステップ後の状態とnステップ後までのRを取得する関数</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">memory</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">,</span> <span class="n">s_</span>

        <span class="c1"># one-hotコーディングにしたa_catsをつくり、、s,a_cats,r,s_を自分のメモリに追加</span>
        <span class="n">a_cats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">)</span>  <span class="c1"># turn action into one-hot representation</span>
        <span class="n">a_cats</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">a_cats</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">))</span>

        <span class="c1"># 前ステップの「時間割引Nステップ分の総報酬R」を使用して、現ステップのRを計算</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">+</span> <span class="n">r</span> <span class="o">*</span> <span class="n">GAMMA_N</span><span class="p">)</span> <span class="o">/</span> <span class="n">GAMMA</span> <span class="c1"># r0はあとで引き算している、この式はヤロミルさんのサイトを参照</span>

        <span class="c1"># advantageを考慮しながら、LocalBrainに経験を入力する</span>
        <span class="k">if</span> <span class="n">s_</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span>
                <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">get_sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">train_push</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span> <span class="o">/</span> <span class="n">GAMMA</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 次の試行に向けて0にしておく</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">N_STEP_RETURN</span><span class="p">:</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">get_sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">N_STEP_RETURN</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">train_push</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>     <span class="c1"># # r0を引き算</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

</pre></div></div>

<h2>
<span id="parameterserver" class="fragment"></span><a href="#parameterserver"><i class="fa fa-link"></i></a>ParameterServer</h2>

<p>次に全スレッドで共有して持つParameterServerクラスを紹介します。<br>
ここではネットワークの形を定義しており、Kerasを使用しています。</p>

<p>注意点としては、学習にRMSPropOptimizerを使用しています。<br>
RMSPropとは、それまでのパラメータ変化の仕方を考慮した、パラメータ更新方法で、ディープラーニングでよく用いられる手法です。</p>

<p>重要な点は、</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">"parameter_server"</span><span class="p">):</span>      <span class="c1"># スレッド名で重み変数に名前を与え、識別します（Name Space）</span>
</pre></div></div>

<p>で、このネットワークのすべてのパラメータの名前の前に、"parameter_server"を付加していることです。</p>

<p>こうすることで、このネットワークの重みパラメータを</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">weights_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">"parameter_server"</span><span class="p">)</span>
</pre></div></div>

<p>で定義できます。</p>

<p>今回複数のネットワークが出てくるので、各ネットワークのパラメータを識別するために、scopeを利用することが重要です。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># --グローバルなTensorFlowのDeep Neural Networkのクラスです　-------</span>
<span class="k">class</span> <span class="nc">ParameterServer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">"parameter_server"</span><span class="p">):</span>      <span class="c1"># スレッド名で重み変数に名前を与え、識別します（Name Space）</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>            <span class="c1"># ニューラルネットワークの形を決定</span>

        <span class="c1"># serverのパラメータを宣言</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">"parameter_server"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">,</span> <span class="n">RMSPropDecaly</span><span class="p">)</span>    <span class="c1"># loss関数を最小化していくoptimizerの定義です</span>

    <span class="c1"># 関数名がアンダースコア2つから始まるものは「外部から参照されない関数」、「1つは基本的に参照しない関数」という意味</span>
    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>     <span class="c1"># Kerasでネットワークの形を定義します</span>
        <span class="n">l_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">batch_shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_STATES</span><span class="p">))</span>
        <span class="n">l_dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">l_input</span><span class="p">)</span>
        <span class="n">out_actions</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">out_value</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">l_input</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">out_actions</span><span class="p">,</span> <span class="n">out_value</span><span class="p">])</span>
        <span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">to_file</span><span class="o">=</span><span class="s1">'A3C.png'</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># Qネットワークの可視化</span>
        <span class="k">return</span> <span class="n">model</span>
</pre></div></div>

<h2>
<span id="localbrain" class="fragment"></span><a href="#localbrain"><i class="fa fa-link"></i></a>LocalBrain</h2>

<p>最後にLocalBrainクラスについて説明します。<br>
ボリュームが多くて大変ですが、A3Cの中心となるクラスです。</p>

<p>build_model()は基本的にはParameterServerと同じです。<br>
ですが、実行前にinitでKeras.set_sessionを実行しています。</p>

<p>model._make_predict_function()  # have to initialize before threading<br>
で、その後のメソッドが定義できる状態にします。</p>

<p>_build_graph()はこのネットワークに対して実行する様々なメソッドを定義している部分です。<br>
まず、loss関数を定義しています。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre><span></span>loss_policy = - log_prob * tf.stop_gradient(advantage)  # stop_gradientでadvantageは定数として扱います
</pre></div></div>

<p>のtf.stop_gradientで「advantage」をバックプロパゲーションの計算時に定数として扱っています。</p>

<p>loss関数の定義はActor-Criticで説明したとおりですが、エントロピー項が追加されています。<br>
エントロピー項は、p(右)、p(左)の更新が、一気に間違った方向に進まないようにする項です。</p>

<p>エントロピー項のさらなる詳細は、ヤロミルさんのサイトが分かりやすいです。<br>
●<a href="https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation/" rel="nofollow noopener" target="_blank">Let's make an A3C</a></p>

<p>その後、<br>
・パラメータを自分の名前scopeで求める<br>
・自分のネットワークのgradを求める手法<br>
を定義しています。</p>

<p>後半のメソッドたちは、ParameterServerと自分のネットワークでのやりとりを定義しています。<br>
具体的には、自分のgradientを適用してParameterServerを更新したり、ネットワークの重みをコピーしたりするメソッドの定義です。</p>

<p>ここでzip()はひとつの変数ずつ取り出すコマンドです。<br>
ベクトル変数になっているものから、要素をひとつずつ取り出して、実行しています。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre><span></span>self.update_global_weight_params =
parameter_server.optimizer.apply_gradients(zip(self.grads, parameter_server.weights_params))
</pre></div></div>

<p>あとは定義した操作を、実行するメソッドを定義しています。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># --各スレッドで走るTensorFlowのDeep Neural Networkのクラスです　-------</span>
<span class="k">class</span> <span class="nc">LocalBrain</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">):</span>   <span class="c1"># globalなparameter_serverをメンバ変数として持つ</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]]</span>  <span class="c1"># s, a, r, s', s' terminal mask</span>
            <span class="n">K</span><span class="o">.</span><span class="n">set_session</span><span class="p">(</span><span class="n">SESS</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>  <span class="c1"># ニューラルネットワークの形を決定</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_graph</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">)</span>  <span class="c1"># ネットワークの学習やメソッドを定義</span>

    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>     <span class="c1"># Kerasでネットワークの形を定義します</span>
        <span class="n">l_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">batch_shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_STATES</span><span class="p">))</span>
        <span class="n">l_dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">l_input</span><span class="p">)</span>
        <span class="n">out_actions</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">out_value</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">l_input</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">out_actions</span><span class="p">,</span> <span class="n">out_value</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_make_predict_function</span><span class="p">()</span>  <span class="c1"># have to initialize before threading</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">_build_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">):</span>      <span class="c1"># TensorFlowでネットワークの重みをどう学習させるのかを定義します</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_STATES</span><span class="p">))</span>  <span class="c1"># placeholderは変数が格納される予定地となります</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_ACTIONS</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># not immediate, but discounted n step reward</span>

        <span class="n">p</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s_t</span><span class="p">)</span>

        <span class="c1"># loss関数を定義します</span>
        <span class="n">log_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>
        <span class="n">advantage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_t</span> <span class="o">-</span> <span class="n">v</span>
        <span class="n">loss_policy</span> <span class="o">=</span> <span class="o">-</span> <span class="n">log_prob</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">advantage</span><span class="p">)</span>  <span class="c1"># stop_gradientでadvantageは定数として扱います</span>
        <span class="n">loss_value</span> <span class="o">=</span> <span class="n">LOSS_V</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">advantage</span><span class="p">)</span>  <span class="c1"># minimize value error</span>
        <span class="n">entropy</span> <span class="o">=</span> <span class="n">LOSS_ENTROPY</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># maximize entropy (regularization)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_total</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss_policy</span> <span class="o">+</span> <span class="n">loss_value</span> <span class="o">+</span> <span class="n">entropy</span><span class="p">)</span>

        <span class="c1"># 重みの変数を定義</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>  <span class="c1"># パラメータを宣言</span>
        <span class="c1"># 勾配を取得する定義</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_total</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_params</span><span class="p">)</span>

        <span class="c1"># ParameterServerの重み変数を更新する定義(zipで各変数ごとに計算)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_global_weight_params</span> <span class="o">=</span> \
            <span class="n">parameter_server</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grads</span><span class="p">,</span> <span class="n">parameter_server</span><span class="o">.</span><span class="n">weights_params</span><span class="p">))</span>

        <span class="c1"># PrameterServerの重み変数の値を、localBrainにコピーする定義</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pull_global_weight_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">l_p</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">g_p</span><span class="p">)</span>
                                          <span class="k">for</span> <span class="n">l_p</span><span class="p">,</span> <span class="n">g_p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_params</span><span class="p">,</span> <span class="n">parameter_server</span><span class="o">.</span><span class="n">weights_params</span><span class="p">)]</span>

        <span class="c1"># localBrainの重み変数の値を、PrameterServerにコピーする定義</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">push_local_weight_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">g_p</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">l_p</span><span class="p">)</span>
                                          <span class="k">for</span> <span class="n">g_p</span><span class="p">,</span> <span class="n">l_p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parameter_server</span><span class="o">.</span><span class="n">weights_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_params</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">pull_parameter_server</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># localスレッドがglobalの重みを取得する</span>
        <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pull_global_weight_params</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">push_parameter_server</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># localスレッドの重みをglobalにコピーする</span>
        <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">push_local_weight_params</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_parameter_server</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>     <span class="c1"># localbrainの勾配でParameterServerの重みを学習・更新します</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">MIN_BATCH</span><span class="p">:</span>    <span class="c1"># データがたまっていない場合は更新しない</span>
            <span class="k">return</span>

        <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">s_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]]</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>    <span class="c1"># vstackはvertical-stackで縦方向に行列を連結、いまはただのベクトル転置操作</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
        <span class="n">s_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
        <span class="n">s_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s_mask</span><span class="p">)</span>

        <span class="c1"># Nステップあとの状態s_から、その先得られるであろう時間割引総報酬vを求めます</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>

        <span class="c1"># N-1ステップあとまでの時間割引総報酬rに、Nから先に得られるであろう総報酬vに割引N乗したものを足します</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">GAMMA_N</span> <span class="o">*</span> <span class="n">v</span> <span class="o">*</span> <span class="n">s_mask</span>  <span class="c1"># set v to 0 where s_ is terminal state</span>
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">s_t</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span><span class="p">:</span> <span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_t</span><span class="p">:</span> <span class="n">r</span><span class="p">}</span>     <span class="c1"># 重みの更新に使用するデータ</span>
        <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_global_weight_params</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>   <span class="c1"># ParameterServerの重みを更新</span>

    <span class="k">def</span> <span class="nf">predict_p</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>    <span class="c1"># 状態sから各actionの確率pベクトルを返します</span>
        <span class="n">p</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">p</span>

    <span class="k">def</span> <span class="nf">train_push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">s_</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">NONE_STATE</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span>

</pre></div></div>

<h2>
<span id="実行" class="fragment"></span><a href="#%E5%AE%9F%E8%A1%8C"><i class="fa fa-link"></i></a>実行</h2>

<p>以上で、コードは完成です（ただし定数宣言部分を除く）。<br>
全コードは記事の最後に掲載しています。</p>

<p>このA3Cを実行すると、8つのlearningスレッドが実行され、およそ各スレッドが130試行から200試行弱で、つまり、合計すると1000試行ほどで学習が終わります。</p>

<p>試行数としてはDQNより多いのですが、実行時間は圧倒的にA3Cの方が早いです。</p>

<p>こんな感じの挙動をします。</p>

<p><a href="https://camo.qiitausercontent.com/b8e58470890a37718e595ddf40a1e6d26f134c07/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f62653232366366652d626266662d366139642d373432612d3333646464373436306331342e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/b8e58470890a37718e595ddf40a1e6d26f134c07/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f62653232366366652d626266662d366139642d373432612d3333646464373436306331342e676966" alt="openaigym.video.0.3522.video000000.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/be226cfe-bbff-6a9d-742a-33ddd7460c14.gif"></a></p>

<p>以上、A3C実装の解説でした。<br>
次回記事ではUNREALを行う予定です。</p>

<h1>
<span id="最終的なコード" class="fragment"></span><a href="#%E6%9C%80%E7%B5%82%E7%9A%84%E3%81%AA%E3%82%B3%E3%83%BC%E3%83%89"><i class="fa fa-link"></i></a>最終的なコード</h1>

<p>最後に全コードを掲載します。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># coding:utf-8</span>
<span class="c1"># -----------------------------------</span>
<span class="c1"># OpenGym CartPole-v0 with A3C on CPU</span>
<span class="c1"># -----------------------------------</span>
<span class="c1">#</span>
<span class="c1"># A3C implementation with TensorFlow multi threads.</span>
<span class="c1">#</span>
<span class="c1"># Made as part of Qiita article, available at</span>
<span class="c1"># https://??/</span>
<span class="c1">#</span>
<span class="c1"># author: Sugulu, 2017</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">gym</span><span class="o">,</span> <span class="nn">time</span><span class="o">,</span> <span class="nn">random</span><span class="o">,</span> <span class="nn">threading</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>  <span class="c1"># gymの画像保存</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'TF_CPP_MIN_LOG_LEVEL'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'2'</span>    <span class="c1"># TensorFlow高速化用のワーニングを表示させない</span>

<span class="c1"># -- constants of Game</span>
<span class="n">ENV</span> <span class="o">=</span> <span class="s1">'CartPole-v0'</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">ENV</span><span class="p">)</span>
<span class="n">NUM_STATES</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>     <span class="c1"># CartPoleは4状態</span>
<span class="n">NUM_ACTIONS</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>        <span class="c1"># CartPoleは、右に左に押す2アクション</span>
<span class="n">NONE_STATE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">NUM_STATES</span><span class="p">)</span>

<span class="c1"># -- constants of LocalBrain</span>
<span class="n">MIN_BATCH</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">LOSS_V</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>  <span class="c1"># v loss coefficient</span>
<span class="n">LOSS_ENTROPY</span> <span class="o">=</span> <span class="o">.</span><span class="mo">01</span>  <span class="c1"># entropy coefficient</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">5e-3</span>
<span class="n">RMSPropDecaly</span> <span class="o">=</span> <span class="mf">0.99</span>

<span class="c1"># -- params of Advantage-ベルマン方程式</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">N_STEP_RETURN</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">GAMMA_N</span> <span class="o">=</span> <span class="n">GAMMA</span> <span class="o">**</span> <span class="n">N_STEP_RETURN</span>

<span class="n">N_WORKERS</span> <span class="o">=</span> <span class="mi">8</span>   <span class="c1"># スレッドの数</span>
<span class="n">Tmax</span> <span class="o">=</span> <span class="mi">10</span>   <span class="c1"># 各スレッドの更新ステップ間隔</span>

<span class="c1"># ε-greedyのパラメータ</span>
<span class="n">EPS_START</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">EPS_END</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">EPS_STEPS</span> <span class="o">=</span> <span class="mi">200</span><span class="o">*</span><span class="n">N_WORKERS</span>


<span class="c1"># --グローバルなTensorFlowのDeep Neural Networkのクラスです　-------</span>
<span class="k">class</span> <span class="nc">ParameterServer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">"parameter_server"</span><span class="p">):</span>      <span class="c1"># スレッド名で重み変数に名前を与え、識別します（Name Space）</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>            <span class="c1"># ニューラルネットワークの形を決定</span>

        <span class="c1"># serverのパラメータを宣言</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">"parameter_server"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="n">LEARNING_RATE</span><span class="p">,</span> <span class="n">RMSPropDecaly</span><span class="p">)</span>    <span class="c1"># loss関数を最小化していくoptimizerの定義です</span>

    <span class="c1"># 関数名がアンダースコア2つから始まるものは「外部から参照されない関数」、「1つは基本的に参照しない関数」という意味</span>
    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>     <span class="c1"># Kerasでネットワークの形を定義します</span>
        <span class="n">l_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">batch_shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_STATES</span><span class="p">))</span>
        <span class="n">l_dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">l_input</span><span class="p">)</span>
        <span class="n">out_actions</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">out_value</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">l_input</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">out_actions</span><span class="p">,</span> <span class="n">out_value</span><span class="p">])</span>
        <span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">to_file</span><span class="o">=</span><span class="s1">'A3C.png'</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># Qネットワークの可視化</span>
        <span class="k">return</span> <span class="n">model</span>


<span class="c1"># --各スレッドで走るTensorFlowのDeep Neural Networkのクラスです　-------</span>
<span class="k">class</span> <span class="nc">LocalBrain</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">):</span>   <span class="c1"># globalなparameter_serverをメンバ変数として持つ</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]]</span>  <span class="c1"># s, a, r, s', s' terminal mask</span>
            <span class="n">K</span><span class="o">.</span><span class="n">set_session</span><span class="p">(</span><span class="n">SESS</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>  <span class="c1"># ニューラルネットワークの形を決定</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_graph</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">)</span>  <span class="c1"># ネットワークの学習やメソッドを定義</span>

    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>     <span class="c1"># Kerasでネットワークの形を定義します</span>
        <span class="n">l_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">batch_shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_STATES</span><span class="p">))</span>
        <span class="n">l_dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">l_input</span><span class="p">)</span>
        <span class="n">out_actions</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">out_value</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">l_input</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">out_actions</span><span class="p">,</span> <span class="n">out_value</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_make_predict_function</span><span class="p">()</span>  <span class="c1"># have to initialize before threading</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">_build_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">):</span>      <span class="c1"># TensorFlowでネットワークの重みをどう学習させるのかを定義します</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_STATES</span><span class="p">))</span>  <span class="c1"># placeholderは変数が格納される予定地となります</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_ACTIONS</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># not immediate, but discounted n step reward</span>

        <span class="n">p</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s_t</span><span class="p">)</span>

        <span class="c1"># loss関数を定義します</span>
        <span class="n">log_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>
        <span class="n">advantage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_t</span> <span class="o">-</span> <span class="n">v</span>
        <span class="n">loss_policy</span> <span class="o">=</span> <span class="o">-</span> <span class="n">log_prob</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">advantage</span><span class="p">)</span>  <span class="c1"># stop_gradientでadvantageは定数として扱います</span>
        <span class="n">loss_value</span> <span class="o">=</span> <span class="n">LOSS_V</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">advantage</span><span class="p">)</span>  <span class="c1"># minimize value error</span>
        <span class="n">entropy</span> <span class="o">=</span> <span class="n">LOSS_ENTROPY</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># maximize entropy (regularization)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_total</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss_policy</span> <span class="o">+</span> <span class="n">loss_value</span> <span class="o">+</span> <span class="n">entropy</span><span class="p">)</span>

        <span class="c1"># 重みの変数を定義</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>  <span class="c1"># パラメータを宣言</span>
        <span class="c1"># 勾配を取得する定義</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_total</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_params</span><span class="p">)</span>

        <span class="c1"># ParameterServerの重み変数を更新する定義(zipで各変数ごとに計算)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_global_weight_params</span> <span class="o">=</span> \
            <span class="n">parameter_server</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grads</span><span class="p">,</span> <span class="n">parameter_server</span><span class="o">.</span><span class="n">weights_params</span><span class="p">))</span>

        <span class="c1"># PrameterServerの重み変数の値を、localBrainにコピーする定義</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pull_global_weight_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">l_p</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">g_p</span><span class="p">)</span>
                                          <span class="k">for</span> <span class="n">l_p</span><span class="p">,</span> <span class="n">g_p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_params</span><span class="p">,</span> <span class="n">parameter_server</span><span class="o">.</span><span class="n">weights_params</span><span class="p">)]</span>

        <span class="c1"># localBrainの重み変数の値を、PrameterServerにコピーする定義</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">push_local_weight_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">g_p</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">l_p</span><span class="p">)</span>
                                          <span class="k">for</span> <span class="n">g_p</span><span class="p">,</span> <span class="n">l_p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parameter_server</span><span class="o">.</span><span class="n">weights_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_params</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">pull_parameter_server</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># localスレッドがglobalの重みを取得する</span>
        <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pull_global_weight_params</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">push_parameter_server</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># localスレッドの重みをglobalにコピーする</span>
        <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">push_local_weight_params</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_parameter_server</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>     <span class="c1"># localbrainの勾配でParameterServerの重みを学習・更新します</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">MIN_BATCH</span><span class="p">:</span>    <span class="c1"># データがたまっていない場合は更新しない</span>
            <span class="k">return</span>

        <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">s_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]]</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>    <span class="c1"># vstackはvertical-stackで縦方向に行列を連結、いまはただのベクトル転置操作</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
        <span class="n">s_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
        <span class="n">s_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s_mask</span><span class="p">)</span>

        <span class="c1"># Nステップあとの状態s_から、その先得られるであろう時間割引総報酬vを求めます</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>

        <span class="c1"># N-1ステップあとまでの時間割引総報酬rに、Nから先に得られるであろう総報酬vに割引N乗したものを足します</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">GAMMA_N</span> <span class="o">*</span> <span class="n">v</span> <span class="o">*</span> <span class="n">s_mask</span>  <span class="c1"># set v to 0 where s_ is terminal state</span>
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">s_t</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span><span class="p">:</span> <span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_t</span><span class="p">:</span> <span class="n">r</span><span class="p">}</span>     <span class="c1"># 重みの更新に使用するデータ</span>
        <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_global_weight_params</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>   <span class="c1"># ParameterServerの重みを更新</span>

    <span class="k">def</span> <span class="nf">predict_p</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>    <span class="c1"># 状態sから各actionの確率pベクトルを返します</span>
        <span class="n">p</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">p</span>

    <span class="k">def</span> <span class="nf">train_push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">s_</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">NONE_STATE</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span>


<span class="c1"># --行動を決定するクラスです、CartPoleであれば、棒付き台車そのものになります　-------</span>
<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">brain</span> <span class="o">=</span> <span class="n">LocalBrain</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">)</span>   <span class="c1"># 行動を決定するための脳（ニューラルネットワーク）</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="p">[]</span>        <span class="c1"># s,a,r,s_の保存メモリ、　used for n_step return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="mf">0.</span>             <span class="c1"># 時間割引した、「いまからNステップ分あとまで」の総報酬R</span>

    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">frames</span> <span class="o">&gt;=</span> <span class="n">EPS_STEPS</span><span class="p">:</span>   <span class="c1"># ε-greedy法で行動を決定します 171115修正</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">EPS_END</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">EPS_START</span> <span class="o">+</span> <span class="n">frames</span> <span class="o">*</span> <span class="p">(</span><span class="n">EPS_END</span> <span class="o">-</span> <span class="n">EPS_START</span><span class="p">)</span> <span class="o">/</span> <span class="n">EPS_STEPS</span>  <span class="c1"># linearly interpolate</span>

        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">NUM_ACTIONS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># ランダムに行動</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="p">])</span>
            <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">predict_p</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

            <span class="c1"># a = np.argmax(p)  # これだと確率最大の行動を、毎回選択</span>

            <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="c1"># probability = p のこのコードだと、確率p[0]にしたがって、行動を選択</span>
            <span class="c1"># pにはいろいろな情報が入っていますが確率のベクトルは要素0番目</span>
            <span class="k">return</span> <span class="n">a</span>

    <span class="k">def</span> <span class="nf">advantage_push_local_brain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">):</span>   <span class="c1"># advantageを考慮したs,a,r,s_をbrainに与える</span>
        <span class="k">def</span> <span class="nf">get_sample</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>  <span class="c1"># advantageを考慮し、メモリからnステップ後の状態とnステップ後までのRを取得する関数</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">memory</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">,</span> <span class="n">s_</span>

        <span class="c1"># one-hotコーディングにしたa_catsをつくり、、s,a_cats,r,s_を自分のメモリに追加</span>
        <span class="n">a_cats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">)</span>  <span class="c1"># turn action into one-hot representation</span>
        <span class="n">a_cats</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">a_cats</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">))</span>

        <span class="c1"># 前ステップの「時間割引Nステップ分の総報酬R」を使用して、現ステップのRを計算</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">+</span> <span class="n">r</span> <span class="o">*</span> <span class="n">GAMMA_N</span><span class="p">)</span> <span class="o">/</span> <span class="n">GAMMA</span>     <span class="c1"># r0はあとで引き算している、この式はヤロミルさんのサイトを参照</span>

        <span class="c1"># advantageを考慮しながら、LocalBrainに経験を入力する</span>
        <span class="k">if</span> <span class="n">s_</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span>
                <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">get_sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">train_push</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span> <span class="o">/</span> <span class="n">GAMMA</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 次の試行に向けて0にしておく</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">N_STEP_RETURN</span><span class="p">:</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">get_sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">N_STEP_RETURN</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">train_push</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>     <span class="c1"># # r0を引き算</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>


<span class="c1"># --CartPoleを実行する環境です、TensorFlowのスレッドになります　-------</span>
<span class="k">class</span> <span class="nc">Environment</span><span class="p">:</span>
    <span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># 総報酬を10試行分格納して、平均総報酬をもとめる</span>
    <span class="n">count_trial_each_thread</span> <span class="o">=</span> <span class="mi">0</span>     <span class="c1"># 各環境の試行数</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="o">=</span> <span class="n">thread_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">ENV</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">)</span>    <span class="c1"># 環境内で行動するagentを生成</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">pull_parameter_server</span><span class="p">()</span>  <span class="c1"># ParameterSeverの重みを自身のLocalBrainにコピー</span>
        <span class="k">global</span> <span class="n">frames</span>  <span class="c1"># セッション全体での試行数、global変数を書き換える場合は、関数内でglobal宣言が必要です</span>
        <span class="k">global</span> <span class="n">isLearned</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">wrappers</span><span class="o">.</span><span class="n">Monitor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="s1">'./movie/A3C'</span><span class="p">)</span>  <span class="c1"># 動画保存する場合</span>

        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>   <span class="c1"># 学習後のテストでは描画する</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

            <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>   <span class="c1"># 行動を決定</span>
            <span class="n">s_</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>   <span class="c1"># 行動を実施</span>
            <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">frames</span> <span class="o">+=</span> <span class="mi">1</span>     <span class="c1"># セッショントータルの行動回数をひとつ増やします</span>

            <span class="n">r</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>  <span class="c1"># terminal state</span>
                <span class="n">s_</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="k">if</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="mi">199</span><span class="p">:</span>
                    <span class="n">r</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">r</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># Advantageを考慮した報酬と経験を、localBrainにプッシュ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">advantage_push_local_brain</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>

            <span class="n">s</span> <span class="o">=</span> <span class="n">s_</span>
            <span class="n">R</span> <span class="o">+=</span> <span class="n">r</span>
            <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="p">(</span><span class="n">step</span> <span class="o">%</span> <span class="n">Tmax</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>  <span class="c1"># 終了時がTmaxごとに、parameterServerの重みを更新し、それをコピーする</span>
                <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">update_parameter_server</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">pull_parameter_server</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">step</span><span class="p">))</span>  <span class="c1"># トータル報酬の古いのを破棄して最新10個を保持</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># このスレッドの総試行回数を増やす</span>
                <span class="k">break</span>
        <span class="c1"># 総試行数、スレッド名、今回の報酬を出力</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">"スレッド："</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">"、試行数："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"、今回のステップ:"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">step</span><span class="p">)</span><span class="o">+</span><span class="s2">"、平均ステップ："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

        <span class="c1"># スレッドで平均報酬が一定を越えたら終了</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">199</span><span class="p">:</span>
            <span class="n">isLearned</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>     <span class="c1"># この間に他のlearningスレッドが止まります</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">push_parameter_server</span><span class="p">()</span>    <span class="c1"># この成功したスレッドのパラメータをparameter-serverに渡します</span>


<span class="c1"># --スレッドになるクラスです　-------</span>
<span class="k">class</span> <span class="nc">Worker_thread</span><span class="p">:</span>
    <span class="c1"># スレッドは学習環境environmentを持ちます</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">environment</span> <span class="o">=</span> <span class="n">Environment</span><span class="p">(</span><span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">parameter_server</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="o">=</span> <span class="n">thread_type</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>     <span class="c1"># learning threadが走る</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

            <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>    <span class="c1"># test threadを止めておく</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">isLearned</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>     <span class="c1"># learning threadを止めておく</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">isLearned</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>     <span class="c1"># test threadが走る</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>


<span class="c1"># -- main ここからメイン関数です------------------------------</span>
<span class="c1"># M0.global変数の定義と、セッションの開始です</span>
<span class="n">frames</span> <span class="o">=</span> <span class="mi">0</span>              <span class="c1"># 全スレッドで共有して使用する総ステップ数</span>
<span class="n">isLearned</span> <span class="o">=</span> <span class="bp">False</span>       <span class="c1"># 学習が終了したことを示すフラグ</span>
<span class="n">SESS</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>     <span class="c1"># TensorFlowのセッション開始</span>

<span class="c1"># M1.スレッドを作成します</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"/cpu:0"</span><span class="p">):</span>
    <span class="n">parameter_server</span> <span class="o">=</span> <span class="n">ParameterServer</span><span class="p">()</span>    <span class="c1"># 全スレッドで共有するパラメータを持つエンティティです</span>
    <span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>     <span class="c1"># 並列して走るスレッド</span>
    <span class="c1"># 学習するスレッドを用意</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_WORKERS</span><span class="p">):</span>
        <span class="n">thread_name</span> <span class="o">=</span> <span class="s2">"local_thread"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker_thread</span><span class="p">(</span><span class="n">thread_name</span><span class="o">=</span><span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="o">=</span><span class="s2">"learning"</span><span class="p">,</span> <span class="n">parameter_server</span><span class="o">=</span><span class="n">parameter_server</span><span class="p">))</span>

    <span class="c1"># 学習後にテストで走るスレッドを用意</span>
    <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker_thread</span><span class="p">(</span><span class="n">thread_name</span><span class="o">=</span><span class="s2">"test_thread"</span><span class="p">,</span> <span class="n">thread_type</span><span class="o">=</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">parameter_server</span><span class="o">=</span><span class="n">parameter_server</span><span class="p">))</span>

<span class="c1"># M2.TensorFlowでマルチスレッドを実行します</span>
<span class="n">COORD</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Coordinator</span><span class="p">()</span>                  <span class="c1"># TensorFlowでマルチスレッドにするための準備です</span>
<span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>     <span class="c1"># TensorFlowを使う場合、最初に変数初期化をして、実行します</span>

<span class="n">running_threads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">worker</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
    <span class="n">job</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">worker</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>      <span class="c1"># この辺は、マルチスレッドを走らせる作法だと思って良い</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">job</span><span class="p">)</span>
    <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="c1">#running_threads.append(t)</span>

<span class="c1"># M3.スレッドの終了を合わせます</span>
<span class="c1">#COORD.join(running_threads)</span>
</pre></div></div>

<p>A3C実装の解説でした。<br>
次はUNREALを行う予定です。<br>
以上、ご一読いただき、ありがとうございました。</p>
</div>
	</td>
</tr>
</table>
<br />
<table border="1">
<tr>
	<td rowspan="3"><center>suguluさんの<br />8位</center></td>
	<td colspan="4">
		<kbd><i><img alt="いいね" width="16" height="16" src="../thumb-up-120px.png" /></i>21</kbd>
		<a target="_blank" href="https://qiita.com/sugulu/items/8925d170f030878d6582">﻿【強化学習】実装しながら学ぶPPO【CartPoleで棒立て：1ファイルで完結】</a>
	</td>
</tr>
<tr>
	<td style="width:100px;"><center>投稿日時</center></td>
	<td style="width:200px;"><center>投稿者</center></td>
	<td style="width:150px;"><center>タグ</center></td>
	<td style="width:350px;"><center>本文</center></td>
</tr>
<tr>
	<td style="width:100px;">
		<!--投稿日時--><center>2017-10-28 17:02:08</center>
	</td>
	<td style="width:200px;">
		@sugulu<br />(都内IT企業 所属)<br><img width="80" height="80" src="https://qiita-image-store.s3.amazonaws.com/0/191401/profile-images/1509357969">
	</td>
	<td style="width:150px;">
		<!--タグ-->
		<center><b>[Python]</b> <b>[機械学習]</b> <b>[DeepLearning]</b> <b>[強化学習]</b> <b>[TensorFlow]</b> </center>
	</td>
	<td style="width:350px;">
		<!--本文-->
		<div style="width:350px;height:150px;overflow-x:hidden;overflow-y:scroll;"><p>2017年に発表された強化学習のアルゴリズム「PPO」を実装しながら、解説します。</p>

<p>PPO(Proximal Policy Optimization) は、openAIから発表された強化学習手法です。<br>
●<a href="https://blog.openai.com/openai-baselines-ppo/" rel="nofollow noopener" target="_blank">Proximal Policy Optimization - OpenAI Blog</a></p>

<p>Proximalは日本語にすると、「近位」という意味です。</p>

<p>本記事では、PPOを解説したのちに、CartPoleでの実装コードを紹介します。</p>

<p>※171115<br>
tarutoさまにお気づきいただき、AgentクラスのAct関数を修正いたしました。</p>

<h1>
<span id="ppoアルゴリズム解説" class="fragment"></span><a href="#ppo%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E8%A7%A3%E8%AA%AC"><i class="fa fa-link"></i></a>PPOアルゴリズム解説</h1>

<p>PPOは、A3CとTRPO(Trust Region Policy Optimization)を発展させて、作られています。</p>

<p>そのため、まずはこの二つを理解する必要があります。</p>

<p>A3Cについては前回の記事で紹介していますので、こちらをご覧ください。<br>
●<a href="https://qiita.com/sugulu/items/acbc909dd9b74b043e45" id="reference-1c5667d404306cfc9ab1">﻿【強化学習】実装しながら学ぶA3C【CartPoleで棒立て：1ファイルで完結】</a></p>

<p>TRPOについて、紹介します。</p>

<p>これまでの勾配法の式が以下となります。</p>

<p><a href="https://camo.qiitausercontent.com/2a7584316918a3964e8a22928f054b7d5a656c9d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39663233306135652d623238642d356561352d303431322d3831363066353134656661372e6a706567" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/2a7584316918a3964e8a22928f054b7d5a656c9d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f39663233306135652d623238642d356561352d303431322d3831363066353134656661372e6a706567" alt="s1.JPG" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/9f230a5e-b28d-5ea5-0412-8160f514efa7.jpeg"></a></p>

<p>この式で困っていたのが、方策π(a|s)が更新時に、ときおり大きく変化して変になってしまうことでした。</p>

<p>CartPoleで例えれば、右に押すのが良い確率p(右|s)が0.3だったのに、一度の更新でp(右|s)が0.9になってしまう、というイメージです。</p>

<p>そこで、2015年にTRPO(Trust Region Policy Optimization) が提案されました。</p>

<p>Trust Regionという名前の通り、信頼のできる範囲内で方策を更新する手法です。<br>
式では以下のように書かれます。</p>

<p><a href="https://camo.qiitausercontent.com/5b11c2af17bb22f9032e6bf672b020c908f7f83f/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f37313039386664372d623363322d636134322d643634322d3436363435396666326162392e6a706567" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/5b11c2af17bb22f9032e6bf672b020c908f7f83f/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f37313039386664372d623363322d636134322d643634322d3436363435396666326162392e6a706567" alt="s2.JPG" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/71098fd7-b3c2-ca42-d642-466459ff2ab9.jpeg"></a></p>

<p>KLはカルバックライブラーダイバージェンスです。<br>
更新前のπ_oldと、更新後のπの「変化の大きさ」を測る関数です。</p>

<p>罰則項にKLを与えることで、突然方策が大きく変化するのを防いでいます。</p>

<p>TRPOの解説としては、こちらもおすすめです。<br>
●<a href="https://www.slideshare.net/mooopan/trust-region-policy-optimization?qid=f7ff3d91-33a6-410a-9b40-a24b405e5616&amp;v=&amp;b=&amp;from_search=1" rel="nofollow noopener" target="_blank">Trust Region Policy Optimization (日本語)</a></p>

<p>ですが、TRPOは</p>

<ol>
<li>実装が複雑</li>
<li>ドロップアウト手法がつかえない</li>
<li>Actor-Criticに適応できない</li>
</ol>

<p>という問題がありました。</p>

<p>それらを改善するために提案されたのが、PPOです。</p>

<p>PPOは以下の式で書かれます。</p>

<p><a href="https://camo.qiitausercontent.com/2ef849ef043f2e83f0bfa84c7017313747f80dd7/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f32316438613637332d636230342d336433392d333939622d3665313736313938323634622e6a706567" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/2ef849ef043f2e83f0bfa84c7017313747f80dd7/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f32316438613637332d636230342d336433392d333939622d3665313736313938323634622e6a706567" alt="s3.JPG" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/21d8a673-cb04-3d39-399b-6e176198264b.jpeg"></a></p>

<p>要は、r_t = π / π_old が小さすぎるときは1-εにし、大きすぎるときは1+εにし、この範囲内にr_tをクリッピングしてあげることで、方策が大きく変化するのを防ぐというアイデアです。</p>

<h1>
<span id="ppo実装方法の解説" class="fragment"></span><a href="#ppo%E5%AE%9F%E8%A3%85%E6%96%B9%E6%B3%95%E3%81%AE%E8%A7%A3%E8%AA%AC"><i class="fa fa-link"></i></a>PPO実装方法の解説</h1>

<p>今回は前回作成したA3Cのプログラムを流用して作成しているので、A3Cの記事を先にご覧ください。<br>
主に大きな差分のみを解説します。</p>

<h2>
<span id="メイン関数" class="fragment"></span><a href="#%E3%83%A1%E3%82%A4%E3%83%B3%E9%96%A2%E6%95%B0"><i class="fa fa-link"></i></a>メイン関数</h2>

<p>メイン関数を紹介します。<br>
A3Cのときとまったく同じです。</p>

<p>ここでは、TensorFlowをマルチスレッドで実行します。</p>

<p>メイン関数はほとんどTensorFlowでマルチスレッドを走らせるときのお手本コード通りです。<br>
Worker_threadクラスを生成し、同時に走らせています。</p>

<p>スレッドは、training用のスレッド複数個と、学習後に実行されるtestスレッド1つがあります</p>

<p>学習とテストはファイルを分割し、学習後のパラメータを保存し、別ファイルで読み込んで走らせる方が実用的ですが、今回はスレッドを2種類用意し、学習とテストをひとつのファイルで実行します。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># -- main ここからメイン関数です------------------------------</span>
<span class="c1"># M0.global変数の定義と、セッションの開始です</span>
<span class="n">frames</span> <span class="o">=</span> <span class="mi">0</span>              <span class="c1"># 全スレッドで共有して使用する総ステップ数</span>
<span class="n">isLearned</span> <span class="o">=</span> <span class="bp">False</span>       <span class="c1"># 学習が終了したことを示すフラグ</span>
<span class="n">SESS</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>     <span class="c1"># TensorFlowのセッション開始</span>

<span class="c1"># M1.スレッドを作成します</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"/cpu:0"</span><span class="p">):</span>
    <span class="n">brain</span> <span class="o">=</span> <span class="n">Brain</span><span class="p">()</span>     <span class="c1"># ディープニューラルネットワークのクラスです</span>
    <span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>     <span class="c1"># 並列して走るスレッド</span>
    <span class="c1"># 学習するスレッドを用意</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_WORKERS</span><span class="p">):</span>
        <span class="n">thread_name</span> <span class="o">=</span> <span class="s2">"local_thread"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker_thread</span><span class="p">(</span><span class="n">thread_name</span><span class="o">=</span><span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="o">=</span><span class="s2">"learning"</span><span class="p">,</span> <span class="n">brain</span><span class="o">=</span><span class="n">brain</span><span class="p">))</span>

    <span class="c1"># 学習後にテストで走るスレッドを用意</span>
    <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker_thread</span><span class="p">(</span><span class="n">thread_name</span><span class="o">=</span><span class="s2">"test_thread"</span><span class="p">,</span> <span class="n">thread_type</span><span class="o">=</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">brain</span><span class="o">=</span><span class="n">brain</span><span class="p">))</span>

<span class="c1"># M2.TensorFlowでマルチスレッドを実行します</span>
<span class="n">COORD</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Coordinator</span><span class="p">()</span>                  <span class="c1"># TensorFlowでマルチスレッドにするための準備です</span>
<span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>     <span class="c1"># TensorFlowを使う場合、最初に変数初期化をして、実行します</span>

<span class="n">running_threads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">worker</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
    <span class="n">job</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">worker</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>      <span class="c1"># この辺は、マルチスレッドを走らせる作法だと思って良い</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">job</span><span class="p">)</span>
    <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="c1">#running_threads.append(t)</span>

<span class="c1"># M3.スレッドの終了を合わせます</span>
<span class="c1">#COORD.join(running_threads)</span>
</pre></div></div>

<h2>
<span id="worker_thread" class="fragment"></span><a href="#worker_thread"><i class="fa fa-link"></i></a>Worker_Thread</h2>

<p>ローカルスレッドです。<br>
ここもA3Cとまったく同じです。</p>

<p>メンバ変数として、Environmentを持ちます。<br>
またthread_typeはlearnigかtestで、学習用スレッドか学習後に使用するテストスレッドかを指定します。</p>

<p>run関数の内容が分かりにくいですが、学習が終わるまではlearningスレッドを走らせ、テストスレッドはスリープさせておきます。</p>

<p>学習後は、learningスレッドはスリープさせ、testスレッドを走らせています。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># --スレッドになるクラスです　-------</span>
<span class="k">class</span> <span class="nc">Worker_thread</span><span class="p">:</span>
    <span class="c1"># スレッドは学習環境environmentを持ちます</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">brain</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">environment</span> <span class="o">=</span> <span class="n">Environment</span><span class="p">(</span><span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">brain</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="o">=</span> <span class="n">thread_type</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>     <span class="c1"># learning threadが走る</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

            <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>    <span class="c1"># test threadを止めておく</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">isLearned</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>     <span class="c1"># learning threadを止めておく</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">isLearned</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>     <span class="c1"># test threadが走る</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div></div>

<h2>
<span id="environment" class="fragment"></span><a href="#environment"><i class="fa fa-link"></i></a>Environment</h2>

<p>次に、Envrionmentクラスを紹介します。<br>
ここから少しA3Cと異なります。</p>

<p>Environmentは、メンバ変数にAgentクラスを持ちます。</p>

<p>メソッドはrun()だけです。<br>
CartPoleを1試行実行します。</p>

<p>A3Cと同じく複数のAgentが実行されますが、PPOで異なるのは、各Agentは独自のネットワークを持たず、共有したネットワークのみが存在します。</p>

<p>そのためA3Cよりも簡単に実装することができます。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># --CartPoleを実行する環境です、TensorFlowのスレッドになります　-------</span>
<span class="k">class</span> <span class="nc">Environment</span><span class="p">:</span>
    <span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># 総報酬を10試行分格納して、平均総報酬をもとめる</span>
    <span class="n">count_trial_each_thread</span> <span class="o">=</span> <span class="mi">0</span>     <span class="c1"># 各環境の試行数</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">brain</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="o">=</span> <span class="n">thread_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">ENV</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">brain</span><span class="p">)</span>    <span class="c1"># 環境内で行動するagentを生成</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">global</span> <span class="n">frames</span>  <span class="c1"># セッション全体での試行数、global変数を書き換える場合は、関数内でglobal宣言が必要です</span>
        <span class="k">global</span> <span class="n">isLearned</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
           <span class="c1">#self.env = gym.wrappers.Monitor(self.env, './movie/PPO')  # 動画保存する場合</span>

        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>   <span class="c1"># 学習後のテストでは描画する</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

            <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>   <span class="c1"># 行動を決定</span>
            <span class="n">s_</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>   <span class="c1"># 行動を実施</span>
            <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">frames</span> <span class="o">+=</span> <span class="mi">1</span>     <span class="c1"># セッショントータルの行動回数をひとつ増やします</span>

            <span class="n">r</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>  <span class="c1"># terminal state</span>
                <span class="n">s_</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="k">if</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="mi">199</span><span class="p">:</span>
                    <span class="n">r</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">r</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># 報酬と経験を、Brainにプッシュ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">advantage_push_brain</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>

            <span class="n">s</span> <span class="o">=</span> <span class="n">s_</span>
            <span class="n">R</span> <span class="o">+=</span> <span class="n">r</span>
            <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="p">(</span><span class="n">frames</span> <span class="o">%</span> <span class="n">Tmax</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>  <span class="c1"># 終了時がTmaxごとに、parameterServerの重みを更新</span>
                <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">update_parameter_server</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">step</span><span class="p">))</span>  <span class="c1"># トータル報酬の古いのを破棄して最新10個を保持</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># このスレッドの総試行回数を増やす</span>
                <span class="k">break</span>
        <span class="c1"># 総試行数、スレッド名、今回の報酬を出力</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">"スレッド："</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">"、試行数："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"、今回のステップ:"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">step</span><span class="p">)</span><span class="o">+</span><span class="s2">"、平均ステップ："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

        <span class="c1"># スレッドで平均報酬が一定を越えたら終了</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">199</span><span class="p">:</span>
            <span class="n">isLearned</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>     <span class="c1"># この間に他のlearningスレッドが止まります</span>
</pre></div></div>

<h2>
<span id="agent" class="fragment"></span><a href="#agent"><i class="fa fa-link"></i></a>Agent</h2>

<p>AgentクラスはA3Cとほぼ同じです。</p>

<p>メンバ変数に共有ネットワーククラスのBrainと、メモリを持ちます。<br>
メモリはAdvantageを考慮した、(s, a, r, s_)を格納します。</p>

<p>act()メソッドはε-greedy法でランダム行動と、最適行動を選択します。<br>
最適行動は共有されたBrainのネットワークから求めます。</p>

<p>advantage_push_local_brain（）メソッドは、メモリをBrainのキューに格納します。<br>
このときにAdvantageを考慮した計算を行います。</p>

<p>工夫点は、<br>
・行動aはone-hotcoding(もし選択肢が3つあって、2つ目なら、[0,1,0]の形)にしています<br>
・nステップ分の割引総報酬self.Rを計算する際に、前ステップの結果を利用して計算しています（ヤロミルさんのサイト参照）</p>

<p>●<a href="https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation/" rel="nofollow noopener" target="_blank">Let's make an A3C</a></p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># --行動を決定するクラスです、CartPoleであれば、棒付き台車そのものになります　-------</span>
<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">brain</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">brain</span> <span class="o">=</span> <span class="n">brain</span>   <span class="c1"># 行動を決定するための脳（ニューラルネットワーク）</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="p">[]</span>        <span class="c1"># s,a,r,s_の保存メモリ、　used for n_step return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="mf">0.</span>             <span class="c1"># 時間割引した、「いまからNステップ分あとまで」の総報酬R</span>

    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">frames</span> <span class="o">&gt;=</span> <span class="n">EPS_STEPS</span><span class="p">:</span>   <span class="c1"># ε-greedy法で行動を決定します 171115修正</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">EPS_END</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">EPS_START</span> <span class="o">+</span> <span class="n">frames</span> <span class="o">*</span> <span class="p">(</span><span class="n">EPS_END</span> <span class="o">-</span> <span class="n">EPS_START</span><span class="p">)</span> <span class="o">/</span> <span class="n">EPS_STEPS</span>  <span class="c1"># linearly interpolate</span>

        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">NUM_ACTIONS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># ランダムに行動</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="p">])</span>
            <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">predict_p</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">a</span>

    <span class="k">def</span> <span class="nf">advantage_push_brain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">):</span>   <span class="c1"># advantageを考慮したs,a,r,s_をbrainに与える</span>
        <span class="k">def</span> <span class="nf">get_sample</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>  <span class="c1"># advantageを考慮し、メモリからnステップ後の状態とnステップ後までのRを取得する関数</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">memory</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">,</span> <span class="n">s_</span>

        <span class="c1"># one-hotコーディングにしたa_catsをつくり、、s,a_cats,r,s_を自分のメモリに追加</span>
        <span class="n">a_cats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">)</span>  <span class="c1"># turn action into one-hot representation</span>
        <span class="n">a_cats</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">a_cats</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">))</span>

        <span class="c1"># 前ステップの「時間割引Nステップ分の総報酬R」を使用して、現ステップのRを計算</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">+</span> <span class="n">r</span> <span class="o">*</span> <span class="n">GAMMA_N</span><span class="p">)</span> <span class="o">/</span> <span class="n">GAMMA</span>     <span class="c1"># r0はあとで引き算している、この式はヤロミルさんのサイトを参照</span>

        <span class="c1"># advantageを考慮しながら、LocalBrainに経験を入力する</span>
        <span class="k">if</span> <span class="n">s_</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span>
                <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">get_sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">train_push</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span> <span class="o">/</span> <span class="n">GAMMA</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 次の試行に向けて0にしておく</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">N_STEP_RETURN</span><span class="p">:</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">get_sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">N_STEP_RETURN</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">train_push</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>     <span class="c1"># # r0を引き算</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div></div>

<h2>
<span id="brain" class="fragment"></span><a href="#brain"><i class="fa fa-link"></i></a>Brain</h2>

<p>最後に共有されたニューラルネットワークであるBrainクラスについて説明します。<br>
ボリュームが多くて大変ですが、PPOの中心となるクラスです。</p>

<p>build_model()はA3Cと同じくActor-Criticのニューラルネットワークを定義しています。<br>
前回のA3Cと形も同じです。</p>

<p>model._make_predict_function()  # have to initialize before threading<br>
で、その後のメソッドが定義できる状態にします。</p>

<p>_build_graph()はこのネットワークに対して実行する様々なメソッドを定義している部分です。<br>
まず、loss関数を定義しています。</p>

<p>PPOで提案されたCLIP損失関数をここで実装しています。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre><span></span># loss関数を定義します
        advantage = tf.subtract(self.r_t , v)
        self.prob = tf.multiply(p, self.a_t) + 1e-10
        r_theta = tf.div(self.prob , self.prob_old)
        loss_CPI = -tf.multiply(r_theta , tf.stop_gradient(advantage))  # stop_gradientでadvantageは定数として扱います

        # CLIPした場合を計算して、小さい方を使用します。
        r_clip = r_theta
        tf.clip_by_value(r_clip, r_theta-EPSILON, r_theta+EPSILON)
        clipped_loss_CPI = -tf.multiply(r_clip , tf.stop_gradient(advantage))  # stop_gradientでadvantageは定数として扱います
        loss_CLIP = tf.reduce_mean(tf.minimum(loss_CPI, clipped_loss_CPI), axis=1, keep_dims=True)
</pre></div></div>

<p>CLIP損失関数以外に、価値関数Vの誤差と、エントロピー項があるのはA3Cと同じです。</p>

<p>また損失関数の更新方法はAdamを使用しています。<br>
A3Cの場合はRMSPropでしたが、PPOではAdamが推奨されています。</p>

<p>update_parameter_serverメソッドで、キューに蓄えた(s, a, r, s_)からNステップアドバンテージの価値を計算し、損失関数を小さくする方向にネットワークのパラメータを更新しています。</p>

<p>PPOでは価値の計算に、GAE（generalized advantage estimator）を使用します。<br>
GAEでは以下の式でアドバンテージ関数A()を求めます。</p>

<p><a href="https://camo.qiitausercontent.com/e3744ba808e668711890e42bf58d35d6e1292a9e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f62333462613832372d636362632d386631642d323732612d3037306236373933623932652e6a706567" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/e3744ba808e668711890e42bf58d35d6e1292a9e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f62333462613832372d636362632d386631642d323732612d3037306236373933623932652e6a706567" alt="s4.JPG" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/b34ba827-ccbc-8f1d-272a-070b6793b92e.jpeg"></a></p>

<p>これは、λの値が0だと1step先まで考慮するアドバンテージのない計算になり、λ=1だと、通常のアドバンテージ計算になります。</p>

<p>GAEは一度実装するとλを変えるだけで、アドバンテージの考慮具合を変更できるため、generalized という名前がついています。</p>

<p>PPOの論文だとλ=0.95が使われていましたが、面倒なので、A3Cのアドバンテージ関数（つまりλ=1の場合）をそのまま使用しています。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># --各スレッドで共有するTensorFlowのDeep Neural Networkのクラスです　-------</span>
<span class="k">class</span> <span class="nc">Brain</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>   <span class="c1"># globalなparameter_serverをメンバ変数として持つ</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">"brain"</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]]</span>  <span class="c1"># s, a, r, s', s' terminal mask</span>
            <span class="n">K</span><span class="o">.</span><span class="n">set_session</span><span class="p">(</span><span class="n">SESS</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>  <span class="c1"># ニューラルネットワークの形を決定</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>  <span class="c1"># loss関数を最小化していくoptimizerの定義です</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob_old</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_graph</span><span class="p">()</span>  <span class="c1"># ネットワークの学習やメソッドを定義</span>

    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>     <span class="c1"># Kerasでネットワークの形を定義します</span>
        <span class="n">l_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">batch_shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_STATES</span><span class="p">))</span>
        <span class="n">l_dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">l_input</span><span class="p">)</span>
        <span class="n">out_actions</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">out_value</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">l_input</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">out_actions</span><span class="p">,</span> <span class="n">out_value</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_make_predict_function</span><span class="p">()</span>  <span class="c1"># have to initialize before threading</span>
        <span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">to_file</span><span class="o">=</span><span class="s1">'PPO.png'</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># Qネットワークの可視化</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">build_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>      <span class="c1"># TensorFlowでネットワークの重みをどう学習させるのかを定義します</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_STATES</span><span class="p">))</span>  <span class="c1"># placeholderは変数が格納される予定地となります</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_ACTIONS</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># not immediate, but discounted n step reward</span>

        <span class="n">p</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s_t</span><span class="p">)</span>

        <span class="c1"># loss関数を定義します</span>
        <span class="n">advantage</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r_t</span> <span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span>
        <span class="n">r_theta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prob</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob_old</span><span class="p">)</span>
        <span class="n">loss_CPI</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">r_theta</span> <span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">advantage</span><span class="p">))</span>  <span class="c1"># stop_gradientでadvantageは定数として扱います</span>

        <span class="c1"># CLIPした場合を計算して、小さい方を使用します。</span>
        <span class="n">r_clip</span> <span class="o">=</span> <span class="n">r_theta</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">r_clip</span><span class="p">,</span> <span class="n">r_theta</span><span class="o">-</span><span class="n">EPSILON</span><span class="p">,</span> <span class="n">r_theta</span><span class="o">+</span><span class="n">EPSILON</span><span class="p">)</span>
        <span class="n">clipped_loss_CPI</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">r_clip</span> <span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">advantage</span><span class="p">))</span>  <span class="c1"># stop_gradientでadvantageは定数として扱います</span>
        <span class="n">loss_CLIP</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">loss_CPI</span><span class="p">,</span> <span class="n">clipped_loss_CPI</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">loss_value</span> <span class="o">=</span> <span class="n">LOSS_V</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">advantage</span><span class="p">)</span>  <span class="c1"># minimize value error</span>
        <span class="n">entropy</span> <span class="o">=</span> <span class="n">LOSS_ENTROPY</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># maximize entropy (regularization)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_total</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss_CLIP</span> <span class="o">+</span> <span class="n">loss_value</span> <span class="o">+</span> <span class="n">entropy</span><span class="p">)</span>

        <span class="n">minimize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_total</span><span class="p">)</span>   <span class="c1"># 求めた勾配で重み変数を更新する定義</span>
        <span class="k">return</span> <span class="n">minimize</span>

    <span class="k">def</span> <span class="nf">update_parameter_server</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>     <span class="c1"># localbrainの勾配でParameterServerの重みを学習・更新します</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">MIN_BATCH</span><span class="p">:</span>    <span class="c1"># データがたまっていない場合は更新しない</span>
            <span class="k">return</span>

        <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">s_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]]</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>    <span class="c1"># vstackはvertical-stackで縦方向に行列を連結、いまはただのベクトル転置操作</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
        <span class="n">s_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
        <span class="n">s_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s_mask</span><span class="p">)</span>

        <span class="c1"># Nステップあとの状態s_から、その先得られるであろう時間割引総報酬vを求めます</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>

        <span class="c1"># N-1ステップあとまでの時間割引総報酬rに、Nから先に得られるであろう総報酬vに割引N乗したものを足します</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">GAMMA_N</span> <span class="o">*</span> <span class="n">v</span> <span class="o">*</span> <span class="n">s_mask</span>  <span class="c1"># set v to 0 where s_ is terminal state</span>
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">s_t</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span><span class="p">:</span> <span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_t</span><span class="p">:</span> <span class="n">r</span><span class="p">}</span>     <span class="c1"># 重みの更新に使用するデータ</span>

        <span class="n">minimize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span>
        <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">minimize</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>   <span class="c1"># Brainの重みを更新</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob_old</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob</span>

    <span class="k">def</span> <span class="nf">predict_p</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>    <span class="c1"># 状態sから各actionの確率pベクトルを返します</span>
        <span class="n">p</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">p</span>

    <span class="k">def</span> <span class="nf">train_push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">s_</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">NONE_STATE</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span>
</pre></div></div>

<h2>
<span id="実行" class="fragment"></span><a href="#%E5%AE%9F%E8%A1%8C"><i class="fa fa-link"></i></a>実行</h2>

<p>以上で、コードは完成です（ただし定数宣言部分を除く）。<br>
全コードは記事の最後に掲載しています。</p>

<p>このPPOを実行すると、8つのlearningスレッドが実行され、およそ各スレッド250試行ほどで学習が終わります。</p>

<p>学習終了の条件設定などがちょっと良くないので、学習後も挙動が微妙なときがありますが、概ね200step立ちつづけてくれます。</p>

<p>こんな感じの挙動をします。</p>

<p><a href="https://camo.qiitausercontent.com/2884eff3af423ac4fd4a6d7da0d4e6bb4b3e3fec/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f31663764656165612d316462352d626133622d623437662d6535626334323436366637332e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/2884eff3af423ac4fd4a6d7da0d4e6bb4b3e3fec/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f31663764656165612d316462352d626133622d623437662d6535626334323436366637332e676966" alt="openaigym.video.0.2643.video000000.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/1f7deaea-1db5-ba3b-b47f-e5bc42466f73.gif"></a></p>

<p>以上、PPO実装の解説でした。</p>

<h1>
<span id="最終的なコード" class="fragment"></span><a href="#%E6%9C%80%E7%B5%82%E7%9A%84%E3%81%AA%E3%82%B3%E3%83%BC%E3%83%89"><i class="fa fa-link"></i></a>最終的なコード</h1>

<p>最後に全コードを掲載します。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre><span></span><span class="c1"># coding:utf-8</span>
<span class="c1"># -----------------------------------</span>
<span class="c1"># OpenGym CartPole-v0 with PPO on CPU</span>
<span class="c1"># -----------------------------------</span>
<span class="c1">#</span>
<span class="c1"># A3C implementation with TensorFlow multi threads.</span>
<span class="c1">#</span>
<span class="c1"># Made as part of Qiita article, available at</span>
<span class="c1"># https://??/</span>
<span class="c1">#</span>
<span class="c1"># author: Sugulu, 2017</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">gym</span><span class="o">,</span> <span class="nn">time</span><span class="o">,</span> <span class="nn">random</span><span class="o">,</span> <span class="nn">threading</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>  <span class="c1"># gymの画像保存</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'TF_CPP_MIN_LOG_LEVEL'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'2'</span>    <span class="c1"># TensorFlow高速化用のワーニングを表示させない</span>

<span class="c1"># -- constants of Game</span>
<span class="n">ENV</span> <span class="o">=</span> <span class="s1">'CartPole-v0'</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">ENV</span><span class="p">)</span>
<span class="n">NUM_STATES</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>     <span class="c1"># CartPoleは4状態</span>
<span class="n">NUM_ACTIONS</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>        <span class="c1"># CartPoleは、右に左に押す2アクション</span>
<span class="n">NONE_STATE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">NUM_STATES</span><span class="p">)</span>

<span class="c1"># -- constants of LocalBrain</span>
<span class="n">MIN_BATCH</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="c1"># loss_CPIをCLIPする範囲を決めます</span>
<span class="n">LOSS_V</span> <span class="o">=</span> <span class="mf">0.2</span>  <span class="c1"># v loss coefficient</span>
<span class="n">LOSS_ENTROPY</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># entropy coefficient</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">2e-3</span>

<span class="c1"># -- params of Advantage-ベルマン方程式</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">N_STEP_RETURN</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">GAMMA_N</span> <span class="o">=</span> <span class="n">GAMMA</span> <span class="o">**</span> <span class="n">N_STEP_RETURN</span>

<span class="n">N_WORKERS</span> <span class="o">=</span> <span class="mi">8</span>   <span class="c1"># スレッドの数</span>
<span class="n">Tmax</span> <span class="o">=</span> <span class="mi">3</span><span class="o">*</span><span class="n">N_WORKERS</span>   <span class="c1"># 各スレッドの更新ステップ間隔</span>

<span class="c1"># ε-greedyのパラメータ</span>
<span class="n">EPS_START</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">EPS_END</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">EPS_STEPS</span> <span class="o">=</span> <span class="mi">200</span><span class="o">*</span><span class="n">N_WORKERS</span>


<span class="c1"># --各スレッドで共有するTensorFlowのDeep Neural Networkのクラスです　-------</span>
<span class="k">class</span> <span class="nc">Brain</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>   <span class="c1"># globalなparameter_serverをメンバ変数として持つ</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">"brain"</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]]</span>  <span class="c1"># s, a, r, s', s' terminal mask</span>
            <span class="n">K</span><span class="o">.</span><span class="n">set_session</span><span class="p">(</span><span class="n">SESS</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>  <span class="c1"># ニューラルネットワークの形を決定</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>  <span class="c1"># loss関数を最小化していくoptimizerの定義です</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob_old</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_graph</span><span class="p">()</span>  <span class="c1"># ネットワークの学習やメソッドを定義</span>

    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>     <span class="c1"># Kerasでネットワークの形を定義します</span>
        <span class="n">l_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">batch_shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_STATES</span><span class="p">))</span>
        <span class="n">l_dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">l_input</span><span class="p">)</span>
        <span class="n">out_actions</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">out_value</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">)(</span><span class="n">l_dense</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">l_input</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">out_actions</span><span class="p">,</span> <span class="n">out_value</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_make_predict_function</span><span class="p">()</span>  <span class="c1"># have to initialize before threading</span>
        <span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">to_file</span><span class="o">=</span><span class="s1">'PPO.png'</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># Qネットワークの可視化</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">build_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>      <span class="c1"># TensorFlowでネットワークの重みをどう学習させるのかを定義します</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_STATES</span><span class="p">))</span>  <span class="c1"># placeholderは変数が格納される予定地となります</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">NUM_ACTIONS</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># not immediate, but discounted n step reward</span>

        <span class="n">p</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s_t</span><span class="p">)</span>

        <span class="c1"># loss関数を定義します</span>
        <span class="n">advantage</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r_t</span> <span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span>
        <span class="n">r_theta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prob</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob_old</span><span class="p">)</span>
        <span class="n">loss_CPI</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">r_theta</span> <span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">advantage</span><span class="p">))</span>  <span class="c1"># stop_gradientでadvantageは定数として扱います</span>

        <span class="c1"># CLIPした場合を計算して、小さい方を使用します。</span>
        <span class="n">r_clip</span> <span class="o">=</span> <span class="n">r_theta</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">r_clip</span><span class="p">,</span> <span class="n">r_theta</span><span class="o">-</span><span class="n">EPSILON</span><span class="p">,</span> <span class="n">r_theta</span><span class="o">+</span><span class="n">EPSILON</span><span class="p">)</span>
        <span class="n">clipped_loss_CPI</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">r_clip</span> <span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">advantage</span><span class="p">))</span>  <span class="c1"># stop_gradientでadvantageは定数として扱います</span>
        <span class="n">loss_CLIP</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">loss_CPI</span><span class="p">,</span> <span class="n">clipped_loss_CPI</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">loss_value</span> <span class="o">=</span> <span class="n">LOSS_V</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">advantage</span><span class="p">)</span>  <span class="c1"># minimize value error</span>
        <span class="n">entropy</span> <span class="o">=</span> <span class="n">LOSS_ENTROPY</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># maximize entropy (regularization)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_total</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss_CLIP</span> <span class="o">+</span> <span class="n">loss_value</span> <span class="o">+</span> <span class="n">entropy</span><span class="p">)</span>

        <span class="n">minimize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_total</span><span class="p">)</span>   <span class="c1"># 求めた勾配で重み変数を更新する定義</span>
        <span class="k">return</span> <span class="n">minimize</span>

    <span class="k">def</span> <span class="nf">update_parameter_server</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>     <span class="c1"># localbrainの勾配でParameterServerの重みを学習・更新します</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">MIN_BATCH</span><span class="p">:</span>    <span class="c1"># データがたまっていない場合は更新しない</span>
            <span class="k">return</span>

        <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">s_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]]</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>    <span class="c1"># vstackはvertical-stackで縦方向に行列を連結、いまはただのベクトル転置操作</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
        <span class="n">s_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
        <span class="n">s_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">s_mask</span><span class="p">)</span>

        <span class="c1"># Nステップあとの状態s_から、その先得られるであろう時間割引総報酬vを求めます</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>

        <span class="c1"># N-1ステップあとまでの時間割引総報酬rに、Nから先に得られるであろう総報酬vに割引N乗したものを足します</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">GAMMA_N</span> <span class="o">*</span> <span class="n">v</span> <span class="o">*</span> <span class="n">s_mask</span>  <span class="c1"># set v to 0 where s_ is terminal state</span>
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">s_t</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_t</span><span class="p">:</span> <span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_t</span><span class="p">:</span> <span class="n">r</span><span class="p">}</span>     <span class="c1"># 重みの更新に使用するデータ</span>

        <span class="n">minimize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span>
        <span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">minimize</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>   <span class="c1"># Brainの重みを更新</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob_old</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob</span>

    <span class="k">def</span> <span class="nf">predict_p</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>    <span class="c1"># 状態sから各actionの確率pベクトルを返します</span>
        <span class="n">p</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">p</span>

    <span class="k">def</span> <span class="nf">train_push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">s_</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">NONE_STATE</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_queue</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span>


<span class="c1"># --行動を決定するクラスです、CartPoleであれば、棒付き台車そのものになります　-------</span>
<span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">brain</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">brain</span> <span class="o">=</span> <span class="n">brain</span>   <span class="c1"># 行動を決定するための脳（ニューラルネットワーク）</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="p">[]</span>        <span class="c1"># s,a,r,s_の保存メモリ、　used for n_step return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="mf">0.</span>             <span class="c1"># 時間割引した、「いまからNステップ分あとまで」の総報酬R</span>

    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">frames</span> <span class="o">&gt;=</span> <span class="n">EPS_STEPS</span><span class="p">:</span>   <span class="c1"># ε-greedy法で行動を決定します 171115修正</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">EPS_END</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">EPS_START</span> <span class="o">+</span> <span class="n">frames</span> <span class="o">*</span> <span class="p">(</span><span class="n">EPS_END</span> <span class="o">-</span> <span class="n">EPS_START</span><span class="p">)</span> <span class="o">/</span> <span class="n">EPS_STEPS</span>  <span class="c1"># linearly interpolate</span>

        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">NUM_ACTIONS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># ランダムに行動</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="p">])</span>
            <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">predict_p</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">a</span>

    <span class="k">def</span> <span class="nf">advantage_push_brain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">):</span>   <span class="c1"># advantageを考慮したs,a,r,s_をbrainに与える</span>
        <span class="k">def</span> <span class="nf">get_sample</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>  <span class="c1"># advantageを考慮し、メモリからnステップ後の状態とnステップ後までのRを取得する関数</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">memory</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">,</span> <span class="n">s_</span>

        <span class="c1"># one-hotコーディングにしたa_catsをつくり、、s,a_cats,r,s_を自分のメモリに追加</span>
        <span class="n">a_cats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">NUM_ACTIONS</span><span class="p">)</span>  <span class="c1"># turn action into one-hot representation</span>
        <span class="n">a_cats</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">a_cats</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">))</span>

        <span class="c1"># 前ステップの「時間割引Nステップ分の総報酬R」を使用して、現ステップのRを計算</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">+</span> <span class="n">r</span> <span class="o">*</span> <span class="n">GAMMA_N</span><span class="p">)</span> <span class="o">/</span> <span class="n">GAMMA</span>     <span class="c1"># r0はあとで引き算している、この式はヤロミルさんのサイトを参照</span>

        <span class="c1"># advantageを考慮しながら、LocalBrainに経験を入力する</span>
        <span class="k">if</span> <span class="n">s_</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span>
                <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">get_sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">train_push</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span> <span class="o">/</span> <span class="n">GAMMA</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 次の試行に向けて0にしておく</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">N_STEP_RETURN</span><span class="p">:</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">get_sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">N_STEP_RETURN</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">train_push</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>     <span class="c1"># # r0を引き算</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>


<span class="c1"># --CartPoleを実行する環境です、TensorFlowのスレッドになります　-------</span>
<span class="k">class</span> <span class="nc">Environment</span><span class="p">:</span>
    <span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># 総報酬を10試行分格納して、平均総報酬をもとめる</span>
    <span class="n">count_trial_each_thread</span> <span class="o">=</span> <span class="mi">0</span>     <span class="c1"># 各環境の試行数</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">brain</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="o">=</span> <span class="n">thread_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">ENV</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">brain</span><span class="p">)</span>    <span class="c1"># 環境内で行動するagentを生成</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">global</span> <span class="n">frames</span>  <span class="c1"># セッション全体での試行数、global変数を書き換える場合は、関数内でglobal宣言が必要です</span>
        <span class="k">global</span> <span class="n">isLearned</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="c1">#self.env = gym.wrappers.Monitor(self.env, './movie/PPO')  # 動画保存する場合</span>


        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>   <span class="c1"># 学習後のテストでは描画する</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

            <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>   <span class="c1"># 行動を決定</span>
            <span class="n">s_</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>   <span class="c1"># 行動を実施</span>
            <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">frames</span> <span class="o">+=</span> <span class="mi">1</span>     <span class="c1"># セッショントータルの行動回数をひとつ増やします</span>

            <span class="n">r</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>  <span class="c1"># terminal state</span>
                <span class="n">s_</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="k">if</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="mi">199</span><span class="p">:</span>
                    <span class="n">r</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">r</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># 報酬と経験を、Brainにプッシュ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">advantage_push_brain</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_</span><span class="p">)</span>

            <span class="n">s</span> <span class="o">=</span> <span class="n">s_</span>
            <span class="n">R</span> <span class="o">+=</span> <span class="n">r</span>
            <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="p">(</span><span class="n">frames</span> <span class="o">%</span> <span class="n">Tmax</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>  <span class="c1"># 終了時がTmaxごとに、parameterServerの重みを更新</span>
                <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">brain</span><span class="o">.</span><span class="n">update_parameter_server</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">step</span><span class="p">))</span>  <span class="c1"># トータル報酬の古いのを破棄して最新10個を保持</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># このスレッドの総試行回数を増やす</span>
                <span class="k">break</span>
        <span class="c1"># 総試行数、スレッド名、今回の報酬を出力</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">"スレッド："</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">"、試行数："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">count_trial_each_thread</span><span class="p">)</span> <span class="o">+</span> <span class="s2">"、今回のステップ:"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">step</span><span class="p">)</span><span class="o">+</span><span class="s2">"、平均ステップ："</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

        <span class="c1"># スレッドで平均報酬が一定を越えたら終了</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">199</span><span class="p">:</span>
            <span class="n">isLearned</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>     <span class="c1"># この間に他のlearningスレッドが止まります</span>

<span class="c1"># --スレッドになるクラスです　-------</span>
<span class="k">class</span> <span class="nc">Worker_thread</span><span class="p">:</span>
    <span class="c1"># スレッドは学習環境environmentを持ちます</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">brain</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">environment</span> <span class="o">=</span> <span class="n">Environment</span><span class="p">(</span><span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="p">,</span> <span class="n">brain</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="o">=</span> <span class="n">thread_type</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>     <span class="c1"># learning threadが走る</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

            <span class="k">if</span> <span class="ow">not</span><span class="p">(</span><span class="n">isLearned</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>    <span class="c1"># test threadを止めておく</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">isLearned</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'learning'</span><span class="p">:</span>     <span class="c1"># learning threadを止めておく</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">isLearned</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread_type</span> <span class="ow">is</span> <span class="s1">'test'</span><span class="p">:</span>     <span class="c1"># test threadが走る</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>


<span class="c1"># -- main ここからメイン関数です------------------------------</span>
<span class="c1"># M0.global変数の定義と、セッションの開始です</span>
<span class="n">frames</span> <span class="o">=</span> <span class="mi">0</span>              <span class="c1"># 全スレッドで共有して使用する総ステップ数</span>
<span class="n">isLearned</span> <span class="o">=</span> <span class="bp">False</span>       <span class="c1"># 学習が終了したことを示すフラグ</span>
<span class="n">SESS</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>     <span class="c1"># TensorFlowのセッション開始</span>

<span class="c1"># M1.スレッドを作成します</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"/cpu:0"</span><span class="p">):</span>
    <span class="n">brain</span> <span class="o">=</span> <span class="n">Brain</span><span class="p">()</span>     <span class="c1"># ディープニューラルネットワークのクラスです</span>
    <span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>     <span class="c1"># 並列して走るスレッド</span>
    <span class="c1"># 学習するスレッドを用意</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_WORKERS</span><span class="p">):</span>
        <span class="n">thread_name</span> <span class="o">=</span> <span class="s2">"local_thread"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker_thread</span><span class="p">(</span><span class="n">thread_name</span><span class="o">=</span><span class="n">thread_name</span><span class="p">,</span> <span class="n">thread_type</span><span class="o">=</span><span class="s2">"learning"</span><span class="p">,</span> <span class="n">brain</span><span class="o">=</span><span class="n">brain</span><span class="p">))</span>

    <span class="c1"># 学習後にテストで走るスレッドを用意</span>
    <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Worker_thread</span><span class="p">(</span><span class="n">thread_name</span><span class="o">=</span><span class="s2">"test_thread"</span><span class="p">,</span> <span class="n">thread_type</span><span class="o">=</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">brain</span><span class="o">=</span><span class="n">brain</span><span class="p">))</span>

<span class="c1"># M2.TensorFlowでマルチスレッドを実行します</span>
<span class="n">COORD</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Coordinator</span><span class="p">()</span>                  <span class="c1"># TensorFlowでマルチスレッドにするための準備です</span>
<span class="n">SESS</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>     <span class="c1"># TensorFlowを使う場合、最初に変数初期化をして、実行します</span>

<span class="n">running_threads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">worker</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
    <span class="n">job</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">worker</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>      <span class="c1"># この辺は、マルチスレッドを走らせる作法だと思って良い</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">job</span><span class="p">)</span>
    <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="c1">#running_threads.append(t)</span>

<span class="c1"># M3.スレッドの終了を合わせます</span>
<span class="c1">#COORD.join(running_threads)</span>

</pre></div></div>
</div>
	</td>
</tr>
</table>
<br />
<table border="1">
<tr>
	<td rowspan="3"><center>suguluさんの<br />9位</center></td>
	<td colspan="4">
		<kbd><i><img alt="いいね" width="16" height="16" src="../thumb-up-120px.png" /></i>14</kbd>
		<a target="_blank" href="https://qiita.com/sugulu/items/7a14117bbd3d926eb1f2">【強化学習初心者向け】シンプルな実装例で学ぶSARSA法およびモンテカルロ法【CartPoleで棒立て：1ファイルで完結】</a>
	</td>
</tr>
<tr>
	<td style="width:100px;"><center>投稿日時</center></td>
	<td style="width:200px;"><center>投稿者</center></td>
	<td style="width:150px;"><center>タグ</center></td>
	<td style="width:350px;"><center>本文</center></td>
</tr>
<tr>
	<td style="width:100px;">
		<!--投稿日時--><center>2017-10-03 21:54:38</center>
	</td>
	<td style="width:200px;">
		@sugulu<br />(都内IT企業 所属)<br><img width="80" height="80" src="https://qiita-image-store.s3.amazonaws.com/0/191401/profile-images/1509357969">
	</td>
	<td style="width:150px;">
		<!--タグ-->
		<center><b>[Python]</b> <b>[機械学習]</b> <b>[強化学習]</b> </center>
	</td>
	<td style="width:350px;">
		<!--本文-->
		<div style="width:350px;height:150px;overflow-x:hidden;overflow-y:scroll;"><p>強化学習の代表的な手法である「SARSA法」と「モンテカルロ法」の、実装コード紹介と解説を行います</p>

<p>学習する対象には、強化学習の「Hello World！」的存在である「CartPole」を使用します。</p>

<p><a href="https://camo.qiitausercontent.com/257a4395926bf6ed0c4f92f8c88ffde2adcf8589/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f63613238393263372d393362612d303036362d363133302d3961656530636636316465332e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/257a4395926bf6ed0c4f92f8c88ffde2adcf8589/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f63613238393263372d393362612d303036362d363133302d3961656530636636316465332e676966" alt="montecarlo.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/ca2892c7-93ba-0066-6130-9aee0cf61de3.gif"></a></p>

<h1>
<span id="概要" class="fragment"></span><a href="#%E6%A6%82%E8%A6%81"><i class="fa fa-link"></i></a>概要</h1>

<p>強化学習の代表的な手法であるSARSA法、モンテカルロ法の2通りを実装・解説します。<br>
※ディープラーニングは使用しません。古典的？な強化学習です。</p>

<p>・どちらも150行程度の短いプログラムです<br>
・外部の強化学習ライブラリなどを使用せず、自力で組んでいます<br>
・コメント多めです<br>
・保守性よりも、初学者が分かりやすいことを優先してコードを書いています</p>

<p>【対象者】<br>
・Qiitaの強化学習の記事<a href="https://qiita.com/icoxfog417/items/242439ecd1a477ece312" id="reference-cf332200d5ada285b9aa">「ゼロからDeepまで学ぶ強化学習」</a>を読み、次は実装方法を知りたい方<br>
・強化学習に興味はあるが、実装方法が思い浮かばない方<br>
・SARSAやモンテカルロ法を、実装してみたい方<br>
・難しい数式を並べられるよりも、実際のコードを見たほうが理解が進む方</p>

<p>【得られるもの】<br>
SARSA法およびモンテカルロ法を用いた「シンプルミニマムな強化学習の実装例」を知ることができます。</p>

<p>【注意】<br>
本記事に入る前に、以下の記事で、強化学習、Q学習について概要をつかんでください。<br>
SARSA、モンテカルロ法の説明もあります。</p>

<p>●<a href="http://blog.brainpad.co.jp/entry/2017/02/24/121500" rel="nofollow noopener" target="_blank">強化学習入門 ～これから強化学習を学びたい人のための基礎知識～</a></p>

<p>その後、以下の記事で、棒立て問題を制御する「Open AI gymのCartPoleの使い方」と「Q学習の実装方法」をご覧ください。</p>

<p>●<a href="http://neuro-educator.com/rl1/" rel="nofollow noopener" target="_blank">CartPoleでQ学習（Q-learning）を実装・解説【Phythonで強化学習：第1回】</a></p>

<p>それではまずSARSA法について説明します。</p>

<h1>
<span id="sarsa法" class="fragment"></span><a href="#sarsa%E6%B3%95"><i class="fa fa-link"></i></a>SARSA法</h1>

<p>SARSAとは、<br>
State, Action, Reward, State(next), Action(next)の頭文字をとった手法です。</p>

<p>SARSA法とQ学習と比べてみると、Q関数の更新方法が少し異なるだけです。</p>

<p>Q学習の実装を理解していれば、SARSAは簡単に理解することができます。</p>

<p>まずおさらいとして、Q学習での <br>
Q関数 = Q(State, Action)<br>
の更新について説明します。</p>

<p>Q学習では、Q(State, Action)が</p>

<p>Reward + γ*MAX[Q(State(next), Action(next))]</p>

<p>に近づくように更新しました。<br>
（γは時間割引率）</p>

<p>そしてQ関数更新後に、実際に行う次の行動Action(next)を、ε-greedy法にしたがって決定しました。</p>

<p>※ε-greedy法<br>
報酬が最大になると期待される行動を選択するが、ときおりランダムに行動して、探索と最適化のバランスをとる手法</p>

<p>そのため、Q学習の場合はQ関数の更新に使用したAction(next)と、次の時刻での実際の行動Action(next)が異なる可能性がありました。</p>

<p>次に、SARSA法でのQ(State, Action)の更新について説明します。</p>

<p>SARSAでは次の時刻の行動Action(next)を、Q関数の更新より前に決定します。</p>

<p>なお、SARSAでも、次の行動Action(next)はε-greedy法にしたがって決定します。</p>

<p>そしてQ関数の更新を</p>

<p>Q(State, Action)が<br>
Reward + γ*Q(State(next), Action(next))</p>

<p>に近づくように更新します。</p>

<p>つまりSARSAでは、実際に行う次の行動のQ値を使用して、Q関数を更新します。</p>

<p>ステップ数が進み、ε-greedy法がほとんど探索を行わず最適行動のみを行う場合には、Q学習もSARSAも同じとなります。</p>

<p>一方で試行の初期で探索が多い場面では、SARSAは実際の行動を反映し、Q学習は期待される最大のものを使用するという特徴があります。</p>

<p>一般的にSARSAはQ関数の更新に最適値を使わないため、Q学習よりも収束が遅いですが、局所解に陥りにくいそうです。by <a href="https://www.amazon.co.jp/%E3%81%93%E3%82%8C%E3%81%8B%E3%82%89%E3%81%AE%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E7%89%A7%E9%87%8E-%E8%B2%B4%E6%A8%B9/dp/4627880316" rel="nofollow noopener" target="_blank">これからの強化学習</a></p>

<p>SARSAという方法もあること、そしてその実装方法を知っておくのは良いと思います。</p>

<p>SARSAでの学習は約1000試行で収束し、例えば以下のような結果になります。</p>

<p><a href="https://camo.qiitausercontent.com/7f62da360691d105c2a080c670842f77c46e0595/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65356230303362302d396361352d316533362d373561392d3563643033326239323837362e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/7f62da360691d105c2a080c670842f77c46e0595/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65356230303362302d396361352d316533362d373561392d3563643033326239323837362e676966" alt="sarsa.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/e5b003b0-9ca5-1e36-75a9-5cd032b92876.gif"></a></p>

<h2>
<span id="実装sarsa" class="fragment"></span><a href="#%E5%AE%9F%E8%A3%85sarsa"><i class="fa fa-link"></i></a>実装：SARSA</h2>

<p>実装コードは以下の通りです。</p>

<div class="code-frame" data-lang="python">
<div class="code-lang"><span class="bold">sarsa.py</span></div>
<div class="highlight"><pre><span></span><span class="c1"># coding:utf-8</span>
<span class="c1"># [0]ライブラリのインポート</span>
<span class="kn">import</span> <span class="nn">gym</span>  <span class="c1">#倒立振子(cartpole)の実行環境</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>  <span class="c1">#gymの画像保存</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>


<span class="c1"># [1]Q関数を離散化して定義する関数　------------</span>
<span class="c1"># 観測した状態を離散値にデジタル変換する</span>
<span class="k">def</span> <span class="nf">bins</span><span class="p">(</span><span class="n">clip_min</span><span class="p">,</span> <span class="n">clip_max</span><span class="p">,</span> <span class="n">num</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">clip_min</span><span class="p">,</span> <span class="n">clip_max</span><span class="p">,</span> <span class="n">num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>


<span class="c1"># 各値を離散値に変換</span>
<span class="k">def</span> <span class="nf">digitize_state</span><span class="p">(</span><span class="n">observation</span><span class="p">):</span>
    <span class="n">cart_pos</span><span class="p">,</span> <span class="n">cart_v</span><span class="p">,</span> <span class="n">pole_angle</span><span class="p">,</span> <span class="n">pole_v</span> <span class="o">=</span> <span class="n">observation</span>
    <span class="n">digitized</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">cart_pos</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">)),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">cart_v</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">)),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">pole_angle</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">)),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">pole_v</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">))</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_dizitized</span><span class="o">**</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">digitized</span><span class="p">)])</span>


<span class="c1"># [2]行動a(t)を求める関数 -------------------------------------</span>
<span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>    <span class="c1"># 徐々に最適行動のみをとる、ε-greedy法</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">episode</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&lt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">next_action</span>


<span class="c1"># [3]Qテーブルを更新する関数(SARSA) ＊Qlearningと異なる＊ -------------------------------------</span>
<span class="k">def</span> <span class="nf">update_Qtable_sarsa</span><span class="p">(</span><span class="n">q_table</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">):</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span>\
            <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">q_table</span>


<span class="c1"># [4]. メイン関数開始 パラメータ設定--------------------------------------------------------</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">'CartPole-v0'</span><span class="p">)</span>
<span class="n">max_number_of_steps</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1">#1試行のstep数</span>
<span class="n">num_consecutive_iterations</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1">#学習完了評価に使用する平均試行回数</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">2000</span>  <span class="c1">#総試行回数</span>
<span class="n">goal_average_reward</span> <span class="o">=</span> <span class="mi">195</span>  <span class="c1">#この報酬を超えると学習終了（中心への制御なし）</span>
<span class="c1"># 状態を6分割^（4変数）にデジタル変換してQ関数（表）を作成</span>
<span class="n">num_dizitized</span> <span class="o">=</span> <span class="mi">6</span>  <span class="c1">#分割数</span>
<span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_dizitized</span><span class="o">**</span><span class="mi">4</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
<span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_consecutive_iterations</span><span class="p">)</span>  <span class="c1">#各試行の報酬を格納</span>
<span class="n">final_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_episodes</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1">#学習後、各試行のt=200でのｘの位置を格納</span>
<span class="n">islearned</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1">#学習が終わったフラグ</span>
<span class="n">isrender</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1">#描画フラグ</span>


<span class="c1"># [5] メインルーチン--------------------------------------------------</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>  <span class="c1">#試行数分繰り返す</span>
    <span class="c1"># 環境の初期化</span>
    <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">digitize_state</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
    <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_number_of_steps</span><span class="p">):</span>  <span class="c1">#1試行のループ</span>
        <span class="k">if</span> <span class="n">islearned</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1">#学習終了したらcartPoleを描画する</span>
            <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
            <span class="k">print</span> <span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1">#カートのx位置を出力</span>

        <span class="c1"># 行動a_tの実行により、s_{t+1}, r_{t}などを計算する</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c1"># 報酬を設定し与える</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="mi">195</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">200</span>  <span class="c1">#こけたら罰則</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1">#立ったまま終了時は罰則はなし</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1">#各ステップで立ってたら報酬追加</span>

        <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>  <span class="c1">#報酬を追加</span>

        <span class="c1"># 離散状態s_{t+1}を求める</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">digitize_state</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>  <span class="c1">#t+1での観測状態を、離散値に変換</span>

        <span class="c1">#　＊ここがQlearningと異なる＊</span>
        <span class="n">next_action</span> <span class="o">=</span> <span class="n">get_action</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">episode</span><span class="p">)</span>    <span class="c1"># 次の行動a_{t+1}を求める</span>
        <span class="n">q_table</span> <span class="o">=</span> <span class="n">update_Qtable_sarsa</span><span class="p">(</span><span class="n">q_table</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">)</span>

        <span class="c1"># 次の行動と状態に更新</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">next_action</span>    <span class="c1"># a_{t+1}</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>      <span class="c1"># s_{t+1}</span>

        <span class="c1"># 終了時の処理</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="si">%d</span><span class="s1"> Episode finished after </span><span class="si">%f</span><span class="s1"> time steps / mean </span><span class="si">%f</span><span class="s1">'</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">episode</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
            <span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">total_reward_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
                                          <span class="n">episode_reward</span><span class="p">))</span>  <span class="c1">#報酬を記録</span>
            <span class="k">if</span> <span class="n">islearned</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1">#学習終わってたら最終のx座標を格納</span>
                <span class="n">final_x</span><span class="p">[</span><span class="n">episode</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">break</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;=</span>
            <span class="n">goal_average_reward</span><span class="p">):</span>  <span class="c1"># 直近の100エピソードが規定報酬以上であれば成功</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Episode </span><span class="si">%d</span><span class="s1"> train agent successfuly!'</span> <span class="o">%</span> <span class="n">episode</span><span class="p">)</span>
        <span class="n">islearned</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="c1">#np.savetxt('learned_Q_table.csv',q_table, delimiter=",") #Qtableの保存する場合</span>
        <span class="k">if</span> <span class="n">isrender</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1">#env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合</span>
            <span class="n">isrender</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1">#10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す</span>
    <span class="c1">#if episode&gt;10:</span>
    <span class="c1">#    if isrender == 0:</span>
    <span class="c1">#        env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合</span>
    <span class="c1">#        isrender = 1</span>
    <span class="c1">#    islearned=1;</span>

<span class="k">if</span> <span class="n">islearned</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s1">'final_x.csv'</span><span class="p">,</span> <span class="n">final_x</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">","</span><span class="p">)</span>

</pre></div>
</div>

<p>Q学習のコード</p>

<p><a href="http://neuro-educator.com/rl1/" rel="nofollow noopener" target="_blank">CartPoleでQ学習（Q-learning）を実装・解説【Phythonで強化学習：第1回】</a><br>
に比べて、</p>

<p>[3] Qテーブルを更新する関数(SARSA)<br>
のQ関数更新部分が異なっています。</p>

<p>Q学習では次の行動を決めておく必要はありませんでしたが、SARSAでは次の行動Action(next)を決め、Q関数の更新に使用しています。</p>

<p>それでは次に、モンテカルロ法による強化学習について説明します。</p>

<h1>
<span id="モンテカルロ法による強化学習" class="fragment"></span><a href="#%E3%83%A2%E3%83%B3%E3%83%86%E3%82%AB%E3%83%AB%E3%83%AD%E6%B3%95%E3%81%AB%E3%82%88%E3%82%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"><i class="fa fa-link"></i></a>モンテカルロ法による強化学習</h1>

<p>モンテカルロ法による強化学習は、Q学習やSARSAとは少し毛色が異なります。</p>

<p>Q関数を使用するという点は同じです。</p>

<p>一方で、モンテカルロ法では、各ステップごとにQ関数を更新しないという特徴があります。</p>

<p>その代わりに、試行が終了した時点で、Q関数を全ステップ分、一気に更新します。</p>

<p>そのため、試行終了までの、各ステップでの（状態s、行動a、得た報酬r)をすべて記憶しておきます。</p>

<p>それではモンテカルロ法でのQ関数の更新を説明します。</p>

<p>例えば、ステップ t = T　で、Poleが倒れたとします。</p>

<p>そのときの状態と行動はs(T)とa(T)と表されます。<br>
またそのステップで得た報酬はr(T)となります。</p>

<p>するとQ関数の更新は、</p>

<p>Q(s_T, a_T) が r(T) に近づくように更新します。</p>

<p>その後、現在時刻を含めその先で得られた報酬を、total_reward_tとして表すことにします。</p>

<p>時刻Tでは<br>
total_reward_t = r(T)<br>
です。</p>

<p>total_reward_tは、次のステップ以降で順繰りに使用します。</p>

<p>次にステップを一つ前に戻り、t = T-1 を更新します。</p>

<p>そこでの更新は、</p>

<p>Q(s_{T-1}, a_{T-1})が、<br>
r(T-1) + γ * r(T) に近づくように更新します。</p>

<p>これは先ほど定義したtotal_reward_tを使用すれば、</p>

<p>r(T-1) + γ * total_reward_t</p>

<p>と表されます。</p>

<p>先に<br>
total_reward_t　←　γ * total_reward_t<br>
と更新しておけば、</p>

<p>r(T-1) + γ * r(T) は、r(T-1) + total_reward_t　と表されます。</p>

<p>そのため、Q(s_{T-1}, a_{T-1}) が<br>
r(T-1) + total_reward_t<br>
に近づくように更新することになります。</p>

<p>最後に、<br>
total_reward_t　←　r(T-1) + total_reward_t<br>
と更新しておきます。</p>

<p>次にさらにステップを一つさかのぼり、t = T-2 を考えます。</p>

<p>そこでの更新は、</p>

<p>Q(s_{T-2}, a_{T-2})が、</p>

<p>r(T-2) + γ * r(T-1) + γ * γ * r(T)</p>

<p>に近づくように更新します。</p>

<p>先に<br>
total_reward_t　←　γ * total_reward_t<br>
と更新しておけば、</p>

<p>r(T-2) + total_reward_t</p>

<p>となります。</p>

<p>そのため、Q(s_{T-2}, a_{T-2})が<br>
r(T-2) + total_reward_t<br>
に近づくように更新することになります。</p>

<p>最後に、<br>
total_reward_t　←　r(T-2)+total_reward_t<br>
と更新しておきます。</p>

<p>次は t = T-3 を行います。</p>

<p>total_reward_t　←　γ * total_reward_t<br>
と更新して、</p>

<p>Q(s_{T-3}, a_{T-3})が<br>
r(T-3) + total_reward_t<br>
に近づくように更新することになります。</p>

<p>ずっとこの繰り返しです。</p>

<p>このようにQ関数を、試行の最後のステップから時刻0まで順番に更新していきます。</p>

<p>モンテカルロ法には試行の途中でQ関数を更新できないという欠点があります。<br>
一方で2つの利点があります</p>

<p>1つ目の利点は、実際に得た報酬でQ関数を更新できるという点です。</p>

<p>Q学習やSARSAの場合にはQ関数：Q(s_t, a_t)の更新にQ(s_{t+1}, a_{t+1})というまだ学習が終わっていないQ関数を使用していました。</p>

<p>一方でモンテカルロ法では実際に得た報酬を更新に使用するので、学習初期でQ関数が確かな方向に学習しやすいという利点につながります。</p>

<p>2つ目の利点が、「試行の途中で報酬がもらえない」、もしくは「報酬をうまく規定しづらいタスク」の学習に対応しやすいということです。</p>

<p>例えば囲碁や将棋などでは、途中の報酬を決めるのが難しいです。<br>
（飛車を取られても、局面が有利になることもあるかもしれないですし）</p>

<p>その場合、最終的に勝った、負けた、だけが、信頼ある報酬と考えることができます。</p>

<p>このような最終的結果からQ関数を学習することができます。</p>

<p>私がモンテカルロ法を知ってはじめに疑問だったのが「行動の決め方」です。<br>
ですが、これは通常のε-greedy法で大丈夫です。</p>

<p>以上の点を踏まえて、結果とコードを紹介します。<br>
だいたい1000試行以下で学習が収束します。</p>

<p>例えば以下のような結果になります。</p>

<p><a href="https://camo.qiitausercontent.com/a22e4d4639d941e79811c0e0b2d703c0ede3a47b/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f31613430313635302d633266322d323437302d333831652d3165653161333962633735322e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/a22e4d4639d941e79811c0e0b2d703c0ede3a47b/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f31613430313635302d633266322d323437302d333831652d3165653161333962633735322e676966" alt="montecarlo.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/1a401650-c2f2-2470-381e-1ee1a39bc752.gif"></a></p>

<h2>
<span id="実装モンテカルロ法" class="fragment"></span><a href="#%E5%AE%9F%E8%A3%85%E3%83%A2%E3%83%B3%E3%83%86%E3%82%AB%E3%83%AB%E3%83%AD%E6%B3%95"><i class="fa fa-link"></i></a>実装：モンテカルロ法</h2>

<p>実装したコードがこちらです。</p>

<div class="code-frame" data-lang="python">
<div class="code-lang"><span class="bold">montecarlo.py</span></div>
<div class="highlight"><pre><span></span><span class="c1"># coding:utf-8</span>
<span class="c1"># [0]ライブラリのインポート</span>
<span class="kn">import</span> <span class="nn">gym</span>  <span class="c1"># 倒立振子(cartpole)の実行環境</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>  <span class="c1">#gymの画像保存</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>



<span class="c1"># [1]Q関数を離散化して定義する関数　------------</span>
<span class="c1"># 観測した状態を離散値にデジタル変換する</span>
<span class="k">def</span> <span class="nf">bins</span><span class="p">(</span><span class="n">clip_min</span><span class="p">,</span> <span class="n">clip_max</span><span class="p">,</span> <span class="n">num</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">clip_min</span><span class="p">,</span> <span class="n">clip_max</span><span class="p">,</span> <span class="n">num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>


<span class="c1"># 各値を離散値に変換</span>
<span class="k">def</span> <span class="nf">digitize_state</span><span class="p">(</span><span class="n">observation</span><span class="p">):</span>
    <span class="n">cart_pos</span><span class="p">,</span> <span class="n">cart_v</span><span class="p">,</span> <span class="n">pole_angle</span><span class="p">,</span> <span class="n">pole_v</span> <span class="o">=</span> <span class="n">observation</span>
    <span class="n">digitized</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">cart_pos</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">)),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">cart_v</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">)),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">pole_angle</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">)),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">pole_v</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">num_dizitized</span><span class="p">))</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_dizitized</span><span class="o">**</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">digitized</span><span class="p">)])</span>


<span class="c1"># [2]行動a(t)を求める関数 -------------------------------------</span>
<span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">episode</span><span class="p">):</span>    <span class="c1"># 徐々に最適行動のみをとる、ε-greedy法</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">episode</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&lt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">next_action</span>


<span class="c1"># [3]1試行の各ステップの行動を保存しておくメモリクラス</span>
<span class="k">class</span> <span class="nc">Memory</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experience</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>  <span class="c1"># 最後尾のメモリを取り出す</span>

    <span class="k">def</span> <span class="nf">len</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>


<span class="c1"># [4]Qテーブルを更新する(モンテカルロ法) ＊Qlearningと異なる＊ -------------------------------------</span>
<span class="k">def</span> <span class="nf">update_Qtable_montecarlo</span><span class="p">(</span><span class="n">q_table</span><span class="p">,</span> <span class="n">memory</span><span class="p">):</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">total_reward_t</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="p">(</span><span class="n">memory</span><span class="o">.</span><span class="n">len</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="n">total_reward_t</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">total_reward_t</span>       <span class="c1"># 時間割引率をかける</span>
        <span class="c1"># Q関数を更新</span>
        <span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">reward</span><span class="o">+</span><span class="n">total_reward_t</span><span class="o">-</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span>
        <span class="n">total_reward_t</span> <span class="o">=</span> <span class="n">total_reward_t</span> <span class="o">+</span> <span class="n">reward</span>    <span class="c1"># ステップtより先でもらえた報酬の合計を更新</span>

    <span class="k">return</span> <span class="n">q_table</span>


<span class="c1"># [5]. メイン関数開始 パラメータ設定--------------------------------------------------------</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">'CartPole-v0'</span><span class="p">)</span>
<span class="n">max_number_of_steps</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1">#1試行のstep数</span>
<span class="n">num_consecutive_iterations</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1">#学習完了評価に使用する平均試行回数</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">2000</span>  <span class="c1">#総試行回数</span>
<span class="n">goal_average_reward</span> <span class="o">=</span> <span class="mi">195</span>  <span class="c1">#この報酬を超えると学習終了（中心への制御なし）</span>
<span class="c1"># 状態を6分割^（4変数）にデジタル変換してQ関数（表）を作成</span>
<span class="n">num_dizitized</span> <span class="o">=</span> <span class="mi">6</span>  <span class="c1">#分割数</span>
<span class="n">memory_size</span> <span class="o">=</span> <span class="n">max_number_of_steps</span>            <span class="c1"># バッファーメモリの大きさ</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">Memory</span><span class="p">(</span><span class="n">max_size</span><span class="o">=</span><span class="n">memory_size</span><span class="p">)</span>
<span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_dizitized</span><span class="o">**</span><span class="mi">4</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
<span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_consecutive_iterations</span><span class="p">)</span>  <span class="c1">#各試行の報酬を格納</span>
<span class="n">final_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_episodes</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1">#学習後、各試行のt=200でのｘの位置を格納</span>
<span class="n">islearned</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1">#学習が終わったフラグ</span>
<span class="n">isrender</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1">#描画フラグ</span>


<span class="c1"># [5] メインルーチン--------------------------------------------------</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>  <span class="c1">#試行数分繰り返す</span>
    <span class="c1"># 環境の初期化</span>
    <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">digitize_state</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
    <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_number_of_steps</span><span class="p">):</span>  <span class="c1">#1試行のループ</span>
        <span class="k">if</span> <span class="n">islearned</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1">#学習終了したらcartPoleを描画する</span>
            <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
            <span class="k">print</span> <span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1">#カートのx位置を出力</span>


        <span class="c1"># 行動a_tの実行により、s_{t+1}, r_{t}などを計算する</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c1"># 報酬を設定し与える</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="mi">195</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">200</span>  <span class="c1">#こけたら罰則</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1">#立ったまま終了時は罰則はなし</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1">#各ステップで立ってたら報酬追加</span>


        <span class="c1"># メモリに、現在の状態と行った行動、得た報酬を記録する</span>
        <span class="n">memory</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">))</span>

        <span class="c1"># 次ステップへ行動と状態を更新</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">digitize_state</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>  <span class="c1"># t+1での観測状態を、離散値に変換</span>
        <span class="n">next_action</span> <span class="o">=</span> <span class="n">get_action</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">episode</span><span class="p">)</span>  <span class="c1"># 次の行動a_{t+1}を求める</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">next_action</span>  <span class="c1"># a_{t+1}</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>  <span class="c1"># s_{t+1}</span>

        <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>  <span class="c1">#報酬を追加</span>

        <span class="c1"># 終了時の処理</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># これまでの行動の記憶と、最終的な結果からQテーブルを更新していく</span>
            <span class="n">q_table</span> <span class="o">=</span> <span class="n">update_Qtable_montecarlo</span><span class="p">(</span><span class="n">q_table</span><span class="p">,</span> <span class="n">memory</span><span class="p">)</span>

            <span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="si">%d</span><span class="s1"> Episode finished after </span><span class="si">%f</span><span class="s1"> time steps / mean </span><span class="si">%f</span><span class="s1">'</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">episode</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
            <span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">total_reward_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
                                          <span class="n">episode_reward</span><span class="p">))</span>  <span class="c1">#報酬を記録</span>
            <span class="k">if</span> <span class="n">islearned</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1">#学習終わってたら最終のx座標を格納</span>
                <span class="n">final_x</span><span class="p">[</span><span class="n">episode</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">break</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;=</span>
            <span class="n">goal_average_reward</span><span class="p">):</span>  <span class="c1"># 直近の100エピソードが規定報酬以上であれば成功</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Episode </span><span class="si">%d</span><span class="s1"> train agent successfuly!'</span> <span class="o">%</span> <span class="n">episode</span><span class="p">)</span>
        <span class="n">islearned</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="c1">#np.savetxt('learned_Q_table.csv',q_table, delimiter=",") #Qtableの保存する場合</span>
        <span class="k">if</span> <span class="n">isrender</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合</span>
            <span class="n">isrender</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1">#10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す</span>
    <span class="c1">#if episode&gt;10:</span>
    <span class="c1">#    if isrender == 0:</span>
    <span class="c1">#        env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合</span>
    <span class="c1">#        isrender = 1</span>
    <span class="c1">#    islearned=1;</span>

<span class="k">if</span> <span class="n">islearned</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s1">'final_x.csv'</span><span class="p">,</span> <span class="n">final_x</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">","</span><span class="p">)</span>
</pre></div>
</div>

<p>以上、強化学習のSARSAとモンテカルロ法を紹介しました。</p>

<p>また次回も強化学習の実装を紹介する予定ですので、よろしくお願いします。</p>

<p>以上、ご一読いただき、ありがとうございました。</p>
</div>
	</td>
</tr>
</table>
<br />
<table border="1">
<tr>
	<td rowspan="3"><center>suguluさんの<br />10位</center></td>
	<td colspan="4">
		<kbd><i><img alt="いいね" width="16" height="16" src="../thumb-up-120px.png" /></i>12</kbd>
		<a target="_blank" href="https://qiita.com/sugulu/items/10ac7ce53de40d4c8891">【強化学習中級者向け】実装例から学ぶ優先順位付き経験再生 prioritized experience replay DQN 【CartPoleで棒立て：1ファイルで完結】</a>
	</td>
</tr>
<tr>
	<td style="width:100px;"><center>投稿日時</center></td>
	<td style="width:200px;"><center>投稿者</center></td>
	<td style="width:150px;"><center>タグ</center></td>
	<td style="width:350px;"><center>本文</center></td>
</tr>
<tr>
	<td style="width:100px;">
		<!--投稿日時--><center>2017-10-10 05:29:42</center>
	</td>
	<td style="width:200px;">
		@sugulu<br />(都内IT企業 所属)<br><img width="80" height="80" src="https://qiita-image-store.s3.amazonaws.com/0/191401/profile-images/1509357969">
	</td>
	<td style="width:150px;">
		<!--タグ-->
		<center><b>[Python]</b> <b>[機械学習]</b> <b>[DeepLearning]</b> <b>[強化学習]</b> <b>[Keras]</b> </center>
	</td>
	<td style="width:350px;">
		<!--本文-->
		<div style="width:350px;height:150px;overflow-x:hidden;overflow-y:scroll;"><p>強化学習DQNの発展編である「優先順位付き経験再生 prioritized experience replay」を実装・解説したので、紹介します。</p>

<p><a href="https://camo.qiitausercontent.com/6d450db15c5bfae5638e5ab95d3d038daff56c91/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61323339623039352d633863612d393461372d383465322d3539326535316666396437372e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/6d450db15c5bfae5638e5ab95d3d038daff56c91/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61323339623039352d633863612d393461372d383465322d3539326535316666396437372e676966" alt="openaigym.video.0.9237.video000000.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/a239b095-c8ca-94a7-84e2-592e51ff9d77.gif"></a></p>

<h1>
<span id="概要" class="fragment"></span><a href="#%E6%A6%82%E8%A6%81"><i class="fa fa-link"></i></a>概要</h1>

<p>Open AI GymのCartPoleで、優先順位付き経験再生 prioritized experience replayにしたDQNの実装・解説をします。</p>

<p>プログラムが1ファイルで完結し、学習・理解しやすいようにしています。</p>

<p>【対象者】<br>
・強化学習DQNの発展版に興味がある方<br>
・<a href="https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0-%E3%83%81%E3%83%A7%E3%83%90-%E3%82%B5%E3%83%91%E3%82%B7%E3%83%90%E3%83%AA/dp/4320124227" rel="nofollow noopener" target="_blank">速習 強化学習: 基礎理論とアルゴリズム（書籍）</a>を読んで、Dueling Networkを知ったが、実装方法がよく分からない方<br>
・実装例を見たほうが、アルゴリズムを理解しやすい方</p>

<p>【得られるもの】<br>
ミニマム・シンプルなプログラムが実装例から、優先順位付き経験再生 prioritized experience replayを理解・使用できるようになります。</p>

<p>【注意】<br>
本記事に入る前に、以下の記事で、「Open AI gymのCartPoleの使い方」と「DQNの理論と実装」をなんとなく理解しておいてください。<br>
●<a href="http://neuro-educator.com/rl2/" rel="nofollow noopener" target="_blank">CartPoleでDQN（deep Q-learning）、DDQNを実装・解説【Phythonで強化学習：第2回】</a></p>

<p>以下の記事で「優先順位付き経験再生」の概要をなんとなく感じておいてください。<br>
●<a href="https://www.slideshare.net/ssuser07aa33/introduction-to-prioritized-experience-replay" rel="nofollow noopener" target="_blank">Introduction to Prioritized Experience Replay(日本語)</a></p>

<p><a href="https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0-%E3%83%81%E3%83%A7%E3%83%90-%E3%82%B5%E3%83%91%E3%82%B7%E3%83%90%E3%83%AA/dp/4320124227" rel="nofollow noopener" target="_blank">速習 強化学習: 基礎理論とアルゴリズム（書籍）</a>の、優先順位付き経験再生を説明も読んでおくと、なお良いです。</p>

<h1>
<span id="優先順位付き経験再生について" class="fragment"></span><a href="#%E5%84%AA%E5%85%88%E9%A0%86%E4%BD%8D%E4%BB%98%E3%81%8D%E7%B5%8C%E9%A8%93%E5%86%8D%E7%94%9F%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6"><i class="fa fa-link"></i></a>優先順位付き経験再生について</h1>

<p>prioritized experience replayは、DQNでメモリに保存していた状態(s(t), a(t), r(t), s(t+1), a(t+1) )をexperience Replayする際に、優先順位をつけましょうって方法です。</p>

<p>では、何で優先順位をつけるかというと、TD誤差の大きさです。</p>

<p>TD誤差の大きさとは、<br>
[r(t) + γ × max[Q(s(t+1), a(t+1))] - Q(s(t), a(t))</p>

<p>のことです。</p>

<p>このTD誤差が大きいサンプルを優先的に学習して、DQNのネットワークの誤差が小さくなるようにしましょうって作戦になります。</p>

<p>やりたいことはとてもシンプルです。</p>

<h1>
<span id="優先順位付き経験再生の実装" class="fragment"></span><a href="#%E5%84%AA%E5%85%88%E9%A0%86%E4%BD%8D%E4%BB%98%E3%81%8D%E7%B5%8C%E9%A8%93%E5%86%8D%E7%94%9F%E3%81%AE%E5%AE%9F%E8%A3%85"><i class="fa fa-link"></i></a>優先順位付き経験再生の実装</h1>

<p>実装には以下のサイトを参考にしました。<br>
●<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" rel="nofollow noopener" target="_blank">LET’S MAKE A DQN: DOUBLE LEARNING AND PRIORITIZED EXPERIENCE REPLAY</a></p>

<p>Jaromiruさんのサイトでは、二分木でTD誤差を格納すると最も早いと説明されていますが、分かりづらくなるので、今回はシンプルなリスト（deque）で実装しています。</p>

<p>まずは、全コードを紹介して、その後、重要な部分を解説します。</p>

<div class="code-frame" data-lang="python">
<div class="code-lang"><span class="bold">duelingNetwork.py</span></div>
<div class="highlight"><pre><span></span><span class="c1"># coding:utf-8</span>
<span class="c1"># [0]必要なライブラリのインポート</span>
<span class="kn">import</span> <span class="nn">gym</span>  <span class="c1"># 倒立振子(cartpole)の実行環境</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>  <span class="c1"># gymの画像保存</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>


<span class="c1"># [1]損失関数の定義</span>
<span class="c1"># 損失関数にhuber関数を使用します 参考https://github.com/jaara/AI-blog/blob/master/CartPole-DQN.py</span>
<span class="k">def</span> <span class="nf">huberloss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span>
    <span class="n">cond</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
    <span class="n">L2</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
    <span class="n">L1</span> <span class="o">=</span> <span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">L2</span><span class="p">,</span> <span class="n">L1</span><span class="p">)</span>  <span class="c1"># Keras does not cover where function in tensorflow :-(</span>
    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>


<span class="c1"># [2]Q関数をディープラーニングのネットワークをクラスとして定義</span>
<span class="k">class</span> <span class="nc">QNetwork</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">state_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">action_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">state_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">action_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>  <span class="c1"># 誤差を減らす学習方法はAdamとし、勾配は最大1にクリップする</span>
        <span class="c1"># self.model.compile(loss='mse', optimizer=self.optimizer)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">huberloss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>

    <span class="c1"># 重みの学習</span>
    <span class="k">def</span> <span class="nf">replay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">mini_batch</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">state_b</span><span class="p">,</span> <span class="n">action_b</span><span class="p">,</span> <span class="n">reward_b</span><span class="p">,</span> <span class="n">next_state_b</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_b</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">reward_b</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">next_state_b</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">state_b</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
                <span class="c1"># 価値計算（DDQNにも対応できるように、行動決定のQネットワークと価値観数のQネットワークは分離）</span>
                <span class="n">retmainQs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state_b</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">retmainQs</span><span class="p">)</span>  <span class="c1"># 最大の報酬を返す行動を選択する</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">reward_b</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">targetQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state_b</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="n">next_action</span><span class="p">]</span>

            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state_b</span><span class="p">)</span>  <span class="c1"># Qネットワークの出力</span>
            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">action_b</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span>  <span class="c1"># 教師信号</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># epochsは訓練データの反復回数、verbose=0は表示なしの設定</span>


    <span class="c1"># [※p1] 優先順位付き経験再生で重みの学習</span>
    <span class="k">def</span> <span class="nf">pioritized_experience_replay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">,</span> <span class="n">memory_TDerror</span><span class="p">):</span>

        <span class="c1"># 0からTD誤差の絶対値和までの一様乱数を作成(昇順にしておく)</span>
        <span class="n">sum_absolute_TDerror</span> <span class="o">=</span> <span class="n">memory_TDerror</span><span class="o">.</span><span class="n">get_sum_absolute_TDerror</span><span class="p">()</span>
        <span class="n">generatedrand_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sum_absolute_TDerror</span><span class="p">,</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">generatedrand_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">generatedrand_list</span><span class="p">)</span>

        <span class="c1"># [※p2]作成した乱数で串刺しにして、バッチを作成する</span>
        <span class="n">batch_memory</span> <span class="o">=</span> <span class="n">Memory</span><span class="p">(</span><span class="n">max_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">tmp_sum_absolute_TDerror</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">randnum</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">generatedrand_list</span><span class="p">):</span>
            <span class="k">while</span> <span class="n">tmp_sum_absolute_TDerror</span> <span class="o">&lt;</span> <span class="n">randnum</span><span class="p">:</span>
                <span class="n">tmp_sum_absolute_TDerror</span> <span class="o">+=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">memory_TDerror</span><span class="o">.</span><span class="n">buffer</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">+</span> <span class="mf">0.0001</span>
                <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">batch_memory</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">memory</span><span class="o">.</span><span class="n">buffer</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>


        <span class="c1"># あとはこのバッチで学習する</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">state_b</span><span class="p">,</span> <span class="n">action_b</span><span class="p">,</span> <span class="n">reward_b</span><span class="p">,</span> <span class="n">next_state_b</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch_memory</span><span class="o">.</span><span class="n">buffer</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_b</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">reward_b</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">next_state_b</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">state_b</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
                <span class="c1"># 価値計算（DDQNにも対応できるように、行動決定のQネットワークと価値観数のQネットワークは分離）</span>
                <span class="n">retmainQs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state_b</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">retmainQs</span><span class="p">)</span>  <span class="c1"># 最大の報酬を返す行動を選択する</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">reward_b</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">targetQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state_b</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="n">next_action</span><span class="p">]</span>

            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state_b</span><span class="p">)</span>  <span class="c1"># Qネットワークの出力</span>
            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">action_b</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span>  <span class="c1"># 教師信号</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># epochsは訓練データの反復回数、verbose=0は表示なしの設定</span>


<span class="c1"># [2]Experience ReplayとFixed Target Q-Networkを実現するメモリクラス</span>
<span class="k">class</span> <span class="nc">Memory</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experience</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)),</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span> <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">len</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>


<span class="c1"># [※p3] Memoryクラスを継承した、TD誤差を格納するクラスです</span>
<span class="k">class</span> <span class="nc">Memory_TDerror</span><span class="p">(</span><span class="n">Memory</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">max_size</span><span class="p">)</span>

    <span class="c1"># add, sample, len は継承されているので定義不要</span>

    <span class="c1"># TD誤差を取得</span>
    <span class="k">def</span> <span class="nf">get_TDerror</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mainQN</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">):</span>
        <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">buffer</span><span class="p">[</span><span class="n">memory</span><span class="o">.</span><span class="n">len</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>   <span class="c1">#最新の状態データを取り出す</span>
        <span class="c1"># 価値計算（DDQNにも対応できるように、行動決定のQネットワークと価値観数のQネットワークは分離）</span>
        <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">mainQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># 最大の報酬を返す行動を選択する</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">targetQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="n">next_action</span><span class="p">]</span>
        <span class="n">TDerror</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">targetQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">TDerror</span>

    <span class="c1"># TD誤差をすべて更新</span>
    <span class="k">def</span> <span class="nf">update_TDerror</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mainQN</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)):</span>
            <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># 最新の状態データを取り出す</span>
            <span class="c1"># 価値計算（DDQNにも対応できるように、行動決定のQネットワークと価値観数のQネットワークは分離）</span>
            <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">mainQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># 最大の報酬を返す行動を選択する</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">targetQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="n">next_action</span><span class="p">]</span>
            <span class="n">TDerror</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">targetQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">TDerror</span>

    <span class="c1"># TD誤差の絶対値和を取得</span>
    <span class="k">def</span> <span class="nf">get_sum_absolute_TDerror</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">sum_absolute_TDerror</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">len</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)):</span>
            <span class="n">sum_absolute_TDerror</span> <span class="o">+=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="mf">0.0001</span>  <span class="c1"># 最新の状態データを取り出す</span>

        <span class="k">return</span> <span class="n">sum_absolute_TDerror</span>


<span class="c1"># [3]カートの状態に応じて、行動を決定するクラス</span>
<span class="k">class</span> <span class="nc">Actor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">):</span>  <span class="c1"># [C]ｔ＋１での行動を返す</span>
        <span class="c1"># 徐々に最適行動のみをとる、ε-greedy法</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">+</span> <span class="mf">0.9</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">episode</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&lt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">retTargetQs</span> <span class="o">=</span> <span class="n">targetQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">retTargetQs</span><span class="p">)</span>  <span class="c1"># 最大の報酬を返す行動を選択する</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># ランダムに行動する</span>

        <span class="k">return</span> <span class="n">action</span>


<span class="c1"># [4] メイン関数開始----------------------------------------------------</span>
<span class="c1"># [4.1] 初期設定--------------------------------------------------------</span>
<span class="n">DQN_MODE</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 1がDQN、0がDDQNです</span>
<span class="n">LENDER_MODE</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 0は学習後も描画なし、1は学習終了後に描画する</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">'CartPole-v0'</span><span class="p">)</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">299</span>  <span class="c1"># 総試行回数</span>
<span class="n">max_number_of_steps</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># 1試行のstep数</span>
<span class="n">goal_average_reward</span> <span class="o">=</span> <span class="mi">195</span>  <span class="c1"># この報酬を超えると学習終了</span>
<span class="n">num_consecutive_iterations</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 学習完了評価の平均計算を行う試行回数</span>
<span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_consecutive_iterations</span><span class="p">)</span>  <span class="c1"># 各試行の報酬を格納</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>  <span class="c1"># 割引係数</span>
<span class="n">islearned</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 学習が終わったフラグ</span>
<span class="n">isrender</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 描画フラグ</span>
<span class="c1"># ---</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># Q-networkの隠れ層のニューロンの数</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.00001</span>  <span class="c1"># Q-networkの学習係数</span>
<span class="n">memory_size</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># バッファーメモリの大きさ</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># Q-networkを更新するバッチの大記載</span>

<span class="c1"># [4.2]Qネットワークとメモリ、Actorの生成--------------------------------------------------------</span>
<span class="n">mainQN</span> <span class="o">=</span> <span class="n">QNetwork</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>  <span class="c1"># メインのQネットワーク</span>
<span class="n">targetQN</span> <span class="o">=</span> <span class="n">QNetwork</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>  <span class="c1"># 価値を計算するQネットワーク</span>
<span class="c1"># plot_model(mainQN.model, to_file='Qnetwork.png', show_shapes=True)        # Qネットワークの可視化</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">Memory</span><span class="p">(</span><span class="n">max_size</span><span class="o">=</span><span class="n">memory_size</span><span class="p">)</span>
<span class="n">memory_TDerror</span> <span class="o">=</span> <span class="n">Memory_TDerror</span><span class="p">(</span><span class="n">max_size</span><span class="o">=</span><span class="n">memory_size</span><span class="p">)</span>

<span class="n">actor</span> <span class="o">=</span> <span class="n">Actor</span><span class="p">()</span>

<span class="c1"># [4.3]メインルーチン--------------------------------------------------------</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>  <span class="c1"># 試行数分繰り返す</span>
    <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># cartPoleの環境初期化</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span>  <span class="c1"># 1step目は適当な行動をとる</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>  <span class="c1"># list型のstateを、1行4列の行列に変換</span>
    <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_number_of_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># 1試行のループ</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">islearned</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">LENDER_MODE</span><span class="p">:</span>  <span class="c1"># 学習終了したらcartPoleを描画する</span>
            <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># カートのx位置を出力するならコメントはずす</span>

        <span class="n">action</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">mainQN</span><span class="p">)</span>  <span class="c1"># 時刻tでの行動を決定する</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>  <span class="c1"># 行動a_tの実行による、s_{t+1}, _R{t}を計算する</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>  <span class="c1"># list型のstateを、1行4列の行列に変換</span>

        <span class="c1"># 報酬を設定し、与える</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 次の状態s_{t+1}はない</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="mi">195</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># 報酬クリッピング、報酬は1, 0, -1に固定</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 立ったまま195step超えて終了時は報酬</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 各ステップで立ってたら報酬追加（はじめからrewardに1が入っているが、明示的に表す）</span>

        <span class="n">episode_reward</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># reward  # 合計報酬を更新</span>

        <span class="n">memory</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">))</span>  <span class="c1"># メモリの更新する</span>

        <span class="c1"># [※p4]TD誤差を格納する</span>
        <span class="n">TDerror</span> <span class="o">=</span> <span class="n">memory_TDerror</span><span class="o">.</span><span class="n">get_TDerror</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mainQN</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">)</span>
        <span class="n">memory_TDerror</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">TDerror</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>  <span class="c1"># 状態更新</span>

        <span class="c1"># [※p5]Qネットワークの重みを学習・更新する replay</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">memory</span><span class="o">.</span><span class="n">len</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">batch_size</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">islearned</span><span class="p">:</span>
            <span class="k">if</span>  <span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">20</span><span class="p">:</span>
                <span class="n">mainQN</span><span class="o">.</span><span class="n">replay</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mainQN</span><span class="o">.</span><span class="n">pioritized_experience_replay</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">,</span> <span class="n">memory_TDerror</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">DQN_MODE</span><span class="p">:</span>
            <span class="n">targetQN</span> <span class="o">=</span> <span class="n">mainQN</span>  <span class="c1"># 行動決定と価値計算のQネットワークをおなじにする</span>

        <span class="c1"># 1施行終了時の処理</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># [※p6]TD誤差のメモリを最新に計算しなおす</span>
            <span class="n">targetQN</span> <span class="o">=</span> <span class="n">mainQN</span>  <span class="c1"># 行動決定と価値計算のQネットワークをおなじにする</span>
            <span class="n">memory_TDerror</span><span class="o">.</span><span class="n">update_TDerror</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mainQN</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">)</span>

            <span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">total_reward_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">episode_reward</span><span class="p">))</span>  <span class="c1"># 報酬を記録</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="si">%d</span><span class="s1"> Episode finished after </span><span class="si">%f</span><span class="s1"> time steps / mean </span><span class="si">%f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">episode</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
            <span class="k">break</span>

    <span class="c1"># 複数施行の平均報酬で終了を判断</span>
    <span class="k">if</span> <span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">goal_average_reward</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Episode </span><span class="si">%d</span><span class="s1"> train agent successfuly!'</span> <span class="o">%</span> <span class="n">episode</span><span class="p">)</span>
        <span class="n">islearned</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">isrender</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># 学習済みフラグを更新</span>
            <span class="n">isrender</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">env</span> <span class="o">=</span> <span class="n">wrappers</span><span class="o">.</span><span class="n">Monitor</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="s1">'./movie/cartpole_prioritized'</span><span class="p">)</span>  <span class="c1"># 動画保存する場合</span>

</pre></div>
</div>

<p>およそ70試行と、DQNよりちょっと早く学習できます。</p>

<p>実行結果の一例は以下の通りです。</p>

<p><a href="https://camo.qiitausercontent.com/6d450db15c5bfae5638e5ab95d3d038daff56c91/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61323339623039352d633863612d393461372d383465322d3539326535316666396437372e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/6d450db15c5bfae5638e5ab95d3d038daff56c91/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61323339623039352d633863612d393461372d383465322d3539326535316666396437372e676966" alt="openaigym.video.0.9237.video000000.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/a239b095-c8ca-94a7-84e2-592e51ff9d77.gif"></a></p>

<p>コードの中で重要な部分を解説します。</p>

<h1>
<span id="コードの解説" class="fragment"></span><a href="#%E3%82%B3%E3%83%BC%E3%83%89%E3%81%AE%E8%A7%A3%E8%AA%AC"><i class="fa fa-link"></i></a>コードの解説</h1>

<p>DQNから変化している部分は[※p]でコメントしています。</p>

<p>DQNはこちら●<a href="http://neuro-educator.com/rl2/" rel="nofollow noopener" target="_blank">CartPoleでDQN（deep Q-learning）、DDQNを実装・解説</a></p>

<p>まず、※p1の優先順位付き経験再生で重みの学習の部分です。<br>
コードをピックアップすると次の通りです。</p>

<p>今回はdequeに状態(s(t), a(t), r(t), s(t+1), a(t+1) )を格納するのに加えて、別のメモリmemory_TDerrorを用意して、そのときのTD誤差も保存しています。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre><span></span> # [※p1] 優先順位付き経験再生で重みの学習
    def pioritized_experience_replay(self, memory, batch_size, gamma, targetQN, memory_TDerror):

        # 0からTD誤差の絶対値和までの一様乱数を作成(昇順にしておく)
        sum_absolute_TDerror = memory_TDerror.get_sum_absolute_TDerror()
        generatedrand_list = np.random.uniform(0, sum_absolute_TDerror,batch_size)
        generatedrand_list = np.sort(generatedrand_list)

        # [※p2]作成した乱数で串刺しにして、バッチを作成する
        batch_memory = Memory(max_size=batch_size)
        idx = 0
        tmp_sum_absolute_TDerror = 0
        for (i,randnum) in enumerate(generatedrand_list):
            while tmp_sum_absolute_TDerror &lt; randnum:
                tmp_sum_absolute_TDerror += abs(memory_TDerror.buffer[idx]) + 0.0001
                idx += 1

            batch_memory.add(memory.buffer[idx])


        # あとはこのバッチで学習する
        inputs = np.zeros((batch_size, 4))
        targets = np.zeros((batch_size, 2))
        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(batch_memory.buffer):
            inputs[i:i + 1] = state_b
            target = reward_b

            if not (next_state_b == np.zeros(state_b.shape)).all(axis=1):
                # 価値計算（DDQNにも対応できるように、行動決定のQネットワークと価値観数のQネットワークは分離）
                retmainQs = self.model.predict(next_state_b)[0]
                next_action = np.argmax(retmainQs)  # 最大の報酬を返す行動を選択する
                target = reward_b + gamma * targetQN.model.predict(next_state_b)[0][next_action]

            targets[i] = self.model.predict(state_b)  # Qネットワークの出力
            targets[i][action_b] = target  # 教師信号
            self.model.fit(inputs, targets, epochs=1, verbose=0)  # epochsは訓練データの反復回数、verbose=0は表示なしの設定
</pre></div></div>

<p>上では、最初に、メモリのTD誤差の絶対値和を求めて、その範囲内でバッチ処理分の一様乱数を生成しています。</p>

<p>そして、その一様乱数に当てはまる状態(s(t), a(t), r(t), s(t+1), a(t+1) )をバッチ処理に利用するものとして、ピックアップしています。<br>
図示すると以下の通りです。</p>

<p><a href="https://camo.qiitausercontent.com/24ed68017e14aff067edd297278780efc8a69648/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f36613934373430662d363661322d393761382d383265642d3732653538353638323836622e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/24ed68017e14aff067edd297278780efc8a69648/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f36613934373430662d363661322d393761382d383265642d3732653538353638323836622e706e67" alt="pri.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/6a94740f-66a2-97a8-82ed-72e58568286b.png"></a></p>

<p>上の図ですが、2つのメモリがあります。<br>
まず、TD誤差のメモリの誤差の絶対値の和=sum_absolute_TDerrorを求めます。<br>
次に、0からsum_absolute_TDerrorの範囲で、バッチ処理数分の一様乱数を生成します。<br>
そしてその乱数が突き刺さる部分の (s(t), a(t), r(t), s(t+1), a(t+1))をバッチ処理に使用します。</p>

<p>なお絶対値の和を求めていくときに、0.0001を毎回足して、あまりに誤差が小さいものが無視されるのを防いでいます（※割り算しないので、なくても良いですが、ある方が安定する気がする）。</p>

<p>優先順位付き経験再生を実装する工夫は、ほぼ以上となります。</p>

<p>次に、※p3では、Memoryクラスを継承した、TD誤差を格納するクラスを定義しています。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre><span></span># [※p3] Memoryクラスを継承した、TD誤差を格納するクラスです
class Memory_TDerror(Memory):
</pre></div></div>

<p>このクラスでは3つメソッドを新たに定義しています。<br>
・最新の(s(t), a(t), r(t), s(t+1), a(t+1))のTD誤差を計算し、格納<br>
・メモリ内の全てのTD誤差を更新<br>
・TD誤差の絶対値和=sum_absolute_TDerrorを取得</p>

<p>そしてメインルーチンのなかでは、※p4でTD誤差を格納しています。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre><span></span>TDerror = memory_TDerror.get_TDerror(memory, gamma, mainQN, targetQN)
memory_TDerror.add(TDerror)
</pre></div></div>

<p>またメインルーチンでは、※p5でQネットワークの重みを学習・更新するexperience replayを行いますが、最初は普通のDQNにし、少しQ関数の更新が進んでから、優先順位付き経験再生を使用しています。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre><span></span># [※p5]Qネットワークの重みを学習・更新する replay
        if (memory.len() &gt; batch_size) and not islearned:
            if  total_reward_vec.mean() &lt; 20:
                mainQN.replay(memory, batch_size, gamma, targetQN)
            else:
                mainQN.pioritized_experience_replay(memory, batch_size, gamma, targetQN, memory_TDerror)
</pre></div></div>

<p>Q関数が全然更新されていない状態で、優先順位付き経験再生をすると、そもそものQ(s,a)が、あまりにめちゃくちゃなので、不安定になるのを防ぐ工夫です。</p>

<p>最後に1試行が終わったときには、 ※p6でTD誤差のメモリを最新に計算しなおしています。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre><span></span># 1施行終了時の処理
        if done:
            # [※p6]TD誤差のメモリを最新に計算しなおす
            targetQN = mainQN  # 行動決定と価値計算のQネットワークをおなじにする
            memory_TDerror.update_TDerror(memory, gamma, mainQN, targetQN)

</pre></div></div>

<p>これは1試行が終わってQ関数の精度が高くなると、格納していたTD誤差が実態と合わなくなるので、最新のQ関数でTD誤差を計算し直しています。</p>

<h1>
<span id="まとめ" class="fragment"></span><a href="#%E3%81%BE%E3%81%A8%E3%82%81"><i class="fa fa-link"></i></a>まとめ</h1>

<p>以上、CartPoleで優先順位付き経験再生 prioritized experience replay DQN を実装・解説しました。</p>

<p>次回はディープラーニングを用いた強化学習である<br>
A3Cを実装する予定です。</p>

<p>以上、ご一読いただき、ありがとうございました。</p>
</div>
	</td>
</tr>
</table>
<br />
<table border="1">
<tr>
	<td rowspan="3"><center>suguluさんの<br />11位</center></td>
	<td colspan="4">
		<kbd><i><img alt="いいね" width="16" height="16" src="../thumb-up-120px.png" /></i>9</kbd>
		<a target="_blank" href="https://qiita.com/sugulu/items/6c4d34446d4878cde61a">【強化学習中級者向け】実装例から学ぶDueling Network DQN 【CartPoleで棒立て：1ファイルで完結】</a>
	</td>
</tr>
<tr>
	<td style="width:100px;"><center>投稿日時</center></td>
	<td style="width:200px;"><center>投稿者</center></td>
	<td style="width:150px;"><center>タグ</center></td>
	<td style="width:350px;"><center>本文</center></td>
</tr>
<tr>
	<td style="width:100px;">
		<!--投稿日時--><center>2017-10-09 06:21:59</center>
	</td>
	<td style="width:200px;">
		@sugulu<br />(都内IT企業 所属)<br><img width="80" height="80" src="https://qiita-image-store.s3.amazonaws.com/0/191401/profile-images/1509357969">
	</td>
	<td style="width:150px;">
		<!--タグ-->
		<center><b>[Python]</b> <b>[機械学習]</b> <b>[DeepLearning]</b> <b>[強化学習]</b> <b>[Keras]</b> </center>
	</td>
	<td style="width:350px;">
		<!--本文-->
		<div style="width:350px;height:150px;overflow-x:hidden;overflow-y:scroll;"><p>強化学習DQNの発展編である「Dueling Network」を実装・解説したので、紹介します。</p>

<p><a href="https://camo.qiitausercontent.com/89b0dda7c9eb21c8614664d3ff4ad1ee311b0d9a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61316431383732612d326265612d663730382d653338662d3464336332306539353433372e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/89b0dda7c9eb21c8614664d3ff4ad1ee311b0d9a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61316431383732612d326265612d663730382d653338662d3464336332306539353433372e676966" alt="dueling.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/a1d1872a-2bea-f708-e38f-4d3c20e95437.gif"></a></p>

<h1>
<span id="概要" class="fragment"></span><a href="#%E6%A6%82%E8%A6%81"><i class="fa fa-link"></i></a>概要</h1>

<p>Open AI GymのCartPoleで、Dueling NetworkにしたDQNの実装・解説をします。</p>

<p>プログラムが1ファイルで完結し、学習・理解しやすいようにしています。</p>

<p>【対象者】<br>
・強化学習DQNの発展版に興味がある方<br>
・<a href="https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0-%E3%83%81%E3%83%A7%E3%83%90-%E3%82%B5%E3%83%91%E3%82%B7%E3%83%90%E3%83%AA/dp/4320124227" rel="nofollow noopener" target="_blank">速習 強化学習: 基礎理論とアルゴリズム（書籍）</a>を読んで、Dueling Networkを知ったが、実装方法がよく分からない方<br>
・実装例を見たほうがアルゴリズムを理解しやすい方</p>

<p>【得られるもの】<br>
ミニマム・シンプルなプログラムが実装例から、Dueling Networkを理解・使用できるようになります。</p>

<p>【注意】<br>
本記事に入る前に、以下の記事で、「Open AI gymのCartPoleの使い方」と「DQNの理論と実装」をなんとなく理解しておいてください。<br>
●<a href="http://neuro-educator.com/rl2/" rel="nofollow noopener" target="_blank">CartPoleでDQN（deep Q-learning）、DDQNを実装・解説【Phythonで強化学習：第2回】</a></p>

<p>以下の記事でDueling Networkの概要をなんとなく感じておいてください。<br>
●<a href="https://www.slideshare.net/ssuser07aa33/introduction-to-dueling-network" rel="nofollow noopener" target="_blank">introduction to Dueling network(日本語)</a></p>

<p><a href="https://www.amazon.co.jp/%E9%80%9F%E7%BF%92-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E5%9F%BA%E7%A4%8E%E7%90%86%E8%AB%96%E3%81%A8%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0-%E3%83%81%E3%83%A7%E3%83%90-%E3%82%B5%E3%83%91%E3%82%B7%E3%83%90%E3%83%AA/dp/4320124227" rel="nofollow noopener" target="_blank">速習 強化学習: 基礎理論とアルゴリズム（書籍）</a>の、Dueling Networkの説明も読んでおくと、なお良いです。</p>

<h1>
<span id="dueling-networkについて" class="fragment"></span><a href="#dueling-network%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6"><i class="fa fa-link"></i></a>Dueling Networkについて</h1>

<p>CartPole問題において、通常のDQNのネットワークを図にすると以下のようになります。</p>

<p><a href="https://camo.qiitausercontent.com/52f2b9a6e6e463bdfc8484be06395fb4c2ad6101/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65383763656563612d643834312d316463332d323530612d6336633233613730336364362e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/52f2b9a6e6e463bdfc8484be06395fb4c2ad6101/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65383763656563612d643834312d316463332d323530612d6336633233613730336364362e706e67" alt="dqn_network.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/e87ceeca-d841-1dc3-250a-c6c23a703cd6.png"></a></p>

<p>一方で、Dueling Networkは以下の図のようになります。</p>

<p><a href="https://camo.qiitausercontent.com/a3ddc9965aa79398faaa075520a325739438f954/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65383662393931382d303137632d313536312d343430622d6639316534326630313463652e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/a3ddc9965aa79398faaa075520a325739438f954/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f65383662393931382d303137632d313536312d343430622d6639316534326630313463652e706e67" alt="duelingnetwork.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/e86b9918-017c-1561-440b-f91e42f014ce.png"></a></p>

<p>A(s,右に押す) は<br>
A(s,右に押す) = Q(s,右に押す) - V(s)<br>
のことで、Advantageと呼ばれます。</p>

<p>Dueling Networkが何をしたいのか説明します。</p>

<p>行動価値関数Qには、右に押そうが、左に押そうが、大体そのあと獲得できるであろう報酬合計が、状態sによって決まる部分があります。</p>

<p>例えば、もう倒れる寸前の状態sであれば、actionが右に押そうが、左に押そうが、そのあと得られるであろう報酬合計はとても少ないと予測できます。</p>

<p>つまり、Q関数が持つ情報は、状態sだけで決まる部分と、行動aしだいで決まる部分に分離できます。</p>

<p>そこでQ関数を、状態sだけで決まる部分V(s)と、行動しだいで決まる部分A(s,a)に分けて学習し、最後の出力層でV(s)とA(s,a)を足し算して、Q(s,a)を求めます。</p>

<p>DQNに比べた利点は、V(s)が行動aによらず毎回学習できる点です。</p>

<p>これは選択できる行動が増えれば増えるほど、大きな利点になります。</p>

<p>これを踏まえて、DQNのネットワークを書き換えてあげます。</p>

<h1>
<span id="dueling-networkの実装" class="fragment"></span><a href="#dueling-network%E3%81%AE%E5%AE%9F%E8%A3%85"><i class="fa fa-link"></i></a>Dueling Networkの実装</h1>

<p>実装には以下のサイトを参考にしました。<br>
●<a href="https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py" rel="nofollow noopener" target="_blank">matthiasplappert/keras-rl</a></p>

<p>以下の実装例の<br>
[2]Q関数をディープラーニングのduelingネットワークをクラスとして定義<br>
の部分がDQNと異なっています。</p>

<p>最後の層で、V(s) と A(s,a) を足し算する部分は、KerasのLambda関数を利用して定義しています。</p>

<p>ネットワークを可視化すると以下の通りです。</p>

<p><a href="https://camo.qiitausercontent.com/bc94bad9245495849c071f3b886ea4c605641263/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f36376164376237362d623839322d626435322d666366652d3966663562306538323937372e706e67" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/bc94bad9245495849c071f3b886ea4c605641263/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f36376164376237362d623839322d626435322d666366652d3966663562306538323937372e706e67" alt="dueling.png" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/67ad7b76-b892-bd52-fcfe-9ff5b0e82977.png"></a></p>

<p>Dueling NetworkではAdvantageの平均値を出力から引き算するのが一般的です。<br>
ですが、それをすると、バックプロパゲーションするときにもマスキングの工夫が必要となり、実装が大変になります。</p>

<p>そこで平均値を引かないタイプのDueling Network（naive Dueling Network）を実装しています。</p>

<p>また、隠れ層のニューロンの数をDQNの16個から20個に増やしています。</p>

<div class="code-frame" data-lang="python">
<div class="code-lang"><span class="bold">duelingNetwork.py</span></div>
<div class="highlight"><pre><span></span><span class="c1"># coding:utf-8</span>
<span class="c1"># [0]必要なライブラリのインポート</span>
<span class="kn">import</span> <span class="nn">gym</span>  <span class="c1"># 倒立振子(cartpole)の実行環境</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">Input</span> <span class="c1"># LambdaとInputを追加</span>
<span class="kn">import</span> <span class="nn">keras.models</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>  <span class="c1"># gymの画像保存</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>


<span class="c1"># [1]損失関数の定義</span>
<span class="c1"># 損失関数にhuber関数を使用します 参考https://github.com/jaara/AI-blog/blob/master/CartPole-DQN.py</span>
<span class="k">def</span> <span class="nf">huberloss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span>
    <span class="n">cond</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
    <span class="n">L2</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
    <span class="n">L1</span> <span class="o">=</span> <span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">err</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">L2</span><span class="p">,</span> <span class="n">L1</span><span class="p">)</span>  <span class="c1"># Keras does not cover where function in tensorflow :-(</span>
    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>


<span class="c1"># [2]Q関数をディープラーニングのduelingネットワークをクラスとして定義</span>
<span class="k">class</span> <span class="nc">QNetwork</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">state_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,),</span> <span class="n">action_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">inputlayer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">state_size</span><span class="p">)</span>
        <span class="n">middlelayer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">inputlayer</span><span class="p">)</span>
        <span class="n">middlelayer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">middlelayer</span><span class="p">)</span>

        <span class="c1"># DQNの場合は以下</span>
        <span class="c1">#y = Dense(action_size, activation='linear')(middlelayer)  # 0番目がV(s), 1以降がA(s,a)</span>
        <span class="c1">#outputlayer = Lambda(lambda a: a[:, 0:] - K.mean(a[:, 0:] * 0, keepdims=True),</span>
        <span class="c1">#                     output_shape=(action_size,))(y)</span>

        <span class="c1"># dueling network</span>
        <span class="n">y</span><span class="o">=</span><span class="n">Dense</span><span class="p">(</span><span class="n">action_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">)(</span><span class="n">middlelayer</span><span class="p">)</span>     <span class="c1"># 0番目がV(s), 1以降がA(s,a), 平均値は引かないnaive型にする</span>
        <span class="n">outputlayer</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">a</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">a</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="mf">0.0</span><span class="o">*</span><span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                             <span class="n">output_shape</span><span class="o">=</span><span class="p">(</span><span class="n">action_size</span><span class="p">,))(</span><span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">inputlayer</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">outputlayer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>   <span class="c1"># 誤差を減らす学習方法はAdam</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">huberloss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="c1">#self.model.compile(loss='mse', optimizer=self.optimizer)</span>

    <span class="c1"># 重みの学習</span>
    <span class="k">def</span> <span class="nf">replay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">mini_batch</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">state_b</span><span class="p">,</span> <span class="n">action_b</span><span class="p">,</span> <span class="n">reward_b</span><span class="p">,</span> <span class="n">next_state_b</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_b</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">reward_b</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">next_state_b</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">state_b</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
                <span class="c1"># 価値計算（DDQNにも対応できるように、行動決定のQネットワークと価値観数のQネットワークは分離）</span>
                <span class="n">retmainQs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state_b</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">next_action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">retmainQs</span><span class="p">)</span>  <span class="c1"># 最大の報酬を返す行動を選択する</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">reward_b</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">targetQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state_b</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="n">next_action</span><span class="p">]</span>

            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state_b</span><span class="p">)</span>    <span class="c1"># Qネットワークの出力</span>
            <span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">action_b</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span>               <span class="c1"># 教師信号</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># epochsは訓練データの反復回数、verbose=0は表示なしの設定</span>


<span class="c1"># [3]Experience ReplayとFixed Target Q-Networkを実現するメモリクラス</span>
<span class="k">class</span> <span class="nc">Memory</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experience</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)),</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span> <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">len</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>


<span class="c1"># [4]カートの状態に応じて、行動を決定するクラス</span>
<span class="k">class</span> <span class="nc">Actor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">):</span>   <span class="c1"># [C]ｔ＋１での行動を返す</span>
        <span class="c1"># 徐々に最適行動のみをとる、ε-greedy法</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">+</span> <span class="mf">0.9</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">+</span><span class="n">episode</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&lt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">retTargetQs</span> <span class="o">=</span> <span class="n">targetQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">retTargetQs</span><span class="p">)</span>  <span class="c1"># 最大の報酬を返す行動を選択する</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># ランダムに行動する</span>

        <span class="k">return</span> <span class="n">action</span>


<span class="c1"># [5] メイン関数開始----------------------------------------------------</span>
<span class="c1"># [5.1] 初期設定--------------------------------------------------------</span>
<span class="n">DQN_MODE</span> <span class="o">=</span> <span class="mi">1</span>    <span class="c1"># 1がDQN、0がDDQNです</span>
<span class="n">LENDER_MODE</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># 0は学習後も描画なし、1は学習終了後に描画する</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">'CartPole-v0'</span><span class="p">)</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">299</span>  <span class="c1"># 総試行回数</span>
<span class="n">max_number_of_steps</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># 1試行のstep数</span>
<span class="n">goal_average_reward</span> <span class="o">=</span> <span class="mi">195</span>  <span class="c1"># この報酬を超えると学習終了</span>
<span class="n">num_consecutive_iterations</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 学習完了評価の平均計算を行う試行回数</span>
<span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_consecutive_iterations</span><span class="p">)</span>  <span class="c1"># 各試行の報酬を格納</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>    <span class="c1"># 割引係数</span>
<span class="n">islearned</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 学習が終わったフラグ</span>
<span class="n">isrender</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 描画フラグ</span>
<span class="c1"># ---</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">16</span><span class="o">+</span><span class="mi">4</span>               <span class="c1"># Q-networkの隠れ層のニューロンの数</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.00001</span>         <span class="c1"># Q-networkの学習係数</span>
<span class="n">memory_size</span> <span class="o">=</span> <span class="mi">10000</span>            <span class="c1"># バッファーメモリの大きさ</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>                <span class="c1"># Q-networkを更新するバッチの大記載</span>

<span class="c1"># [5.2]Qネットワークとメモリ、Actorの生成--------------------------------------------------------</span>
<span class="n">mainQN</span> <span class="o">=</span> <span class="n">QNetwork</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>     <span class="c1"># メインのQネットワーク</span>
<span class="n">targetQN</span> <span class="o">=</span> <span class="n">QNetwork</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>   <span class="c1"># 価値を計算するQネットワーク</span>

<span class="n">mainQN</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="c1">#plot_model(mainQN.model, to_file='Qnetwork.png', show_shapes=True)        # Qネットワークの可視化</span>

<span class="n">memory</span> <span class="o">=</span> <span class="n">Memory</span><span class="p">(</span><span class="n">max_size</span><span class="o">=</span><span class="n">memory_size</span><span class="p">)</span>
<span class="n">actor</span> <span class="o">=</span> <span class="n">Actor</span><span class="p">()</span>


<span class="c1"># [5.3]メインルーチン--------------------------------------------------------</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>  <span class="c1"># 試行数分繰り返す</span>
    <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># cartPoleの環境初期化</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span>  <span class="c1"># 1step目は適当な行動をとる</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>   <span class="c1"># list型のstateを、1行4列の行列に変換</span>
    <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">targetQN</span> <span class="o">=</span> <span class="n">mainQN</span>   <span class="c1"># 行動決定と価値計算のQネットワークをおなじにする</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_number_of_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># 1試行のループ</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">islearned</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">LENDER_MODE</span><span class="p">:</span>  <span class="c1"># 学習終了したらcartPoleを描画する</span>
            <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># カートのx位置を出力するならコメントはずす</span>

        <span class="n">action</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">mainQN</span><span class="p">)</span>   <span class="c1"># 時刻tでの行動を決定する</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>   <span class="c1"># 行動a_tの実行による、s_{t+1}, _R{t}を計算する</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>     <span class="c1"># list型のstateを、1行4列の行列に変換</span>

        <span class="c1"># 報酬を設定し、与える</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 次の状態s_{t+1}はない</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="mi">195</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># 報酬クリッピング、報酬は1, 0, -1に固定</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 立ったまま195step超えて終了時は報酬</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 各ステップで立ってたら報酬追加（はじめからrewardに1が入っているが、明示的に表す）</span>

        <span class="n">episode_reward</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># reward  # 合計報酬を更新</span>

        <span class="n">memory</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">))</span>     <span class="c1"># メモリの更新する</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>  <span class="c1"># 状態更新</span>


        <span class="c1"># Qネットワークの重みを学習・更新する replay</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">memory</span><span class="o">.</span><span class="n">len</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">batch_size</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">islearned</span><span class="p">:</span>
            <span class="n">mainQN</span><span class="o">.</span><span class="n">replay</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">targetQN</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">DQN_MODE</span><span class="p">:</span>
            <span class="n">targetQN</span> <span class="o">=</span> <span class="n">mainQN</span>  <span class="c1"># 行動決定と価値計算のQネットワークをおなじにする</span>

        <span class="c1"># 1施行終了時の処理</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">total_reward_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">total_reward_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">episode_reward</span><span class="p">))</span>  <span class="c1"># 報酬を記録</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="si">%d</span><span class="s1"> Episode finished after </span><span class="si">%f</span><span class="s1"> time steps / mean </span><span class="si">%f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">episode</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
            <span class="k">break</span>

    <span class="c1"># 複数施行の平均報酬で終了を判断</span>
    <span class="k">if</span> <span class="n">total_reward_vec</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">goal_average_reward</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Episode </span><span class="si">%d</span><span class="s1"> train agent successfuly!'</span> <span class="o">%</span> <span class="n">episode</span><span class="p">)</span>
        <span class="n">islearned</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">isrender</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>   <span class="c1"># 学習済みフラグを更新</span>
            <span class="n">isrender</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># env = wrappers.Monitor(env, './movie/cartpoleDueling')  # 動画保存する場合</span>
</pre></div>
</div>

<p>およそ50試行と、DQNの半分程度の試行数で学習できます。</p>

<p>実行結果の一例は以下の通りです。</p>

<p><a href="https://camo.qiitausercontent.com/89b0dda7c9eb21c8614664d3ff4ad1ee311b0d9a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61316431383732612d326265612d663730382d653338662d3464336332306539353433372e676966" target="_blank" rel="nofollow noopener"><img src="https://camo.qiitausercontent.com/89b0dda7c9eb21c8614664d3ff4ad1ee311b0d9a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3139313430312f61316431383732612d326265612d663730382d653338662d3464336332306539353433372e676966" alt="dueling.gif" data-canonical-src="https://qiita-image-store.s3.amazonaws.com/0/191401/a1d1872a-2bea-f708-e38f-4d3c20e95437.gif"></a></p>

<p>以上、CartPoleでDueling Netwrokを実装する方法を紹介でした。</p>

<p>次回はディープラーニングを用いた強化学習である<br>
prioritized experience replay<br>
を実装する予定です。</p>

<p>以上、ご一読いただき、ありがとうございました。</p>
</div>
	</td>
</tr>
</table>
<br />
	</body>
</html>
